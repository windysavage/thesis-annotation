{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert import tokenization\n",
    "from bert import bert_tokenization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ielab33949/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ielab33949/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b9ca4d580c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n\u001b[1;32m      2\u001b[0m                             trainable=True)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolved_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'/tmp/tfhub_modules/03d6fb3ce1605ad9e5e9ed5346b2fb9623ef4d3d/assets/vocab.txt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    #input_masks = get_masks(stokens, max_seq_length)\n",
    "    #input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def make_mask(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    #input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    #input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_masks\n",
    "\n",
    "def make_segment(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    #input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    #input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['replaced_sentences'] = df_extend.Sentences.apply(lambda x:replace_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['id_bert'] = df_extend.replaced_sentences.apply(lambda x:make_id(x))\n",
    "df_extend['mask_bert'] = df_extend.replaced_sentences.apply(lambda x:make_mask(x))\n",
    "df_extend['segment_bert'] = df_extend.replaced_sentences.apply(lambda x:make_segment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend = pd.read_pickle('input_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 'D00001'\n",
    "order = 0\n",
    "orders = []\n",
    "\n",
    "for ind,row in df_extend.iterrows():\n",
    "    if row.Id == id_seq:\n",
    "        order += 1\n",
    "    else:\n",
    "        id_seq = row.Id\n",
    "        order = 1  \n",
    "    orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(orders) # prepare for the one-hot encoding???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['orders'] = orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df_extend.replaced_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27177"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tf = np.sum(term,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27177,)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3066.5393238033394"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(sum_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = np.asarray(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27177,)"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    ind = np.argmax(sum_tf)\n",
    "    number = np.max(sum_tf)\n",
    "    \n",
    "    tops[names[ind]] = number\n",
    "    \n",
    "    sum_tf = np.delete(sum_tf,ind,axis=0)\n",
    "    names = np.delete(names,ind,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops_list = list(tops.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markov --> 44.59519837401201 times\n",
      "decoding --> 43.77039498843173 times\n",
      "throughput --> 43.030899105701906 times\n",
      "iterative --> 42.29024989754903 times\n",
      "validate --> 39.66675726575603 times\n",
      "encoder --> 38.65493288243426 times\n",
      "rnn --> 37.9175042300652 times\n",
      "heterogeneous --> 37.559594303700806 times\n",
      "scalable --> 36.90972490284447 times\n",
      "benchmarks --> 36.509035115650725 times\n",
      "trajectories --> 35.332983583481266 times\n",
      "deterministic --> 34.90690307694297 times\n",
      "characterize --> 34.831454320322706 times\n",
      "decoder --> 34.823558437751196 times\n",
      "validated --> 34.59437989467476 times\n",
      "annotated --> 33.77517227935703 times\n",
      "algorithmic --> 33.01938928291916 times\n",
      "optimize --> 33.01776231579412 times\n",
      "optimized --> 32.846703752692 times\n",
      "formulate --> 32.463415502061814 times\n",
      "empirically --> 32.118881456065324 times\n",
      "baselines --> 32.00779768425649 times\n",
      "theoretic --> 31.370530059815987 times\n",
      "visualization --> 31.214197828200376 times\n",
      "github --> 30.491384659114022 times\n",
      "generalize --> 30.41869935511241 times\n",
      "gpu --> 29.943964534920763 times\n",
      "latency --> 29.936131758532557 times\n",
      "regularization --> 29.88370588051196 times\n",
      "annotation --> 29.769543114706195 times\n",
      "scalability --> 29.73058457575114 times\n",
      "dependencies --> 29.574524629102665 times\n",
      "heuristic --> 29.215507130829927 times\n",
      "computations --> 28.715213291746654 times\n",
      "optimizing --> 28.579376720378132 times\n",
      "estimating --> 28.412494664594394 times\n",
      "evaluations --> 28.187725728844473 times\n",
      "sparsity --> 28.14840623669976 times\n",
      "qualitative --> 28.065398441780452 times\n",
      "applicability --> 27.667788508487803 times\n",
      "dimensionality --> 27.304268763124128 times\n",
      "ontology --> 27.190307683841223 times\n",
      "rl --> 26.95007655325958 times\n",
      "discriminative --> 26.817429367248813 times\n",
      "mimo --> 26.70846439515826 times\n",
      "experimentally --> 26.612588269350155 times\n",
      "estimator --> 26.401091372916614 times\n",
      "automata --> 26.13375227870824 times\n",
      "svm --> 25.989070761026394 times\n",
      "quantify --> 25.782119318480333 times\n",
      "forecasting --> 25.1163695315617 times\n",
      "minimization --> 24.677008227777446 times\n",
      "modal --> 24.651110611151005 times\n",
      "interpretable --> 24.60461128935077 times\n",
      "convolution --> 24.324626117605785 times\n"
     ]
    }
   ],
   "source": [
    "for top in tops_list:\n",
    "    ans = top in tokenizer.vocab\n",
    "    if ans == False:\n",
    "        print('{} --> {} times'.format(top,tops[top]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ielab33949/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def replace_words(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    filtered = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token=='dataset':\n",
    "            token = 'data'\n",
    "        if token=='datasets':\n",
    "            token = 'data'\n",
    "        if token=='convolutional' or token=='recurrent' or token=='embedding' \\\n",
    "        or token=='embeddings' or token=='unsupervised' or token=='bayesian' \\\n",
    "        or token=='cnns' or token=='lstm' or token=='rnn':\n",
    "            token = 'technical'\n",
    "        if token=='segmentation':\n",
    "            token = 'segment'\n",
    "        if token=='outperforms' or token=='outperform':\n",
    "            token = 'better'\n",
    "        if token == 'variational':\n",
    "            token = 'variation'\n",
    "        if token == 'clustering':\n",
    "            token = 'cluster'\n",
    "        if token == 'generative':\n",
    "            token = 'generate'\n",
    "        if token == 'benchmark':\n",
    "            token = 'standard'\n",
    "        if token == 'achieves':\n",
    "            token = 'achieve'\n",
    "        if token == 'real-world':\n",
    "            token = 'world'\n",
    "        if token == 'adversarial':\n",
    "            token = 'adversary'\n",
    "        if token == 'state-of-the-art':\n",
    "            token = 'newest'\n",
    "        if token == 'architectures':\n",
    "            token = 'architecture'\n",
    "        if token == 'metrics':\n",
    "            token = 'metric'\n",
    "        if token == 'stochastic':\n",
    "            token = 'random'\n",
    "        if token == 'probabilistic':\n",
    "            token = 'probability'\n",
    "        if token == 'latent':\n",
    "            token = 'hidden'\n",
    "        if token == 'classifier' or token == 'classifiers':\n",
    "            token = 'classify'\n",
    "        if token == 'robustness':\n",
    "            token = 'robust'\n",
    "        if token == 'gaussian':            \n",
    "            token = 'normal'\n",
    "        if token == 'large-scale':\n",
    "            token = 'large'\n",
    "        if token == 'localization':\n",
    "            token = 'location'\n",
    "        if token == 'queries':\n",
    "            token = 'query'\n",
    "        \n",
    "        if token == 'iot':\n",
    "            token = 'internet'\n",
    "        if token == 'generalization':\n",
    "            token = 'general'\n",
    "        if token == 'predictive':\n",
    "            token = 'predict'\n",
    "        if token == 'computationally':\n",
    "            token = 'computational'\n",
    "        if token == 'visualization':\n",
    "            token = 'visual'\n",
    "        if token == 'e.g':\n",
    "            token = 'example'\n",
    "        if token == 'i.e':\n",
    "            token = 'then'\n",
    "        if token == '\\'s':\n",
    "            token = ''\n",
    "        \n",
    "        \n",
    "        filtered.append(token)\n",
    "        \n",
    "    filtered_sentence = ' '.join(filtered)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e.g']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('e.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = {}\n",
    "for ind,sentence in df_extend.replaced_sentences.iteritems():\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if token not in tokenizer.vocab:\n",
    "            if token in list(con.keys()):\n",
    "                con[token] += 1\n",
    "            else:\n",
    "                con[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26450"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(con.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end-to-end --> 237 times\n",
      "markov --> 185 times\n",
      "e.g --> 274 times\n",
      "i.e --> 338 times\n",
      "real-time --> 197 times\n",
      "-- --> 373 times\n",
      "`` --> 736 times\n",
      "'' --> 723 times\n",
      "throughput --> 165 times\n",
      "iterative --> 172 times\n",
      "trajectories --> 151 times\n",
      "decoding --> 168 times\n",
      "heterogeneous --> 154 times\n"
     ]
    }
   ],
   "source": [
    "for item in con.keys():\n",
    "    if con[item] >= 150:\n",
    "        print('{} --> {} times'.format(item,con[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['danny', \"'\", 's']"
      ]
     },
     "execution_count": 852,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('danny\\'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'e.g' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iot']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'iot'.split('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_extend.replaced_sentences.iteritems():\n",
    "    tokens = tokenizer.tokenize(str(item))\n",
    "    for token in tokens:\n",
    "        if len(token.split('#'))>1:\n",
    "            if token in not_used_words:\n",
    "                not_used_words[token] = not_used_words[token] + 1\n",
    "            else:\n",
    "                not_used_words[token] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##uro': 82,\n",
       " '##sc': 42,\n",
       " '##ient': 86,\n",
       " '##ists': 9,\n",
       " '##lev': 24,\n",
       " '##el': 95,\n",
       " '##channel': 10,\n",
       " '##gno': 45,\n",
       " '##se': 182,\n",
       " '##2': 3302,\n",
       " '##e': 658,\n",
       " '##mm': 96,\n",
       " '##worth': 11,\n",
       " '##iness': 18,\n",
       " '##s': 8019,\n",
       " '##ins': 28,\n",
       " '##point': 58,\n",
       " '##con': 241,\n",
       " '##stra': 74,\n",
       " '##ined': 34,\n",
       " '##al': 887,\n",
       " '##ine': 115,\n",
       " '##p': 954,\n",
       " '##time': 107,\n",
       " '##ret': 192,\n",
       " '##ization': 901,\n",
       " '##amp': 54,\n",
       " '##ling': 179,\n",
       " '##ing': 2498,\n",
       " '##m': 931,\n",
       " '##fer': 163,\n",
       " '##li': 69,\n",
       " '##fication': 63,\n",
       " '##lings': 6,\n",
       " '##ers': 214,\n",
       " '##a': 640,\n",
       " '##ised': 53,\n",
       " '##uri': 215,\n",
       " '##stic': 426,\n",
       " '##bility': 511,\n",
       " '##ag': 27,\n",
       " '##able': 564,\n",
       " '##ance': 89,\n",
       " '##r': 1223,\n",
       " '##q': 267,\n",
       " '##to': 85,\n",
       " '##gram': 85,\n",
       " '##f': 649,\n",
       " '##para': 108,\n",
       " '##meter': 75,\n",
       " '##nn': 315,\n",
       " '##net': 374,\n",
       " '##i': 505,\n",
       " '##ra': 57,\n",
       " '##ca': 52,\n",
       " '##ding': 290,\n",
       " '##tem': 6,\n",
       " '##en': 171,\n",
       " '##code': 446,\n",
       " '##ano': 9,\n",
       " '##bis': 2,\n",
       " '##her': 73,\n",
       " '##ence': 99,\n",
       " '##group': 20,\n",
       " '##ps': 191,\n",
       " '##mail': 3,\n",
       " '##ally': 384,\n",
       " '##tas': 42,\n",
       " '##k': 205,\n",
       " '##l': 775,\n",
       " '##st': 218,\n",
       " '##ric': 96,\n",
       " '##ts': 224,\n",
       " '##mute': 9,\n",
       " '##d': 1252,\n",
       " '##tiv': 76,\n",
       " '##ating': 75,\n",
       " '##si': 72,\n",
       " '##gna': 9,\n",
       " '##ls': 127,\n",
       " '##pha': 42,\n",
       " '##log': 76,\n",
       " '##raphic': 15,\n",
       " '##g': 477,\n",
       " '##naire': 20,\n",
       " '##line': 167,\n",
       " '##iki': 25,\n",
       " '##ng': 155,\n",
       " '##morphic': 44,\n",
       " '##lic': 12,\n",
       " '##o': 259,\n",
       " '##stor': 10,\n",
       " '##tive': 222,\n",
       " '##ib': 165,\n",
       " '##ration': 87,\n",
       " '##y': 281,\n",
       " '##ulation': 61,\n",
       " '##ity': 1047,\n",
       " '##pc': 79,\n",
       " '##less': 119,\n",
       " '##ym': 218,\n",
       " '##metric': 260,\n",
       " '##v': 582,\n",
       " '##imi': 522,\n",
       " '##zing': 280,\n",
       " '##pressing': 12,\n",
       " '##n': 1033,\n",
       " '##re': 197,\n",
       " '##cise': 47,\n",
       " '##izations': 91,\n",
       " '##eth': 19,\n",
       " '##ora': 108,\n",
       " '##es': 620,\n",
       " '##ize': 539,\n",
       " '##c': 787,\n",
       " '##far': 89,\n",
       " '##hn': 31,\n",
       " '##nist': 3,\n",
       " '##ist': 131,\n",
       " '##le': 173,\n",
       " '##ba': 125,\n",
       " '##ized': 486,\n",
       " '##nc': 42,\n",
       " '##box': 61,\n",
       " '##hi': 104,\n",
       " '##eva': 94,\n",
       " '##ble': 532,\n",
       " '##cast': 40,\n",
       " '##ri': 151,\n",
       " '##ivar': 20,\n",
       " '##iate': 133,\n",
       " '##var': 246,\n",
       " '##ed': 599,\n",
       " '##une': 24,\n",
       " '##lation': 41,\n",
       " '##gl': 20,\n",
       " '##ome': 41,\n",
       " '##fers': 14,\n",
       " '##emi': 2,\n",
       " '##nen': 2,\n",
       " '##ce': 212,\n",
       " '##lo': 87,\n",
       " '##cated': 14,\n",
       " '##wee': 121,\n",
       " '##qui': 48,\n",
       " '##bl': 79,\n",
       " '##og': 116,\n",
       " '##ging': 128,\n",
       " '##ping': 60,\n",
       " '##ess': 29,\n",
       " '##itate': 28,\n",
       " '##ates': 56,\n",
       " '##go': 83,\n",
       " '##dic': 87,\n",
       " '##pt': 337,\n",
       " '##otic': 166,\n",
       " '##ly': 1778,\n",
       " '##ic': 506,\n",
       " '##graphs': 66,\n",
       " '##mini': 179,\n",
       " '##in': 166,\n",
       " '##igo': 4,\n",
       " '##rie': 3,\n",
       " '##pi': 52,\n",
       " '##lr': 7,\n",
       " '##ain': 7,\n",
       " '##morphism': 32,\n",
       " '##ids': 23,\n",
       " '##on': 197,\n",
       " '##sol': 21,\n",
       " '##vable': 73,\n",
       " '##ness': 570,\n",
       " '##os': 195,\n",
       " '##ea': 20,\n",
       " '##ta': 162,\n",
       " '##ctic': 85,\n",
       " '##cal': 117,\n",
       " '##phi': 44,\n",
       " '##ation': 389,\n",
       " '##iom': 76,\n",
       " '##atic': 20,\n",
       " '##is': 177,\n",
       " '##fies': 68,\n",
       " '##ioms': 20,\n",
       " '##pressive': 63,\n",
       " '##encies': 126,\n",
       " '##xing': 48,\n",
       " '##3': 3012,\n",
       " '##izing': 261,\n",
       " '##uis': 18,\n",
       " '##tic': 80,\n",
       " '##pro': 106,\n",
       " '##pa': 138,\n",
       " '##gation': 58,\n",
       " '##ma': 220,\n",
       " '##sp': 176,\n",
       " '##ect': 238,\n",
       " '##ral': 137,\n",
       " '##ability': 312,\n",
       " '##ated': 207,\n",
       " '##tangled': 33,\n",
       " '##up': 155,\n",
       " '##les': 27,\n",
       " '##outs': 6,\n",
       " '##nt': 201,\n",
       " '##lab': 190,\n",
       " '##ele': 107,\n",
       " '##ress': 27,\n",
       " '##tter': 19,\n",
       " '##er': 526,\n",
       " '##ct': 101,\n",
       " '##uating': 8,\n",
       " '##imum': 51,\n",
       " '##bed': 107,\n",
       " '##ex': 64,\n",
       " '##izes': 102,\n",
       " '##bu': 74,\n",
       " '##cci': 22,\n",
       " '##tly': 3,\n",
       " '##aging': 80,\n",
       " '##ern': 9,\n",
       " '##ors': 198,\n",
       " '##or': 284,\n",
       " '##rs': 422,\n",
       " '##tro': 105,\n",
       " '##pies': 12,\n",
       " '##rand': 10,\n",
       " '##om': 22,\n",
       " '##wise': 154,\n",
       " '##uni': 99,\n",
       " '##form': 212,\n",
       " '##ism': 104,\n",
       " '##ei': 18,\n",
       " '##ar': 169,\n",
       " '##de': 206,\n",
       " '##mac': 10,\n",
       " '##ities': 196,\n",
       " '##int': 121,\n",
       " '##pol': 106,\n",
       " '##ur': 110,\n",
       " '##ring': 78,\n",
       " '##ois': 73,\n",
       " '##sume': 19,\n",
       " '##zes': 48,\n",
       " '##ze': 182,\n",
       " '##the': 12,\n",
       " '##quil': 55,\n",
       " '##ria': 56,\n",
       " '##ana': 12,\n",
       " '##rchy': 2,\n",
       " '##ular': 88,\n",
       " '##rel': 58,\n",
       " '##ota': 403,\n",
       " '##ted': 255,\n",
       " '##ical': 39,\n",
       " '##isation': 121,\n",
       " '##po': 29,\n",
       " '##camp': 9,\n",
       " '##tor': 200,\n",
       " '##hin': 15,\n",
       " '##ise': 75,\n",
       " '##bb': 8,\n",
       " '##ian': 187,\n",
       " '##uron': 53,\n",
       " '##ify': 40,\n",
       " '##ilis': 23,\n",
       " '##mi': 299,\n",
       " '##zation': 304,\n",
       " '##com': 78,\n",
       " '##pressed': 2,\n",
       " '##for': 41,\n",
       " '##ese': 28,\n",
       " '##ie': 26,\n",
       " '##wan': 17,\n",
       " '##48': 448,\n",
       " '##5': 1878,\n",
       " '##vis': 40,\n",
       " '##aged': 4,\n",
       " '##iot': 40,\n",
       " '##em': 109,\n",
       " '##por': 42,\n",
       " '##ns': 342,\n",
       " '##rem': 107,\n",
       " '##ental': 100,\n",
       " '##ks': 24,\n",
       " '##put': 269,\n",
       " '##tream': 9,\n",
       " '##hee': 48,\n",
       " '##iga': 48,\n",
       " '##tion': 387,\n",
       " '##ots': 4,\n",
       " '##ot': 42,\n",
       " '##ffi': 31,\n",
       " '##balance': 78,\n",
       " '##par': 53,\n",
       " '##tite': 36,\n",
       " '##tur': 126,\n",
       " '##bation': 96,\n",
       " '##sti': 56,\n",
       " '##llation': 50,\n",
       " '##100': 13,\n",
       " '##th': 225,\n",
       " '##ub': 144,\n",
       " '##hur': 2,\n",
       " '##ram': 34,\n",
       " '##ja': 30,\n",
       " '##ved': 30,\n",
       " '##9': 7244,\n",
       " '##6': 4943,\n",
       " '##vale': 2,\n",
       " '##ncy': 214,\n",
       " '##yn': 114,\n",
       " '##ch': 193,\n",
       " '##ron': 126,\n",
       " '##ous': 169,\n",
       " '##atics': 38,\n",
       " '##smo': 17,\n",
       " '##logy': 141,\n",
       " '##ys': 40,\n",
       " '##29': 441,\n",
       " '##1': 2772,\n",
       " '##xi': 99,\n",
       " '##ili': 87,\n",
       " '##ties': 139,\n",
       " '##rily': 5,\n",
       " '##standing': 4,\n",
       " '##ali': 243,\n",
       " '##gni': 12,\n",
       " '##w': 147,\n",
       " '##ua': 41,\n",
       " '##cting': 17,\n",
       " '##ial': 166,\n",
       " '##rogate': 53,\n",
       " '##back': 56,\n",
       " '##7': 6630,\n",
       " '##8': 6334,\n",
       " '##ologies': 106,\n",
       " '##ant': 20,\n",
       " '##meric': 16,\n",
       " '##logical': 43,\n",
       " '##t': 790,\n",
       " '##lay': 39,\n",
       " '##tra': 114,\n",
       " '##cola': 11,\n",
       " '##4': 2689,\n",
       " '##fl': 39,\n",
       " '##ip': 55,\n",
       " '##ser': 139,\n",
       " '##ogical': 2,\n",
       " '##sing': 169,\n",
       " '##ent': 136,\n",
       " '##ran': 14,\n",
       " '##cy': 183,\n",
       " '##nti': 42,\n",
       " '##nu': 39,\n",
       " '##ire': 104,\n",
       " '##ction': 122,\n",
       " '##ms': 181,\n",
       " '##ima': 226,\n",
       " '##mp': 139,\n",
       " '##ose': 37,\n",
       " '##ifier': 21,\n",
       " '##oc': 152,\n",
       " '##gies': 5,\n",
       " '##station': 18,\n",
       " '##ary': 51,\n",
       " '##ari': 128,\n",
       " '##ty': 98,\n",
       " '##ci': 129,\n",
       " '##metry': 29,\n",
       " '##gau': 5,\n",
       " '##ssi': 82,\n",
       " '##an': 73,\n",
       " '##cl': 107,\n",
       " '##u': 340,\n",
       " '##game': 11,\n",
       " '##base': 32,\n",
       " '##ible': 102,\n",
       " '##eg': 93,\n",
       " '##ency': 35,\n",
       " '##ressed': 2,\n",
       " '##der': 171,\n",
       " '##mo': 300,\n",
       " '##di': 38,\n",
       " '##fied': 40,\n",
       " '##ied': 3,\n",
       " '##istic': 106,\n",
       " '##roll': 10,\n",
       " '##ful': 43,\n",
       " '##onal': 8,\n",
       " '##tative': 146,\n",
       " '##zoo': 2,\n",
       " '##inator': 160,\n",
       " '##mat': 132,\n",
       " '##h': 130,\n",
       " '##ably': 45,\n",
       " '##aga': 51,\n",
       " '##te': 272,\n",
       " '##mc': 61,\n",
       " '##ust': 44,\n",
       " '##rative': 27,\n",
       " '##rre': 47,\n",
       " '##late': 19,\n",
       " '##ens': 125,\n",
       " '##erative': 230,\n",
       " '##eding': 6,\n",
       " '##ive': 317,\n",
       " '##aj': 155,\n",
       " '##ories': 157,\n",
       " '##lica': 108,\n",
       " '##ater': 29,\n",
       " '##nio': 26,\n",
       " '##rot': 25,\n",
       " '##fe': 77,\n",
       " '##asi': 41,\n",
       " '##ifying': 27,\n",
       " '##oh': 18,\n",
       " '##ere': 11,\n",
       " '##let': 79,\n",
       " '##edance': 23,\n",
       " '##ography': 84,\n",
       " '##crow': 3,\n",
       " '##ded': 55,\n",
       " '##uted': 10,\n",
       " '##b': 241,\n",
       " '##fi': 278,\n",
       " '##tting': 49,\n",
       " '##uc': 112,\n",
       " '##gp': 47,\n",
       " '##ti': 308,\n",
       " '##gli': 48,\n",
       " '##gible': 46,\n",
       " '##va': 75,\n",
       " '##bly': 96,\n",
       " '##eak': 6,\n",
       " '##nets': 50,\n",
       " '##tone': 23,\n",
       " '##plication': 14,\n",
       " '##bid': 4,\n",
       " '##io': 105,\n",
       " '##per': 187,\n",
       " '##ative': 151,\n",
       " '##iza': 57,\n",
       " '##oper': 24,\n",
       " '##hort': 16,\n",
       " '##iol': 6,\n",
       " '##ve': 208,\n",
       " '##olar': 10,\n",
       " '##ro': 93,\n",
       " '##lass': 4,\n",
       " '##ification': 75,\n",
       " '##day': 2,\n",
       " '##class': 63,\n",
       " '##ified': 27,\n",
       " '##no': 54,\n",
       " '##mic': 91,\n",
       " '##zie': 1,\n",
       " '##uring': 7,\n",
       " '##ego': 54,\n",
       " '##rization': 67,\n",
       " '##rad': 9,\n",
       " '##iance': 134,\n",
       " '##zed': 189,\n",
       " '##laus': 2,\n",
       " '##ef': 50,\n",
       " '##ciency': 10,\n",
       " '##ace': 134,\n",
       " '##rba': 5,\n",
       " '##tes': 40,\n",
       " '##lates': 19,\n",
       " '##isms': 10,\n",
       " '##ifies': 56,\n",
       " '##ert': 50,\n",
       " '##zer': 33,\n",
       " '##ting': 132,\n",
       " '##eptive': 43,\n",
       " '##spar': 61,\n",
       " '##cular': 57,\n",
       " '##ines': 13,\n",
       " '##gb': 96,\n",
       " '##gate': 14,\n",
       " '##ht': 17,\n",
       " '##berg': 18,\n",
       " '##del': 13,\n",
       " '##ich': 25,\n",
       " '##ou': 26,\n",
       " '##iti': 61,\n",
       " '##ate': 251,\n",
       " '##now': 5,\n",
       " '##trust': 9,\n",
       " '##ema': 32,\n",
       " '##tica': 4,\n",
       " '##mark': 81,\n",
       " '##written': 61,\n",
       " '##sal': 8,\n",
       " '##hh': 4,\n",
       " '##dar': 10,\n",
       " '##tr': 17,\n",
       " '##ratic': 96,\n",
       " '##ya': 44,\n",
       " '##pu': 72,\n",
       " '##nov': 35,\n",
       " '##ment': 246,\n",
       " '##ations': 67,\n",
       " '##ap': 93,\n",
       " '##ining': 10,\n",
       " '##ples': 15,\n",
       " '##tain': 9,\n",
       " '##lating': 13,\n",
       " '##cie': 40,\n",
       " '##neck': 42,\n",
       " '##unda': 46,\n",
       " '##nee': 2,\n",
       " '##rc': 57,\n",
       " '##ums': 1,\n",
       " '##tan': 6,\n",
       " '##tial': 2,\n",
       " '##rl': 9,\n",
       " '##mal': 92,\n",
       " '##ructured': 42,\n",
       " '##ova': 5,\n",
       " '##ati': 64,\n",
       " '##lity': 11,\n",
       " '##cs': 87,\n",
       " '##rod': 60,\n",
       " '##ucible': 48,\n",
       " '##led': 28,\n",
       " '##nr': 72,\n",
       " '##ne': 106,\n",
       " '##gative': 36,\n",
       " '##ility': 31,\n",
       " '##pl': 263,\n",
       " '##icative': 51,\n",
       " '##len': 35,\n",
       " '##gt': 24,\n",
       " '##urs': 102,\n",
       " '##mut': 94,\n",
       " '##rim': 251,\n",
       " '##ina': 166,\n",
       " '##ments': 21,\n",
       " '##space': 11,\n",
       " '##rated': 30,\n",
       " '##gi': 25,\n",
       " '##anza': 2,\n",
       " '##z': 109,\n",
       " '##pan': 50,\n",
       " '##elled': 10,\n",
       " '##osing': 19,\n",
       " '##sur': 15,\n",
       " '##face': 35,\n",
       " '##tric': 37,\n",
       " '##marks': 142,\n",
       " '##gm': 42,\n",
       " '##cc': 129,\n",
       " '##lusion': 56,\n",
       " '##lar': 105,\n",
       " '##na': 18,\n",
       " '##aneous': 34,\n",
       " '##it': 58,\n",
       " '##tm': 45,\n",
       " '##ang': 26,\n",
       " '##21': 444,\n",
       " '##00': 249,\n",
       " '##ctions': 11,\n",
       " '##deh': 3,\n",
       " '##efined': 1,\n",
       " '##imo': 36,\n",
       " '##du': 74,\n",
       " '##pre': 27,\n",
       " '##table': 19,\n",
       " '##la': 58,\n",
       " '##hg': 10,\n",
       " '##our': 76,\n",
       " '##cing': 104,\n",
       " '##ced': 34,\n",
       " '##tors': 122,\n",
       " '##lm': 46,\n",
       " '##cu': 36,\n",
       " '##aux': 2,\n",
       " '##sed': 77,\n",
       " '##aus': 10,\n",
       " '##ole': 13,\n",
       " '##mis': 21,\n",
       " '##ord': 50,\n",
       " '##ering': 36,\n",
       " '##tify': 121,\n",
       " '##fs': 76,\n",
       " '##db': 31,\n",
       " '##0': 3159,\n",
       " '##car': 41,\n",
       " '##rier': 32,\n",
       " '##bm': 49,\n",
       " '##dm': 103,\n",
       " '##path': 55,\n",
       " '##sper': 16,\n",
       " '##sive': 6,\n",
       " '##real': 22,\n",
       " '##iso': 23,\n",
       " '##py': 15,\n",
       " '##ators': 13,\n",
       " '##gence': 51,\n",
       " '##za': 19,\n",
       " '##vert': 63,\n",
       " '##as': 146,\n",
       " '##ln': 79,\n",
       " '##era': 79,\n",
       " '##bilities': 151,\n",
       " '##ft': 71,\n",
       " '##x': 448,\n",
       " '##10': 421,\n",
       " '##51': 340,\n",
       " '##um': 82,\n",
       " '##14': 412,\n",
       " '##uve': 1,\n",
       " '##ign': 5,\n",
       " '##ier': 18,\n",
       " '##sities': 18,\n",
       " '##vac': 7,\n",
       " '##fo': 40,\n",
       " '##und': 21,\n",
       " '##ual': 279,\n",
       " '##au': 17,\n",
       " '##tom': 15,\n",
       " '##ator': 6,\n",
       " '##ul': 57,\n",
       " '##pts': 3,\n",
       " '##sit': 28,\n",
       " '##systems': 16,\n",
       " '##ph': 57,\n",
       " '##nation': 18,\n",
       " '##tron': 6,\n",
       " '##ses': 43,\n",
       " '##oe': 41,\n",
       " '##ctric': 8,\n",
       " '##els': 23,\n",
       " '##tein': 3,\n",
       " '##grade': 34,\n",
       " '##cr': 93,\n",
       " '##yp': 117,\n",
       " '##dgets': 7,\n",
       " '##ste': 31,\n",
       " '##ry': 78,\n",
       " '##ption': 18,\n",
       " '##igate': 84,\n",
       " '##45': 450,\n",
       " '##raphy': 14,\n",
       " '##rous': 3,\n",
       " '##pen': 66,\n",
       " '##et': 124,\n",
       " '##press': 34,\n",
       " '##tten': 9,\n",
       " '##30': 478,\n",
       " '##da': 86,\n",
       " '##wn': 12,\n",
       " '##bank': 23,\n",
       " '##mates': 7,\n",
       " '##craft': 32,\n",
       " '##ura': 58,\n",
       " '##cies': 78,\n",
       " '##lu': 225,\n",
       " '##arable': 20,\n",
       " '##nge': 26,\n",
       " '##imating': 141,\n",
       " '##ons': 168,\n",
       " '##tructing': 23,\n",
       " '##book': 41,\n",
       " '##ok': 8,\n",
       " '##oid': 26,\n",
       " '##max': 52,\n",
       " '##j': 38,\n",
       " '##scan': 8,\n",
       " '##gre': 70,\n",
       " '##bre': 11,\n",
       " '##ulating': 31,\n",
       " '##port': 48,\n",
       " '##mt': 62,\n",
       " '##out': 91,\n",
       " '##ched': 10,\n",
       " '##ase': 5,\n",
       " '##ographic': 71,\n",
       " '##sb': 19,\n",
       " '##ener': 7,\n",
       " '##tri': 81,\n",
       " '##vial': 17,\n",
       " '##by': 16,\n",
       " '##sh': 49,\n",
       " '##ev': 31,\n",
       " '##we': 20,\n",
       " '##oto': 5,\n",
       " '##hedral': 14,\n",
       " '##ten': 66,\n",
       " '##uated': 13,\n",
       " '##ol': 47,\n",
       " '##rent': 10,\n",
       " '##nts': 32,\n",
       " '##aby': 6,\n",
       " '##lane': 9,\n",
       " '##cate': 45,\n",
       " '##nate': 9,\n",
       " '##xt': 8,\n",
       " '##raction': 8,\n",
       " '##ikh': 4,\n",
       " '##word': 43,\n",
       " '##nary': 13,\n",
       " '##red': 80,\n",
       " '##ibility': 71,\n",
       " '##rut': 4,\n",
       " '##ini': 24,\n",
       " '##ml': 30,\n",
       " '##connected': 41,\n",
       " '##lat': 35,\n",
       " '##vity': 28,\n",
       " '##ped': 32,\n",
       " '##hr': 174,\n",
       " '##op': 124,\n",
       " '##omo': 7,\n",
       " '##rp': 47,\n",
       " '##hism': 4,\n",
       " '##text': 31,\n",
       " '##hit': 3,\n",
       " '##rus': 13,\n",
       " '##ulated': 26,\n",
       " '##iable': 57,\n",
       " '##hui': 2,\n",
       " '##kai': 2,\n",
       " '##guide': 9,\n",
       " '##df': 21,\n",
       " '##il': 18,\n",
       " '##ter': 90,\n",
       " '##plane': 9,\n",
       " '##oo': 6,\n",
       " '##pile': 12,\n",
       " '##brate': 3,\n",
       " '##train': 46,\n",
       " '##grad': 20,\n",
       " '##uss': 34,\n",
       " '##ians': 35,\n",
       " '##tian': 1,\n",
       " '##hra': 3,\n",
       " '##bian': 2,\n",
       " '##20': 476,\n",
       " '##man': 84,\n",
       " '##gs': 60,\n",
       " '##tar': 7,\n",
       " '##get': 12,\n",
       " '##lea': 43,\n",
       " '##bic': 15,\n",
       " '##omic': 33,\n",
       " '##ctor': 34,\n",
       " '##us': 179,\n",
       " '##ista': 2,\n",
       " '##sis': 52,\n",
       " '##ac': 62,\n",
       " '##rf': 16,\n",
       " '##ona': 8,\n",
       " '##am': 61,\n",
       " '##bles': 5,\n",
       " '##lean': 59,\n",
       " '##isson': 48,\n",
       " '##lli': 17,\n",
       " '##folding': 11,\n",
       " '##rro': 39,\n",
       " '##bor': 52,\n",
       " '##ero': 205,\n",
       " '##gen': 251,\n",
       " '##eit': 49,\n",
       " '##eous': 167,\n",
       " '##pate': 19,\n",
       " '##uded': 1,\n",
       " '##ude': 4,\n",
       " '##nik': 3,\n",
       " '##von': 4,\n",
       " '##kis': 13,\n",
       " '##hot': 21,\n",
       " '##ple': 29,\n",
       " '##points': 34,\n",
       " '##pp': 41,\n",
       " '##bin': 9,\n",
       " '##cut': 16,\n",
       " '##bi': 99,\n",
       " '##gu': 2,\n",
       " '##was': 5,\n",
       " '##qual': 33,\n",
       " '##abe': 2,\n",
       " '##ups': 23,\n",
       " '##ware': 83,\n",
       " '##sas': 17,\n",
       " '##mbled': 4,\n",
       " '##aries': 49,\n",
       " '##tf': 28,\n",
       " '##16': 413,\n",
       " '##aco': 14,\n",
       " '##sen': 64,\n",
       " '##sor': 40,\n",
       " '##lled': 17,\n",
       " '##uta': 3,\n",
       " '##pe': 49,\n",
       " '##lly': 25,\n",
       " '##tang': 11,\n",
       " '##bit': 93,\n",
       " '##truct': 86,\n",
       " '##rou': 15,\n",
       " '##tine': 19,\n",
       " '##stein': 28,\n",
       " '##ena': 15,\n",
       " '##ivity': 97,\n",
       " '##iv': 24,\n",
       " '##rag': 21,\n",
       " '##glers': 13,\n",
       " '##ir': 32,\n",
       " '##gler': 5,\n",
       " '##bolic': 16,\n",
       " '##des': 28,\n",
       " '##ically': 16,\n",
       " '##tail': 26,\n",
       " '##pm': 13,\n",
       " '##fields': 3,\n",
       " '##lle': 3,\n",
       " '##my': 20,\n",
       " '##gating': 40,\n",
       " '##band': 65,\n",
       " '##vn': 23,\n",
       " '##tens': 24,\n",
       " '##lier': 104,\n",
       " '##oa': 14,\n",
       " '##tile': 6,\n",
       " '##cm': 16,\n",
       " '##lt': 18,\n",
       " '##ime': 29,\n",
       " '##logies': 45,\n",
       " '##erated': 25,\n",
       " '##ab': 12,\n",
       " '##oration': 4,\n",
       " '##ka': 15,\n",
       " '##iter': 3,\n",
       " '##script': 21,\n",
       " '##tation': 21,\n",
       " '##keepers': 3,\n",
       " '##ulate': 34,\n",
       " '##bia': 54,\n",
       " '##ru': 73,\n",
       " '##are': 10,\n",
       " '##nko': 2,\n",
       " '##rt': 17,\n",
       " '##tu': 16,\n",
       " '##dina': 20,\n",
       " '##fine': 85,\n",
       " '##sam': 74,\n",
       " '##pled': 12,\n",
       " '##fin': 37,\n",
       " '##ite': 39,\n",
       " '##gon': 32,\n",
       " '##ometric': 20,\n",
       " '##rice': 1,\n",
       " '##fia': 53,\n",
       " '##formed': 36,\n",
       " '##ver': 23,\n",
       " '##ground': 50,\n",
       " '##inating': 18,\n",
       " '##oni': 106,\n",
       " '##uration': 24,\n",
       " '##gg': 39,\n",
       " '##chi': 28,\n",
       " '##tz': 32,\n",
       " '##dal': 113,\n",
       " '##cted': 55,\n",
       " '##ential': 37,\n",
       " '##ios': 8,\n",
       " '##cratic': 5,\n",
       " '##eti': 28,\n",
       " '##wi': 31,\n",
       " '##dt': 36,\n",
       " '##mar': 148,\n",
       " '##ies': 119,\n",
       " '##ien': 75,\n",
       " '##yan': 3,\n",
       " '##id': 81,\n",
       " '##analysis': 6,\n",
       " '##bble': 6,\n",
       " '##ives': 2,\n",
       " '##med': 17,\n",
       " '##mate': 27,\n",
       " '##rial': 13,\n",
       " '##lia': 45,\n",
       " '##ost': 37,\n",
       " '##vers': 59,\n",
       " '##aria': 20,\n",
       " '##play': 34,\n",
       " '##ore': 15,\n",
       " '##tical': 20,\n",
       " '##ast': 17,\n",
       " '##sm': 56,\n",
       " '##iste': 57,\n",
       " '##actic': 12,\n",
       " '##gy': 35,\n",
       " '##tized': 46,\n",
       " '##ile': 34,\n",
       " '##at': 37,\n",
       " '##urized': 2,\n",
       " '##met': 66,\n",
       " '##ries': 39,\n",
       " '##tters': 2,\n",
       " '##entation': 10,\n",
       " '##imeter': 35,\n",
       " '##plify': 44,\n",
       " '##cript': 141,\n",
       " '##lor': 1,\n",
       " '##inated': 6,\n",
       " '##yl': 24,\n",
       " '##otype': 11,\n",
       " '##rone': 24,\n",
       " '##ject': 43,\n",
       " '##pic': 34,\n",
       " '##ept': 7,\n",
       " '##layer': 43,\n",
       " '##ion': 219,\n",
       " '##tina': 24,\n",
       " '##bet': 7,\n",
       " '##tino': 5,\n",
       " '##pathy': 7,\n",
       " '##vo': 197,\n",
       " '##off': 91,\n",
       " '##hang': 2,\n",
       " '##xie': 5,\n",
       " '##run': 2,\n",
       " '##eration': 2,\n",
       " '##ces': 87,\n",
       " '##agi': 5,\n",
       " '##aris': 4,\n",
       " '##fus': 43,\n",
       " '##cation': 72,\n",
       " '##ge': 71,\n",
       " '##dim': 70,\n",
       " '##ional': 78,\n",
       " '##uso': 13,\n",
       " '##idal': 17,\n",
       " '##mas': 12,\n",
       " '##vb': 7,\n",
       " '##ume': 29,\n",
       " '##sca': 54,\n",
       " '##erate': 17,\n",
       " '##bs': 55,\n",
       " '##flow': 169,\n",
       " '##link': 154,\n",
       " '##stan': 17,\n",
       " '##tia': 11,\n",
       " '##tails': 9,\n",
       " '##dc': 28,\n",
       " '##dating': 34,\n",
       " '##iers': 25,\n",
       " '##ological': 34,\n",
       " '##sic': 2,\n",
       " '##uit': 9,\n",
       " '##iciencies': 9,\n",
       " '##itis': 4,\n",
       " '##pot': 10,\n",
       " '##using': 12,\n",
       " '##ising': 25,\n",
       " '##work': 31,\n",
       " '##rar': 29,\n",
       " '##ily': 43,\n",
       " '##osa': 5,\n",
       " '##rin': 11,\n",
       " '##wl': 16,\n",
       " '##15': 428,\n",
       " '##mers': 9,\n",
       " '##wave': 89,\n",
       " '##listic': 20,\n",
       " '##nse': 5,\n",
       " '##ll': 38,\n",
       " '##gan': 79,\n",
       " '##ered': 18,\n",
       " '##illa': 2,\n",
       " '##rity': 13,\n",
       " '##me': 76,\n",
       " '##ia': 115,\n",
       " '##vic': 16,\n",
       " '##cio': 2,\n",
       " '##vision': 9,\n",
       " '##vi': 31,\n",
       " '##coming': 31,\n",
       " '##matic': 35,\n",
       " '##ves': 52,\n",
       " '##drop': 36,\n",
       " '##ell': 27,\n",
       " '##ff': 40,\n",
       " '##ffs': 2,\n",
       " '##apple': 3,\n",
       " '##fr': 39,\n",
       " '##sse': 13,\n",
       " '##minate': 5,\n",
       " '##ai': 14,\n",
       " '##osi': 39,\n",
       " '##tori': 36,\n",
       " '##gement': 2,\n",
       " '##fle': 19,\n",
       " '##rator': 27,\n",
       " '##sy': 35,\n",
       " '##tics': 13,\n",
       " '##sr': 40,\n",
       " ...}"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_used_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4274"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['internet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_extend.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rapid popularity of internet of things ( iot ) and cloud computing permits neuroscientists to collect multilevel and multichannel brain data to better understand brain functions , diagnose diseases , and devise treatments .',\n",
       " 'to ensure secure and reliable data communication between end-to-end ( e2e ) devices supported by current iot and cloud infrastructure , trust management is needed at the iot and user ends .',\n",
       " 'this paper introduces a neuro-fuzzy based brain-inspired trust management model ( tmm ) to secure iot devices and relay nodes , and to ensure data reliability .',\n",
       " 'the proposed tmm utilizes node behavioral trust and data trust estimated using adaptive neuro-fuzzy inference system and weighted-additive methods respectively to assess the nodes trustworthiness .',\n",
       " 'in contrast to the existing fuzzy based tmms , the ns2 simulation results confirm the robust and accuracy of the proposed tmm in identifying malicious nodes in the communication network .',\n",
       " 'with the growing usage of cloud based iot frameworks in neuroscience research , integrating the proposed tmm into the existing infrastructure will assure secure and reliable data communication among the e2e devices .',\n",
       " 'in this paper , we address the problem of computing optimal paths through three consecutive points for the curvature-constrained forward moving dubins vehicle .',\n",
       " 'given initial and final configurations of the dubins vehicle , and a midpoint with an unconstrained heading , the objective is to compute the midpoint heading that minimizes the total dubins path length .',\n",
       " \"we provide a novel geometrical analysis of the optimal path , and establish new properties of the optimal dubins ' path through three points .\",\n",
       " 'we then show how our method can be used to quickly refine dubins tsp tours produced using newest techniques .',\n",
       " 'we also provide extensive simulation results showing the improvement of the proposed approach in both runtime and solution quality over the conventional method of uniform discretization of the heading at the mid-point , followed by solving the minimum dubins path for each discrete heading .',\n",
       " 'high quality upsampling of sparse 3d point clouds is critically useful for a wide range of geometric operations such as reconstruction , rendering , meshing , and analysis .',\n",
       " 'in this paper , we propose a data-driven algorithm that enables an upsampling of 3d point clouds without the need for hard-coded rules .',\n",
       " 'our approach uses a deep network with chamfer distance as the loss function , capable of learning the hidden features in point clouds belonging to different object categories .',\n",
       " 'we evaluate our algorithm across different amplification factors , with upsampling learned and performed on objects belonging to the same category as well as different categories .',\n",
       " 'we also explore the desirable characteristics of input point clouds as a function of the distribution of the point samples .',\n",
       " 'finally , we demonstrate the performance of our algorithm in single-category training versus multi-category training scenarios .',\n",
       " 'the final proposed model is compared against a baseline , optimization-based upsampling method .',\n",
       " 'results indicate that our algorithm is capable of generating more uniform and accurate upsamplings .',\n",
       " 'internet is the main source of information nowadays .',\n",
       " 'the search engines must have various alternative manners for the search results representation .',\n",
       " 'these representation methods will enable the end users especially the visually impaired vi web searchers to access the information on the web .',\n",
       " 'the aim of this paper is design , evaluate and improve the interface for the vi users to perform search and browse results .',\n",
       " 'this attempt provides a new accessibility tool for the vi web searchers .',\n",
       " 'the conceptual modelling technique proposed in this paper is based on the formal concept analysis fca that hides the detailed information for the collected data results .',\n",
       " 'this approach highlights the main discovered concepts to be focused on .',\n",
       " 'that is combined with context interactive navigation , in an interface called interactive search engine ( interactse ) , which minimize the time and effort required by the vi users .',\n",
       " 'there is no standardised set of guidelines or heuristics , which can be used for the evaluation of usability and accessibility aspects of such an interface .',\n",
       " 'therefore , interactse was evaluated with experts using nielsen heuristics and web content accessibility guidelines wcag 2.0 in terms of both usability and accessibility .',\n",
       " 'the analysis was carried out based on the number of usability problems identified and their average severity ratings .',\n",
       " 'the results show that the most frequently violated heuristics from the nielsen set are consistency and documentation .',\n",
       " 'the average severity rating of all the problems found using nielsen set is minor .',\n",
       " 'the results also show that the most frequently violated wcag 2.0 guidelines are distinguishable , followed by navigable and affordance .',\n",
       " 'the average severity rating of all the problems found using wcag 2.0 guidelines is also minor .',\n",
       " 'the results show that nielsen heuristics and wcag 2.0 guidelines both contributed in identifying a number of usability problems .',\n",
       " 'automated facial expression recognition ( fer ) has been a challenging task for decades .',\n",
       " 'many of the existing works use hand-crafted features such as lbp , hog , lpq , and histogram of optical flow ( hof ) combined with classify such as support vector machines for expression recognition .',\n",
       " 'these methods often require rigorous hyperparameter tuning to achieve good results .',\n",
       " 'recently deep neural networks ( dnn ) have shown to better traditional methods in visual object recognition .',\n",
       " 'in this paper , we propose a two-part network consisting of a dnn-based architecture followed by a conditional random field ( crf ) module for facial expression recognition in videos .',\n",
       " 'the first part captures the spatial relation within facial images using technical layers followed by three inception-resnet modules and two fully-connected layers .',\n",
       " 'to capture the temporal relation between the image frames , we use linear chain crf in the second part of our network .',\n",
       " 'we evaluate our proposed network on three publicly available databases , viz .',\n",
       " 'ck+ , mmi , and fera .',\n",
       " 'experiments are performed in subject-independent and cross-database manners .',\n",
       " 'our experimental results show that cascading the deep network architecture with the crf module considerably increases the recognition of facial expressions in videos and in particular it better the newest methods in the cross-database experiments and yields comparable results in the subject-independent experiments .',\n",
       " 'this paper proposes the continuous semantic topic technical model ( cstem ) which finds hidden topic variables in documents using continuous semantic distance function between the topics and the words by means of the variation autoencoder ( vae ) .',\n",
       " 'the semantic distance could be represented by any symmetric bell-shaped geometric distance function on the euclidean space , for which the mahalanobis distance is used in this paper .',\n",
       " 'in order for the semantic distance to perform more properly , we newly introduce an additional model parameter for each word to take out the global factor from this distance indicating how likely it occurs regardless of its topic .',\n",
       " 'it certainly improves the problem that the normal distribution which is used in previous topic model with continuous word technical could not explain the semantic relation correctly and helps to obtain the higher topic coherence .',\n",
       " 'through the experiments with the data of 20 newsgroup , nips papers and cnn/dailymail corpus , the performance of the recent newest models is accomplished by our model as well as generating topic technical vectors which makes possible to observe where the topic vectors are embedded with the word vectors in the real euclidean space and how the topics are related each other semantically .',\n",
       " 'existing deep multitask learning ( mtl ) approaches align layers shared between tasks in a parallel ordering .',\n",
       " 'such an organization significantly constricts the types of shared structure that can be learned .',\n",
       " 'the necessity of parallel ordering for deep mtl is first tested by comparing it with permuted ordering of shared layers .',\n",
       " 'the results indicate that a flexible ordering can enable more effective sharing , thus motivating the development of a soft ordering approach , which learns how shared layers are applied in different ways for different tasks .',\n",
       " 'deep mtl with soft ordering better parallel ordering methods across a series of domains .',\n",
       " 'these results suggest that the power of deep mtl comes from learning highly general building blocks that can be assembled to meet the demands of each task .',\n",
       " 'in this paper we explore the use of electrical biosignals measured on scalp and corresponding to mental relaxation and concentration tasks in order to control an object in a video game .',\n",
       " 'to evaluate the requirements of such a system in terms of sensors and signal processing we compare two designs .',\n",
       " 'the first one uses only one scalp electroencephalographic ( eeg ) electrode and the power in the alpha frequency band .',\n",
       " 'the second one uses sixteen scalp eeg electrodes and machine learning methods .',\n",
       " 'the role of muscular activity is also evaluated using five electrodes positioned on the face and the neck .',\n",
       " 'results show that the first design enabled 70 % of the participants to successfully control the game , whereas 100 % of the participants managed to do it with the second design based on machine learning .',\n",
       " 'subjective questionnaires confirm these results : users globally felt to have control in both designs , with an increased feeling of control in the second one .',\n",
       " 'offline analysis of face and neck muscle activity shows that this activity could also be used to distinguish between relaxation and concentration tasks .',\n",
       " 'results suggest that the combination of muscular and brain activity could improve performance of this kind of system .',\n",
       " 'they also suggest that muscular activity has probably been recorded by eeg electrodes .',\n",
       " 'how spiking networks are able to perform probability inference is an intriguing question , not only for understanding information processing in the brain , but also for transferring these computational principles to neuromorphic silicon circuits .',\n",
       " 'a number of computationally powerful spiking network models have been proposed , but most of them have only been tested , under ideal conditions , in software simulations .',\n",
       " 'any implementation in an analog , physical system , be it in vivo or in silico , will generally lead to distorted dynamics due to the physical properties of the underlying substrate .',\n",
       " 'in this paper , we discuss several such distortive effects that are difficult or impossible to remove by classical calibration routines or parameter training .',\n",
       " 'we then argue that hierarchical networks of leaky integrate-and-fire neurons can offer the required robust for physical implementation and demonstrate this with both software simulations and emulation on an accelerated analog neuromorphic device .',\n",
       " 'low-density parity-check ( ldpc ) codes on symmetric memoryless channels have been analyzed using statistical physics by several authors .',\n",
       " 'in this paper , statistical mechanical analysis of ldpc codes is performed for asymmetric memoryless channels and general markov channels .',\n",
       " 'it is shown that the saddle point equations of the replica symmetric solution for a markov channel is equivalent to the density evolution of the belief propagation on the factor graph representing ldpc codes on the markov channel .',\n",
       " 'the derivation uses the method of types for markov chain .',\n",
       " 'mobile agent networks , such as multi-uav systems , are constrained by limited resources .',\n",
       " 'in particular , limited energy affects system performance directly , such as system lifetime .',\n",
       " 'it has been demonstrated in the wireless sensor network literature that the communication energy consumption dominates the computational and the sensing energy consumption .',\n",
       " 'hence , the lifetime of the multi-uav systems can be extended significantly by optimizing the amount of communication data , at the expense of increasing computational cost .',\n",
       " 'in this work , we aim at attaining an optimal trade-off between the communication and the computational energy .',\n",
       " 'specifically , we propose a mixed-integer optimization formulation for a multi-hop hierarchical clustering-based self-organizing uav network incorporating data aggregation , to obtain an energy-efficient information routing scheme .',\n",
       " 'the proposed framework is tested on two applications , namely target tracking and area mapping .',\n",
       " 'based on simulation results , our method can significantly save energy compared to a baseline strategy , where there is no data aggregation and cluster scheme .',\n",
       " 'inspired by speech recognition , recent newest algorithms mostly consider scene text recognition as a sequence prediction problem .',\n",
       " 'though achieving excellent performance , these methods usually neglect an important fact that text in images are actually distributed in two-dimensional space .',\n",
       " 'it is a nature quite different from that of speech , which is essentially a one-dimensional signal .',\n",
       " 'in principle , directly compressing features of text into a one-dimensional form may lose useful information and introduce extra noise .',\n",
       " 'in this paper , we approach scene text recognition from a two-dimensional perspective .',\n",
       " 'a simple yet effective model , called character attention fully technical network ( ca-fcn ) , is devised for recognizing the text of arbitrary shapes .',\n",
       " 'scene text recognition is realized with a semantic segment network , where an attention mechanism for characters is adopted .',\n",
       " 'combined with a word formation module , ca-fcn can simultaneously recognize the script and predict the position of each character .',\n",
       " 'experiments demonstrate that the proposed algorithm better previous methods on both regular and irregular text data .',\n",
       " 'moreover , it is proven to be more robust to imprecise localizations in the text detection phase , which are very common in practice .',\n",
       " 'a neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data .',\n",
       " 'a plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong .',\n",
       " 'generate models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel , out-of-distribution inputs .',\n",
       " 'in this paper we challenge this assumption .',\n",
       " 'we find that the density learned by flow-based models , vaes , and pixelcnns can not distinguish images of common objects such as dogs , trucks , and horses ( i.e .',\n",
       " 'cifar-10 ) from those of house numbers ( i.e .',\n",
       " 'svhn ) , assigning a higher likelihood to the latter when the model is trained on the former .',\n",
       " 'moreover , we find evidence of this phenomenon when pairing several popular image data sets : fashionmnist vs mnist , celeba vs svhn , imagenet vs cifar-10 / cifar-100 / svhn .',\n",
       " 'to investigate this curious behavior , we focus analysis on flow-based generate models in particular since they are trained and evaluated via the exact marginal likelihood .',\n",
       " 'we find such behavior persists even when we restrict the flows to constant-volume transformations .',\n",
       " 'these transformations admit some theoretical analysis , and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature .',\n",
       " 'our results caution against using the density estimates from deep generate models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood .',\n",
       " 'in this paper , we propose a design solution for the implementation of virtualized network coding functionality ( vncf ) over a service coverage area .',\n",
       " 'network function virtualization ( nfv ) and network coding ( nc ) architectural designs are integrated as a toolbox of nc design domains so that nc can be implemented over different underlying physical networks including satellite or hybrid networks .',\n",
       " 'the design includes identifying theoretical limits of nc over wireless networks in terms of achievable rate region and optimizing coding rates for nodes that implement vncf .',\n",
       " 'the overall design target is to achieve a given multicast transmission target reliability at receiver sides .',\n",
       " 'in addition , the optimization problem uses databases with geo-tagged link statistics and geo-location information of network nodes in the deployment area for some computational complexity/energy constraints .',\n",
       " 'numerical results provide validation of our design solution on how network conditions and system constraints impact the design and implementation of nc and how vncf allows reliable communication over wireless networks with reliability and connectivity up to theoretical limits .',\n",
       " \"we propose a method that combines signals from many brain regions observed in functional magnetic resonance imaging ( fmri ) to predict the subject 's behavior during a scanning session .\",\n",
       " 'such predictions suffer from the huge number of brain regions sampled on the voxel grid of standard fmri data sets : the curse of dimensionality .',\n",
       " 'dimensionality reduction is thus needed , but it is often performed using a univariate feature selection procedure , that handles neither the spatial structure of the images , nor the multivariate nature of the signal .',\n",
       " 'by introducing a hierarchical cluster of the brain volume that incorporates connectivity constraints , we reduce the span of the possible spatial configurations to a single tree of nested regions tailored to the signal .',\n",
       " 'we then prune the tree in a supervised setting , hence the name supervised cluster , in order to extract a parcellation ( division of the volume ) such that parcel-based signal averages best predict the target information .',\n",
       " 'dimensionality reduction is thus achieved by feature agglomeration , and the constructed features now provide a multi-scale representation of the signal .',\n",
       " 'comparisons with reference methods on both simulated and real data show that our approach yields higher prediction accuracy than standard voxel-based approaches .',\n",
       " 'moreover , the method infers an explicit weighting of the regions involved in the regression or classification task .',\n",
       " 'as global political preeminence gradually shifted from the united kingdom to the united states , so did the capacity to culturally influence the rest of the world .',\n",
       " 'in this work , we analyze how the world-wide varieties of written english are evolving .',\n",
       " 'we study both the spatial and temporal variations of vocabulary and spelling of english using a large corpus of geolocated tweets and the google books data corresponding to books published in the us and the uk .',\n",
       " 'the advantage of our approach is that we can address both standard written language ( google books ) and the more colloquial forms of microblogging messages ( twitter ) .',\n",
       " 'we find that american english is the dominant form of english outside the uk and that its influence is felt even within the uk borders .',\n",
       " 'finally , we analyze how this trend has evolved over time and the impact that some cultural events have had in shaping it .',\n",
       " 'a battery swapping and charging station ( bscs ) is an energy refueling station , where i ) electric vehicles ( evs ) with depleted batteries ( dbs ) can swap their dbs for fully-charged ones , and ii ) the swapped dbs are then charged until they are fully-charged .',\n",
       " 'successful deployment of a bscs system necessitates a careful planning of swapping- and charging-related infrastructures , and thus a comprehensive performance evaluation of the bscs is becoming crucial .',\n",
       " 'this paper studies such a performance evaluation problem with a novel mixed queueing network ( mqn ) model and validates this model with extensive numerical simulation .',\n",
       " \"we adopt the evs ' blocking probability as our quality-of-service measure and focus on studying the impact of the key parameters of the bscs ( e.g. , the numbers of parking spaces , swapping islands , chargers , and batteries ) on the blocking probability .\",\n",
       " 'we prove a necessary and sufficient condition for showing the ergodicity of the mqn when the number of batteries approaches infinity , and further prove that the blocking probability has two different types of asymptotic behaviors .',\n",
       " 'meanwhile , for each type of asymptotic behavior , we analytically derive the asymptotic lower bound of the blocking probability .',\n",
       " 'the success of graph technical or node representation learning in a variety of downstream tasks , such as node classification , link prediction , and recommendation systems , has led to their popularity in recent years .',\n",
       " 'representation learning algorithms aim to preserve local and global network structure by identifying node neighborhood notions .',\n",
       " 'however , many existing algorithms generate technical that fail to properly preserve the network structure , or lead to unstable representations due to random processes ( e.g. , random walks to generate context ) and , thus , can not generate to multi-graph problems .',\n",
       " 'in this paper , we propose recs , a novel , stable graph technical algorithmic framework .',\n",
       " 'recs learns graph representations using connection subgraphs by employing the analogy of graphs with electrical circuits .',\n",
       " 'it preserves both local and global connectivity patterns , and addresses the issue of high-degree nodes .',\n",
       " 'further , it exploits the strength of weak ties and meta-data that have been neglected by baselines .',\n",
       " 'the experiments show that recs better newest algorithms by up to 36.85 % on multi-label classification problem .',\n",
       " 'further , in contrast to baselines , recs , being deterministic , is completely stable .',\n",
       " 'in this paper we propose right-angled artin groups as a platform for secret sharing schemes based on the efficiency ( linear time ) of the word problem .',\n",
       " 'inspired by previous work of grigoriev-shpilrain in the context of graphs , we define two new problems : subgroup isomorphism problem and group homomorphism problem .',\n",
       " 'based on them , we also propose two new authentication schemes .',\n",
       " 'for right-angled artin groups , the group homomorphism and graph homomorphism problems are equivalent , and the later is known to be np-complete .',\n",
       " 'in the case of the subgroup isomorphism problem , we bring some results due to bridson who shows there are right-angled artin groups in which this problem is unsolvable .',\n",
       " 'publishing articles in high-impact english journals is difficult for scholars around the world , especially for non-native english-speaking scholars ( nness ) , most of whom struggle with proficiency in english .',\n",
       " 'in order to uncover the differences in english scientific writing between native english-speaking scholars ( ness ) and nness , we collected a large data set containing more than 150,000 full-text articles published in plos between 2006 and 2015 .',\n",
       " 'we divided these articles into three groups according to the ethnic backgrounds of the first and corresponding authors , obtained by ethnea , and examined the scientific writing styles in english from a two-fold perspective of linguistic complexity : ( 1 ) syntactic complexity , including measurements of sentence length and sentence complexity ; and ( 2 ) lexical complexity , including measurements of lexical diversity , lexical density , and lexical sophistication .',\n",
       " 'the observations suggest marginal differences between groups in syntactical and lexical complexity .',\n",
       " 'centrality is an important notion in complex networks ; it could be used to characterize how influential a node or an edge is in the network .',\n",
       " 'it plays an important role in several other network analysis tools including community detection .',\n",
       " 'even though there are a small number of axiomatic frameworks associated with this notion , the existing formalizations are not generic in nature .',\n",
       " 'in this paper we propose a generic axiomatic framework to capture all the intrinsic properties of a centrality measure ( a.k.a . centrality index ) .',\n",
       " 'we analyze popular centrality measures along with other novel measures of centrality using this framework .',\n",
       " 'we observed that none of the centrality measures considered satisfies all the axioms .',\n",
       " 'reconstruction of signals from compressively sensed measurements is an ill-posed problem .',\n",
       " 'in this paper , we leverage the technical generate model , ride , as an image prior for compressive image reconstruction .',\n",
       " 'technical networks can model long-range dependencies in images and hence are suitable to handle global multiplexing in reconstruction from compressive imaging .',\n",
       " 'we perform map inference with ride using back-propagation to the inputs and projected gradient method .',\n",
       " 'we propose an entropy thresholding based approach for preserving texture in images well .',\n",
       " 'our approach shows superior reconstructions compared to recent global reconstruction approaches like d-amp and tval3 on both simulated and real data .',\n",
       " 'in the last decade , social media has evolved as one of the leading platform to create , share , or exchange information ; it is commonly used as a way for individuals to maintain social connections .',\n",
       " 'in this online digital world , people use to post texts or pictures to express their views socially and create user-user engagement through discussions and conversations .',\n",
       " 'thus , social media has established itself to bear signals relating to human behavior .',\n",
       " \"one can easily design user characteristic network by scraping through someone 's social media profiles .\",\n",
       " 'in this paper , we investigate the potential of social media in characterizing and understanding predominant drunk texters from the perspective of their social , psychological and linguistic behavior as evident from the content generated by them .',\n",
       " 'our research aims to analyze the behavior of drunk texters on social media and to contrast this with non-drunk texters .',\n",
       " 'we use twitter social media to obtain the set of drunk texters and non-drunk texters and show that we can classify users into these two respective sets using various psycholinguistic features with an overall average accuracy of 96.78 % with very high precision and recall .',\n",
       " 'note that such an automatic classification can have far-reaching impact - ( i ) on health research related to addiction prevention and control , and ( ii ) in eliminating abusive and vulgar contents from twitter , borne by the tweets of drunk texters .',\n",
       " 'this paper explores the potential of extreme learning machine based supervised classification algorithm for land cover classification .',\n",
       " 'in comparison to a backpropagation neural network , which requires setting of several user-defined parameters and may produce local minima , extreme learning machine require setting of one parameter and produce a unique solution .',\n",
       " 'etm+ multispectral data set ( england ) was used to judge the suitability of extreme learning machine for remote sensing classifications .',\n",
       " 'a back propagation neural network was used to compare its performance in term of classification accuracy and computational cost .',\n",
       " 'results suggest that the extreme learning machine perform equally well to back propagation neural network in term of classification accuracy with this data set .',\n",
       " 'the computational cost using extreme learning machine is very small in comparison to back propagation neural network .',\n",
       " 'end-to-end models for goal-orientated dialogue are challenging to train , because linguistic and strategic aspects are entangled in hidden state vectors .',\n",
       " 'we introduce an approach to learning representations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions , which decouples the semantics of the dialogue utterance from its linguistic realization .',\n",
       " 'we then use these hidden sentence representations for hierarchical language generation , planning and reinforcement learning .',\n",
       " 'experiments show that our approach increases the end-task reward achieved by the model , improves the effectiveness of long-term planning using rollouts , and allows self-play reinforcement learning to improve decision making without diverging from human language .',\n",
       " 'our hierarchical latent-variable model better previous work both linguistically and strategically .',\n",
       " 'we design a new approach that allows robot learning of new activities from unlabeled human example videos .',\n",
       " \"given videos of humans executing the same activity from a human 's viewpoint ( i.e. , first-person videos ) , our objective is to make the robot learn the temporal structure of the activity as its future regression network , and learn to transfer such model for its own motor execution .\",\n",
       " 'we present a new deep learning model : we extend the newest technical object detection network for the representation/estimation of human hands in training videos , and newly introduce the concept of using a fully technical network to regress ( i.e. , predict ) the intermediate scene representation corresponding to the future frame ( e.g. , 1-2 seconds later ) .',\n",
       " 'combining these allows direct prediction of future locations of human hands and objects , which enables the robot to infer the motor control plan using our manipulation network .',\n",
       " 'we experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible , and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input .',\n",
       " 'in future internet it is possible to change elements of congestion control in order to eliminate jitter and batch loss caused by the current control mechanisms based on packet loss events .',\n",
       " 'we investigate the fundamental problem of adjusting sending rates to achieve optimal utilization of highly variable bandwidth of a network path using accurate packet rate information .',\n",
       " 'this is done by continuously controlling the sending rate with a function of the measured packet rate at the receiver .',\n",
       " 'we propose the relative loss of packet rate between the sender and the receiver ( relative rate reduction , rrr ) as a new accurate and continuous measure of congestion of a network path , replacing the erratically fluctuating packet loss .',\n",
       " 'we demonstrate that with choosing various rrr based feedback functions the optimum is reached with adjustable congestion level .',\n",
       " 'the proposed method guarantees fair bandwidth sharing of competitive flows .',\n",
       " 'finally , we present testbed experiments to demonstrate the performance of the algorithm .',\n",
       " \"software quality in use comprises quality from the user 's perspective .\",\n",
       " 'it has gained its importance in e-government applications , mobile-based applications , embedded systems , and even business process development .',\n",
       " \"user 's decisions on software acquisitions are often ad hoc or based on preference due to difficulty in quantitatively measuring software quality in use .\",\n",
       " 'but , why is quality-in-use measurement difficult ?',\n",
       " \"although there are many software quality models , to the authors ' knowledge no works survey the challenges related to software quality-in-use measurement .\",\n",
       " 'this article has two main contributions : 1 ) it identifies and explains major issues and challenges in measuring software quality in use in the context of the iso square series and related software quality models and highlights open research areas ; and 2 ) it sheds light on a research direction that can be used to predict software quality in use .',\n",
       " 'in short , the quality-in-use measurement issues are related to the complexity of the current standard models and the limitations and incompleteness of the customized software quality models .',\n",
       " 'a sentiment analysis of software reviews is proposed to deal with these issues .',\n",
       " 'static type errors are a common stumbling block for newcomers to typed functional languages .',\n",
       " 'we present a dynamic approach to explaining type errors by generating counterexample witness inputs that illustrate how an ill-typed program goes wrong .',\n",
       " 'first , given an ill-typed function , we symbolically execute the body to synthesize witness values that make the program go wrong .',\n",
       " 'we prove that our procedure synthesizes general witnesses in that if a witness is found , then for all inhabited input types , there exist values that can make the function go wrong .',\n",
       " 'second , we show how to extend this procedure to produce a reduction graph that can be used to interactively visualize and debug witness executions .',\n",
       " 'third , we evaluate the coverage of our approach on two data sets comprising over 4,500 ill-typed student programs .',\n",
       " 'our technique is able to generate witnesses for around 85 % of the programs , our reduction graph yields small counterexamples for over 80 % of the witnesses , and a simple heuristic allows us to use witnesses to locate the source of type errors with around 70 % accuracy .',\n",
       " 'finally , we evaluate whether our witnesses help students understand and fix type errors , and find that students presented with our witnesses show a greater understanding of type errors than those presented with a standard error message .',\n",
       " 'while large knowledge graphs provide vast amounts of structured facts about entities , a short textual description can often be useful to succinctly characterize an entity and its type .',\n",
       " 'unfortunately , many knowledge graph entities lack such textual descriptions .',\n",
       " 'in this paper , we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact technical as well as the dynamic context of the generated sequence of words .',\n",
       " 'we demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines .',\n",
       " 'we consider the problem of extracting entropy by sparse transformations , namely functions with a small number of overall input-output dependencies .',\n",
       " 'in contrast to previous works , we seek extractors for essentially all the entropy without any assumption on the underlying distribution beyond a min-entropy requirement .',\n",
       " 'we give two simple constructions of sparse extractor families , which are collections of sparse functions such that for any distribution x on inputs of sufficiently high min-entropy , the output of most functions from the collection on a random input chosen from x is statistically close to uniform .',\n",
       " 'for strong extractor families ( i.e. , functions in the family do not take additional randomness ) we give upper and lower bounds on the sparsity that are tight up to a constant factor for a wide range of min-entropies .',\n",
       " 'we then prove that for some min-entropies weak extractor families can achieve better sparsity .',\n",
       " 'we show how this construction can be used towards more efficient parallel transformation of ( non-uniform ) one-way functions into pseudorandom generators .',\n",
       " 'more generally , sparse extractor families can be used instead of pairwise independence in various randomized or nonuniform settings where preserving locality ( i.e. , parallelism ) is of interest .',\n",
       " 'one of the most interesting features of technical optimization for direct policy search is that it can leverage priors ( e.g. , from simulation or from previous tasks ) to accelerate learning on a robot .',\n",
       " 'in this paper , we are interested in situations for which several priors exist but we do not know in advance which one fits best the current situation .',\n",
       " 'we tackle this problem by introducing a novel acquisition function , called most likely expected improvement ( mlei ) , that combines the likelihood of the priors and the expected improvement .',\n",
       " 'we evaluate this new acquisition function on a transfer learning task for a 5-dof planar arm and on a possibly damaged , 6-legged robot that has to learn to walk on flat ground and on stairs , with priors corresponding to different stairs and different kinds of damages .',\n",
       " 'our results show that mlei effectively identifies and exploits the priors , even when there is no obvious match between the current situations and the priors .',\n",
       " 'this paper provides a general result on controlling local rademacher complexities , which captures in an elegant form to relate the complexities with constraint on the expected norm to the corresponding ones with constraint on the empirical norm .',\n",
       " 'this result is convenient to apply in real applications and could yield refined local rademacher complexity bounds for function classes satisfying general entropy conditions .',\n",
       " 'we demonstrate the power of our complexity bounds by applying them to derive effective generalization error bounds .',\n",
       " 'while machine learning approaches to image restoration offer great promise , current methods risk training models fixated on performing well only for image corruption of a particular level of difficulty -- -such as a certain level of noise or blur .',\n",
       " \"first , we examine the weakness of conventional `` fixated '' models and demonstrate that training general models to handle arbitrary levels of corruption is indeed non-trivial .\",\n",
       " 'then , we propose an on-demand learning algorithm for training image restoration models with deep technical neural networks .',\n",
       " 'the main idea is to exploit a feedback mechanism to self-generate training instances where they are needed most , thereby learning models that can generalize across difficulty levels .',\n",
       " 'on four restoration tasks -- -image inpainting , pixel interpolation , image deblurring , and image denoising -- -and three diverse data , our approach consistently better both the status quo training procedure and curriculum learning alternatives .',\n",
       " 'we consider a network of prosumers involved in peer-to-peer energy exchanges , with differentiation price preferences on the trades with their neighbors , and we analyze two market designs : ( i ) a centralized market , used as a standard , where a global market operator optimizes the flows ( trades ) between the nodes , local demand and flexibility activation to maximize the system overall social welfare ; ( ii ) a distributed peer-to-peer market design where prosumers in local energy communities optimize selfishly their trades , demand , and flexibility activation .',\n",
       " 'we first characterizethe solution of the peer-to-peer market as a variation equilibrium and prove that the set of variation equilibria coincides with the set of social welfare optimal solutions of market design ( i ) .',\n",
       " 'we give several results that help understanding the structure of the trades at an equilibriumor at the optimum .',\n",
       " 'we characterize the impact of preferences on the network line congestion and renewable energy waste under both designs .',\n",
       " 'we provide a reduced example for which we give the set of all possible generalized equilibria , which enables to give an approximation of the price ofanarchy .',\n",
       " 'we provide a more realistic example which relies on the ieee 14-bus network , for which we can simulate the trades under different preference prices .',\n",
       " 'our analysis shows in particular that the preferences have a large impact on the structure of the trades , but that one equilibrium ( variation ) is optimal .',\n",
       " 'one important factor determining the computational complexity of evaluating a probability network is the cardinality of the state spaces of the nodes .',\n",
       " 'by varying the granularity of the state spaces , one can trade off accuracy in the result for computational efficiency .',\n",
       " 'we present an anytime procedure for approximate evaluation of probability networks based on this idea .',\n",
       " 'on application to some simple networks , the procedure exhibits a smooth improvement in approximation quality as computation time increases .',\n",
       " 'this suggests that state-space abstraction is one more useful control parameter for designing real-time probability reasoners .',\n",
       " 'in this paper we present the rusentrel corpus including analytical texts in the sphere of international relations .',\n",
       " 'for each document we annotated sentiments from the author to mentioned named entities , and sentiments of relations between mentioned entities .',\n",
       " 'in the current experiments , we considered the problem of extracting sentiment relations between entities for the whole documents as a three-class machine learning task .',\n",
       " 'we experimented with conventional machine-learning methods ( naive bayes , svm , random forest ) .',\n",
       " 'this paper explores the idea that the universe is a virtual reality created by information processing , and relates this strange idea to the findings of modern physics about the physical world .',\n",
       " 'the virtual reality concept is familiar to us from online worlds , but our world as a virtual reality is usually a subject for science fiction rather than science .',\n",
       " 'yet logically the world could be an information simulation running on a multi-dimensional space-time screen .',\n",
       " 'indeed , if the essence of the universe is information , matter , charge , energy and movement could be aspects of information , and the many conservation laws could be a single law of information conservation .',\n",
       " 'if the universe were a virtual reality , its creation at the big bang would no longer be paradoxical , as every virtual system must be booted up .',\n",
       " 'it is suggested that whether the world is an objective reality or a virtual reality is a matter for science to resolve .',\n",
       " 'modern information science can suggest how core physical properties like space , time , light , matter and movement could derive from information processing .',\n",
       " 'such an approach could reconcile relativity and quantum theories , with the former being how information processing creates space-time , and the latter how it creates energy and matter .',\n",
       " 'a central problem to understanding intelligence is the concept of generalisation .',\n",
       " 'this allows previously learnt structure to be exploited to solve tasks in novel situations differing in their particularities .',\n",
       " 'we take inspiration from neuroscience , specifically the hippocampal-entorhinal system known to be important for generalisation .',\n",
       " 'we propose that to generalise structural knowledge , the representations of the structure of the world , i.e . how entities in the world relate to each other , need to be separated from representations of the entities themselves .',\n",
       " 'we show , under these principles , artificial neural networks embedded with hierarchy and fast hebbian memory , can learn the statistics of memories and generalise structural knowledge .',\n",
       " 'spatial neuronal representations mirroring those found in the brain emerge , suggesting spatial cognition is an instance of more general organising principles .',\n",
       " 'we further unify many entorhinal cell types as basis functions for constructing transition graphs , and show these representations effectively utilise memories .',\n",
       " 'we experimentally support model assumptions , showing a preserved relationship between entorhinal grid and hippocampal place cells across environments .',\n",
       " 'recently , compressive antenna arrays have been considered for doa estimation with reduced hardware complexity .',\n",
       " 'by utilizing compressive sensing , such arrays employ a linear combining network to combine signals from a larger set of antenna elements in the analog rf domain .',\n",
       " 'in this paper , we develop a design approach based on the minimization of error between spatial correlation function ( scf ) of the compressive and the uncompressed array resulting in the estimation performance of the two arrays to be as close as possible .',\n",
       " 'the proposed design is based on grid-free random gradient descent ( sgd ) optimization .',\n",
       " 'in addition to a low computational cost for the proposed method , we show numerically that the resulting combining matrices perform better than the ones generated by a previous approach and combining matrices generated from a normal ensemble .',\n",
       " 'a mobile robot deployed for remote inspection , surveying or rescue missions can fail due to various possibilities and can be hardware or software related .',\n",
       " 'these failure scenarios necessitate manual recovery ( self-rescue ) of the robot from the environment .',\n",
       " 'it would bring unforeseen challenges to recover the mobile robot if the environment where it was deployed had hazardous or harmful conditions ( e.g . ionizing radiations ) .',\n",
       " 'while it is not fully possible to predict all the failures in the robot , failures can be reduced by employing certain design/usage considerations .',\n",
       " 'few example failure cases based on real experiences are presented in this short article along with generic suggestions on overcoming the illustrated failure situations .',\n",
       " 'this article presents the novel breakthrough general purpose algorithm for large scale optimization problems .',\n",
       " 'the novel algorithm is capable of achieving breakthrough speeds for very large optimization on general purpose laptops and embedded systems .',\n",
       " 'application of the algorithm to the griewank function was possible in up to 1 billion decision variables in double precision took only 64485 seconds ( ~18 hours ) to solve , while consuming 7,630 mb ( 7.6 gb ) or ram on a single threaded laptop cpu .',\n",
       " 'it shows that the algorithm is computationally and memory ( space ) linearly efficient , and can find the optimal or near-optimal solution in a fraction of the time and memory that many conventional algorithms require .',\n",
       " 'it is envisaged that this will open up new possibilities of real-time large problems on personal laptops and embedded systems .',\n",
       " 'high level understanding of sequential visual input is important for safe and stable autonomy , especially in location and object detection .',\n",
       " 'while traditional object classification and tracking approaches are specifically designed to handle variations in rotation and scale , current newest approaches based on deep learning achieve better performance .',\n",
       " 'this paper focuses on developing a spatiotemporal model to handle videos containing moving objects with rotation and scale changes .',\n",
       " 'built on models that combine technical neural networks ( technical ) and technical neural networks ( rnns ) to classify sequential data , this work investigates the effectiveness of incorporating attention modules in the cnn stage for video classification .',\n",
       " 'the superiority of the proposed spatiotemporal model is demonstrated on the moving mnist data augmented with rotation and scaling .',\n",
       " 'we propose a mechanism that incorporates network coding into tcp with only minor changes to the protocol stack , thereby allowing incremental deployment .',\n",
       " 'in our scheme , the source transmits random linear combinations of packets currently in the congestion window .',\n",
       " 'at the heart of our scheme is a new interpretation of acks - the sink acknowledges every degree of freedom ( i.e. , a linear combination that reveals one unit of new information ) even if it does not reveal an original packet immediately .',\n",
       " 'such acks enable a tcp-like sliding-window approach to network coding .',\n",
       " 'our scheme has the nice property that packet losses are essentially masked from the congestion control algorithm .',\n",
       " 'our algorithm therefore reacts to packet drops in a smooth manner , resulting in a novel and effective approach for congestion control over networks involving lossy links such as wireless links .',\n",
       " 'our experiments show that our algorithm achieve higher throughput compared to tcp in the presence of lossy wireless links .',\n",
       " 'we also establish the soundness and fairness properties of our algorithm .',\n",
       " 'we report the results of a project to control the use of end user computing tools for business critical applications in a banking environment .',\n",
       " 'several workstreams were employed in order to bring about a cultural change within the bank towards the use of spreadsheets and other end-user tools , covering policy development , awareness and skills training , inventory monitoring , user licensing , key risk metric and mitigation approaches .',\n",
       " 'the outcomes of these activities are discussed , and conclusions are drawn as to the need for appropriate organisational models to guide the use of these tools .',\n",
       " 'in this work we have proposed a geometric model that is employed to devise a scheme for identifying the hotspots and zones in a chip .',\n",
       " 'these spots or zone need to be guarded thermally to ensure performance and reliability of the chip .',\n",
       " 'the model namely continuous unit sphere model has been presented taking into account that the 3d region of the chip is uniform , thereby reflecting on the possible locations of heat sources and the target observation points .',\n",
       " 'the experimental results for the - continuous domain establish that a region which does not contain any heat sources may become hotter than the regions containing the thermal sources .',\n",
       " 'thus a hotspot may appear away from the active sources , and placing heat sinks on the active thermal sources alone may not suffice to tackle thermal imbalance .',\n",
       " 'power management techniques aid in obtaining a uniform power profile throughout the chip , but we propose an algorithm using minimum bipartite matching where we try to move the sources minimally ( with minimum perturbation in the chip floor plan ) near cooler points ( blocks ) to obtain a uniform power profile due to diffusion of heat from hotter point to cooler ones .',\n",
       " 'one of the key differences between the learning mechanism of humans and artificial neural networks ( anns ) is the ability of humans to learn one task at a time .',\n",
       " 'anns , on the other hand , can only learn multiple tasks simultaneously .',\n",
       " 'any attempts at learning new tasks incrementally cause them to completely forget about previous tasks .',\n",
       " 'this lack of ability to learn incrementally , called catastrophic forgetting , is considered a major hurdle in building a true ai system .',\n",
       " 'in this paper , our goal is to isolate the truly effective existing ideas for incremental learning from those that only work under certain conditions .',\n",
       " 'to this end , we first thoroughly analyze the current state of the art ( icarl ) method for incremental learning and demonstrate that the good performance of the system is not because of the reasons presented in the existing literature .',\n",
       " 'we conclude that the success of icarl is primarily due to knowledge distillation and recognize a key limitation of knowledge distillation , i.e , it often leads to bias in classify .',\n",
       " 'finally , we propose a dynamic threshold moving algorithm that is able to successfully remove this bias .',\n",
       " 'we demonstrate the effectiveness of our algorithm on cifar100 and mnist data showing near-optimal results .',\n",
       " 'our implementation is available at https : //github.com/khurramjaved96/incremental-learning .',\n",
       " 'policy gradient is an efficient technique for improving a policy in a reinforcement learning setting .',\n",
       " 'however , vanilla online variants are on-policy only and not able to take advantage of off-policy data .',\n",
       " 'in this paper we describe a new technique that combines policy gradient with off-policy q-learning , drawing experience from a replay buffer .',\n",
       " 'this is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the q-values .',\n",
       " 'this connection allows us to estimate the q-values from the action preferences of the policy , to which we apply q-learning updates .',\n",
       " \"we refer to the new technique as 'pgql ' , for policy gradient and q-learning .\",\n",
       " 'we also establish an equivalency between action-value fitting techniques and actor-critic algorithms , showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms .',\n",
       " 'we conclude with some numerical examples that demonstrate improved data efficiency and stability of pgql .',\n",
       " 'in particular , we tested pgql on the full suite of atari games and achieved performance exceeding that of both asynchronous advantage actor-critic ( a3c ) and q-learning .',\n",
       " \"this work briefly surveys unconventional research in russia from the end of the 19th until the beginning of the 21th centuries in areas related to generation and detection of a 'high-penetrating ' emission of non-biological origin .\",\n",
       " 'the overview is based on open scientific and journalistic materials .',\n",
       " 'the unique character of this research and its history , originating from governmental programs of the ussr , is shown .',\n",
       " 'relations to modern studies on biological effects of weak electromagnetic emission , several areas of bioinformatics and theories of physical vacuum are discussed .',\n",
       " '2015 is the centennial of einstein general relativity .',\n",
       " 'on this occasion , we examine the general relativity and quantum cosmology ( grqc ) field of research by analysing 38291 papers uploaded on the electronic archives arxiv.org from 2000 to 2012 .',\n",
       " 'we establish a map of the countries contributing to grqc in 2012 .',\n",
       " 'we determine the main journals publishing grqc papers and which countries publish in which journals .',\n",
       " 'we find that more and more papers are written by groups ( instead of single ) of authors with more and more international collaborations .',\n",
       " 'there are huge differences between countries .',\n",
       " 'hence russia is the country where most of papers are written by single authors whereas canada is one of the countries where the most of papers imply international collaborations .',\n",
       " 'we also study authors mobility , determining how some groups of authors spread worldwide with time in different countries .',\n",
       " 'the largest mobilities are between usa-uk and usa-germany .',\n",
       " 'countries attracting the most of grqc authors are netherlands and canada whereas those undergoing a brain drain are italy and india .',\n",
       " 'there are few mobility between europe and asia contrarily to mobility between usa and asia .',\n",
       " 'in wind farms , wake interaction leads to losses in power capture and accelerated structural degradation when compared to freestanding turbines .',\n",
       " 'one method to reduce wake losses is by misaligning the rotor with the incoming flow using its yaw actuator , thereby laterally deflecting the wake away from downstream turbines .',\n",
       " 'however , this demands an accurate and computationally tractable model of the wind farm dynamics .',\n",
       " 'this problem calls for a closed-loop solution .',\n",
       " 'this tutorial paper fills the scientific gap by demonstrating the full closed-loop controller synthesis cycle using a steady-state surrogate model .',\n",
       " 'furthermore , a novel , computationally efficient and modular communication interface is presented that enables researchers to straight-forwardly test their control algorithms in large-eddy simulations .',\n",
       " 'high-fidelity simulations of a 9-turbine farm show a power production increase of up to 11 % using the proposed closed-loop controller compared to traditional , greedy wind farm operation .',\n",
       " 'behavior trees ( bts ) have become a popular framework for designing controllers of autonomous agents in the computer game and in the robotics industry .',\n",
       " 'one of the key advantages of bts lies in their modularity , where independent modules can be composed to create more complex ones .',\n",
       " 'in the classical formulation of bts , modules can be composed using one of the three operators : sequence , fallback , and parallel .',\n",
       " 'the parallel operator is rarely used despite its strong potential against other control architecture as finite state machines .',\n",
       " 'this is due to the fact that concurrent actions may lead to unexpected problems similar to the ones experienced in concurrent programming .',\n",
       " 'in this paper , we introduce concurrent bts ( cbts ) as a generalization of bts in which we introduce the notions of progress and resource usage .',\n",
       " 'we show how cbts allow safe concurrent executions of actions and we analyze the approach from a mathematical standpoint .',\n",
       " 'to illustrate the use of cbts , we provide a set of use cases in robotics scenarios .',\n",
       " 'the problem of comparing concepts of dependence in general rough sets with those in probability theory had been initiated by the present author in some of her recent papers .',\n",
       " 'this problem relates to the identification of the limitations of translating between the methodologies and possibilities in the identification of concepts .',\n",
       " 'comparison of ideas of dependence in the approaches had been attempted from a set-valuation based minimalist perspective by the present author .',\n",
       " 'the deviant probability framework has been the result of such an approach .',\n",
       " 'other technical reasoning perspectives ( involving numeric valuations ) and frequentist approaches are also known .',\n",
       " 'in this research , duality results are adapted to demonstrate the possibility of improved comparisons across implications between ontologically distinct concepts in a common logic-based framework by the present author .',\n",
       " 'both positive and negative results are proved that delimit possible comparisons in a clearer way by her .',\n",
       " 'this paper is concerned with the effect of overlay network topology on the performance of live streaming peer-to-peer systems .',\n",
       " 'the paper focuses on the evaluation of topologies which are aware of the delays experienced between different peers on the network .',\n",
       " 'metric are defined which assess the topologies in terms of delay , bandwidth usage and resilience to peer drop-out .',\n",
       " 'several topology creation algorithms are tested and the metric are measured in a simple simulation testbed .',\n",
       " 'this gives an assessment of the type of gains which might be expected from locality awareness in peer-to-peer networks .',\n",
       " 'bootstrap percolation is an often used model to study the spread of diseases , rumors , and information on sparse random graphs .',\n",
       " 'the percolation process demonstrates a critical value such that the graph is either almost completely affected or almost completely unaffected based on the initial seed being larger or smaller than the critical value .',\n",
       " 'to analyze intervention strategies we provide the first analytic determination of the critical value for basic bootstrap percolation in random graphs when the vertex thresholds are nonuniform and provide an efficient algorithm .',\n",
       " \"this result also helps solve the problem of `` percolation with coinflips '' when the infection process is not deterministic , which has been a criticism about the model .\",\n",
       " 'we also extend the results to clustered random graphs thereby extending the classes of graphs considered .',\n",
       " 'in these graphs the vertices are grouped in a small number of clusters , the clusters model a fixed communication network and the edge probability is dependent if the vertices are in close or far clusters .',\n",
       " 'we present simulations for both basic percolation and interventions that support our theoretical results .',\n",
       " 'we present the first parser for ucca , a cross-linguistically applicable framework for semantic representation , which builds on extensive typological work and supports rapid annotation .',\n",
       " 'ucca poses a challenge for existing parsing techniques , as it exhibits reentrancy ( resulting in dag structures ) , discontinuous structures and non-terminal nodes corresponding to complex semantic units .',\n",
       " 'to our knowledge , the conjunction of these formal properties is not supported by any existing parser .',\n",
       " 'our transition-based parser , which uses a novel transition set and features based on bidirectional lstms , has value not just for ucca parsing : its ability to handle more general graph structures can inform the development of parsers for other semantic dag structures , and in languages that frequently use discontinuous structures .',\n",
       " 'policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance .',\n",
       " 'several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates .',\n",
       " 'to better understand this development , we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested standard domains .',\n",
       " 'we confirm this unexpected result by reviewing the open-source code accompanying these prior papers , and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains .',\n",
       " 'furthermore , the variance decomposition highlights areas for improvement , which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance .',\n",
       " 'identifying the relations that exist between words ( or entities ) is important for various natural language processing tasks such as , relational search , noun-modifier classification and analogy detection .',\n",
       " 'a popular approach to represent the relations between a pair of words is to extract the patterns in which the words co-occur with from a corpus , and assign each word-pair a vector of pattern frequencies .',\n",
       " 'despite the simplicity of this approach , it suffers from data sparseness , information scalability and linguistic creativity as the model is unable to handle previously unseen word pairs in a corpus .',\n",
       " 'in contrast , a compositional approach for representing relations between words overcomes these issues by using the attributes of each individual word to indirectly compose a representation for the common relations that hold between the two words .',\n",
       " 'this study aims to compare different operations for creating relation representations from word-level representations .',\n",
       " 'we investigate the performance of the compositional methods by measuring the relational similarities using several standard data for word analogy .',\n",
       " 'moreover , we evaluate the different relation representations in a knowledge base completion task .',\n",
       " 'we present a language independent , technical method for building word technical using morphological expansion of text .',\n",
       " 'our model handles the problem of data sparsity and yields improved word technical by relying on training word technical on artificially generated sentences .',\n",
       " 'we evaluate our method using small sized training sets on eleven test sets for the word similarity task across seven languages .',\n",
       " 'further , for english , we evaluated the impacts of our approach using a large training set on three standard test sets .',\n",
       " 'our method improved results across all languages .',\n",
       " 'efforts are underway at ut austin to build autonomous robot systems that address the challenges of long-term deployments in office environments and of the more prescribed domestic service tasks of the robocup @ home competition .',\n",
       " 'we discuss the contrasts and synergies of these efforts , highlighting how our work to build a robocup @ home domestic standard platform league entry led us to identify an integrated software architecture that could support both projects .',\n",
       " 'further , naturalistic deployments of our office robot platform as part of the building-wide intelligence project have led us to identify and research new problems in a traditional laboratory setting .',\n",
       " 'although nonstationary data are more common in the real world , most existing causal discovery methods do not take nonstationarity into consideration .',\n",
       " 'in this letter , we propose a kernel embedding-based approach , enci , for nonstationary causal model inference where data are collected from multiple domains with varying distributions .',\n",
       " 'in enci , we transform the complicated relation of a cause-effect pair into a linear model of variables of which observations correspond to the kernel technical of the cause-and-effect distributions in different domains .',\n",
       " 'in this way , we are able to estimate the causal direction by exploiting the causal asymmetry of the transformed linear model .',\n",
       " 'furthermore , we extend enci to causal graph discovery for multiple variables by transforming the relations among them into a linear nongaussian acyclic model .',\n",
       " 'we show that by exploiting the nonstationarity of distributions , both cause-effect pairs and two kinds of causal graphs are identifiable under mild conditions .',\n",
       " 'experiments on synthetic and world data are conducted to justify the efficacy of enci over major existing methods .',\n",
       " 'dou shou qi is a game in which two players control a number of pieces , each of them aiming to move one of their pieces onto a given square .',\n",
       " 'we implemented an engine for analyzing the game .',\n",
       " 'moreover , we created a series of endgame tablebases containing all configurations with up to four pieces .',\n",
       " 'these tablebases are the first steps towards theoretically solving the game .',\n",
       " 'finally , we constructed decision trees based on the endgame tablebases .',\n",
       " 'in this note we report on some interesting patterns .',\n",
       " 'as deep neural networks ( dnns ) have been integrated into critical systems , several methods to attack these systems have been developed .',\n",
       " 'these adversary attacks make imperceptible modifications to an image that fool dnn classify .',\n",
       " 'we present an adaptive jpeg encoder which defends against many of these attacks .',\n",
       " 'experimentally , we show that our method produces images with high visual quality while greatly reducing the potency of newest attacks .',\n",
       " 'our algorithm requires only a modest increase in encoding time , produces a compressed image which can be decompressed by an off-the-shelf jpeg decoder , and classified by an unmodified classify',\n",
       " 'one of the most fundamental questions one can ask about a pair of random variables x and y is the value of their mutual information .',\n",
       " 'unfortunately , this task is often stymied by the extremely large dimension of the variables .',\n",
       " 'we might hope to replace each variable by a lower-dimensional representation that preserves the relationship with the other variable .',\n",
       " 'the theoretically ideal implementation is the use of minimal sufficient statistics , where it is well-known that either x or y can be replaced by their minimal sufficient statistic about the other while preserving the mutual information .',\n",
       " 'while intuitively reasonable , it is not obvious or straightforward that both variables can be replaced simultaneously .',\n",
       " \"we demonstrate that this is in fact possible : the information x 's minimal sufficient statistic preserves about y is exactly the information that y 's minimal sufficient statistic preserves about x .\",\n",
       " \"as an important corollary , we consider the case where one variable is a random process ' past and the other its future and the present is viewed as a memoryful channel .\",\n",
       " \"in this case , the mutual information is the channel transmission rate between the channel 's effective states .\",\n",
       " 'that is , the past-future mutual information ( the excess entropy ) is the amount of information about the future that can be predicted using the past .',\n",
       " 'translating our result about minimal sufficient statistics , this is equivalent to the mutual information between the forward- and reverse-time causal states of computational mechanics .',\n",
       " 'we close by discussing multivariate extensions to this use of minimal sufficient statistics .',\n",
       " 'a conditional generate adversary network allows for generating samples conditioned on certain external information .',\n",
       " 'being able to recover hidden and conditional vectors from a condi- tional gan can be potentially valuable in various applications , ranging from image manipulation for entertaining purposes to diagnosis of the neural networks for security purposes .',\n",
       " 'in this work , we show that it is possible to recover both hidden and conditional vectors from generated images given the generator of a conditional generate adversary network .',\n",
       " 'such a recovery is not trivial due to the often multi-layered non-linearity of deep neural networks .',\n",
       " 'furthermore , the effect of such recovery applied on real natural images are investigated .',\n",
       " 'we discovered that there exists a gap between the recovery performance on generated and real images , which we believe comes from the difference between generated data distribution and real data distribution .',\n",
       " 'experiments are conducted to evaluate the recovered conditional vectors and the reconstructed images from these recovered vectors quantitatively and qualitatively , showing promising results .',\n",
       " 'in this paper , we present discretezoo , a project which illustrates some of the possibilities for computer-supported management of collections of finite combinatorial ( discrete ) objects , in particular graphs with a high degree of symmetry .',\n",
       " 'discretezoo encompasses a data repository , a website and a sagemath package .',\n",
       " 'query-expansion via pseudo-relevance feedback is a popular method of overcoming the problem of vocabulary mismatch and of increasing average retrieval effectiveness .',\n",
       " 'in this paper , we develop a new method that estimates a query topic model from a set of pseudo-relevant documents using a new language modelling framework .',\n",
       " 'we assume that documents are generated via a mixture of multivariate polya distributions , and we show that by identifying the topical terms in each document , we can appropriately select terms that are likely to belong to the query topic model .',\n",
       " 'the results of experiments on several trec collections show that the new approach compares favourably to current newest expansion methods .',\n",
       " 'distribution regression has recently attracted much interest as a generic solution to the problem of supervised learning where labels are available at the group level , rather than at the individual level .',\n",
       " 'current approaches , however , do not propagate the uncertainty in observations due to sampling variability in the groups .',\n",
       " 'this effectively assumes that small and large groups are estimated equally well , and should have equal weight in the final regression .',\n",
       " 'we account for this uncertainty with a technical distribution regression formalism , improving the robust and performance of the model when group sizes vary .',\n",
       " 'we frame our models in a neural network style , allowing for simple map inference using backpropagation to learn the parameters , as well as mcmc-based inference which can fully propagate uncertainty .',\n",
       " 'we demonstrate our approach on illustrative toy data , as well as on a challenging problem of predicting age from images .',\n",
       " 'analyzing job hopping behavior is important for the understanding of job preference and career progression of working individuals .',\n",
       " 'when analyzed at the workforce population level , job hop analysis helps to gain insights of talent flow and organization competition .',\n",
       " 'traditionally , surveys are conducted on job seekers and employers to study job behavior .',\n",
       " 'while surveys are good at getting direct user input to specially designed questions , they are often not scalable and timely enough to cope with fast-changing job landscape .',\n",
       " 'in this paper , we present a data science approach to analyze job hops performed by about 490,000 working professionals located in a city using their publicly shared profiles .',\n",
       " 'we develop several metric to measure how much work experience is needed to take up a job and how recent/established the job is , and then examine how these metric correlate with the propensity of hopping .',\n",
       " 'we also study how job hop behavior is related to job promotion/demotion .',\n",
       " 'finally , we perform network analyses at the job and organization levels in order to derive insights on talent flow as well as job and organizational competitiveness .',\n",
       " 'this paper presents iterative sequential action control ( isac ) , a receding horizon approach for control of nonlinear systems .',\n",
       " 'the isac method has a closed-form open-loop solution , which is iteratively updated between time steps by introducing constant control values applied for short duration .',\n",
       " 'application of a contractive constraint on the cost is shown to lead to closed-loop asymptotic stability under mild assumptions .',\n",
       " 'the effect of asymptotically decaying disturbances on system trajectories is also examined .',\n",
       " 'to demonstrate the applicability of isac to a variety of systems and conditions , we employ five different systems , including a 13-dimensional quaternion-based quadrotor .',\n",
       " 'each system is tested in different scenarios , ranging from feasible and infeasible trajectory tracking , to setpoint stabilization , with or without the presence of external disturbances .',\n",
       " 'finally , limitations of this work are discussed .',\n",
       " 'we consider a compressed sensing problem in which both the measurement and the sparsifying systems are assumed to be frames ( not necessarily tight ) of the underlying hilbert space of signals , which may be finite or infinite dimensional .',\n",
       " 'the main result gives explicit bounds on the number of measurements in order to achieve stable recovery , which depends on the mutual coherence of the two systems .',\n",
       " 'as a simple corollary , we prove the efficiency of nonuniform sampling strategies in cases when the two systems are not incoherent , but only asymptotically incoherent , as with the recovery of wavelet coefficients from fourier samples .',\n",
       " 'this general framework finds applications to inverse problems in partial differential equations , where the standard assumptions of compressed sensing are often not satisfied .',\n",
       " 'several examples are discussed , with a special focus on electrical impedance tomography .',\n",
       " \"social health and emotional wellness is a matter of concern in today 's urban world .\",\n",
       " 'being the part of a metropolis has an effect on mental health through the influence of increased stressors and factors such as overcrowded and polluted environment , high levels of violence , and reduced social support .',\n",
       " 'it is important to realize that only healthy citizens can constitute together a smart city .',\n",
       " 'in this paper , we present a fuzzy-based approach for analyzing the well being of a person .',\n",
       " 'we track the general day to day activities of a person and analyze its performance .',\n",
       " 'to do so , we divide the factors affecting the wellness of a person into three components which are the physical , productive and social .',\n",
       " 'using these parameters , we output a coefficient for the overall well being of a person .',\n",
       " 'the visual observation and tracking of cells and other micrometer-sized objects has many different biomedical applications .',\n",
       " 'the automation of those tasks based on computer methods helps in the evaluation of such measurements .',\n",
       " 'in this work , we present a general purpose algorithm that excels at evaluating deterministic behavior of micrometer-sized objects .',\n",
       " 'our concrete application is the tracking of fast moving objects over large distances along deterministic trajectories in a microscopic video .',\n",
       " 'thereby , we are able to determine characteristic properties of the objects .',\n",
       " 'for this purpose , we use a set of basic algorithms , including blob recognition , feature-based shape recognition and a graph algorithm , and combined them in a novel way .',\n",
       " 'an evaluation of the algorithms performance shows a high accuracy in the recognition of objects as well as of complete trajectories .',\n",
       " 'moreover , a direct comparison to a similar algorithm shows superior recognition rates .',\n",
       " 'in iterative supervised learning algorithms it is common to reach a point in the search where no further induction seems to be possible with the available data .',\n",
       " 'if the search is continued beyond this point , the risk of overfitting increases significantly .',\n",
       " 'following the recent developments in inductive semantic random methods , this paper studies the feasibility of using information gathered from the semantic neighborhood to decide when to stop the search .',\n",
       " 'two semantic stopping criteria are proposed and experimentally assessed in geometric semantic genetic programming ( gsgp ) and in the semantic learning machine ( slm ) algorithm ( the equivalent algorithm for neural networks ) .',\n",
       " 'the experiments are performed on world high-dimensional regression data .',\n",
       " 'the results show that the proposed semantic stopping criteria are able to detect stopping points that result in a competitive generalization for both gsgp and slm .',\n",
       " 'this approach also yields computationally efficient algorithms as it allows the evolution of neural networks in less than 3 seconds on average , and of gp trees in at most 10 seconds .',\n",
       " 'the usage of the proposed semantic stopping criteria in conjunction with the computation of optimal mutation/learning steps also results in small trees and neural networks .',\n",
       " 'technical rectifier networks , i.e . technical neural networks with rectified linear activation and max or average pooling , are the cornerstone of modern deep learning .',\n",
       " 'however , despite their wide use and success , our theoretical understanding of the expressive properties that drive these networks is partial at best .',\n",
       " 'on the other hand , we have a much firmer grasp of these issues in the world of arithmetic circuits .',\n",
       " \"specifically , it is known that technical arithmetic circuits possess the property of `` complete depth efficiency '' , meaning that besides a negligible set , all functions that can be implemented by a deep network of polynomial size , require exponential size in order to be implemented ( or even approximated ) by a shallow network .\",\n",
       " 'in this paper we describe a construction based on generalized tensor decompositions , that transforms technical arithmetic circuits into technical rectifier networks .',\n",
       " 'we then use mathematical tools available from the world of arithmetic circuits to prove new results .',\n",
       " 'first , we show that technical rectifier networks are universal with max pooling but not with average pooling .',\n",
       " 'second , and more importantly , we show that depth efficiency is weaker with technical rectifier networks than it is with technical arithmetic circuits .',\n",
       " 'this leads us to believe that developing effective methods for training technical arithmetic circuits , thereby fulfilling their expressive potential , may give rise to a deep learning architecture that is provably superior to technical rectifier networks but has so far been overlooked by practitioners .',\n",
       " 'an adversary example is an example that has been adjusted to produce the wrong label when presented to a system at test time .',\n",
       " 'if adversary examples existed that could fool a detector , they could be used to ( for example ) wreak havoc on roads populated with smart vehicles .',\n",
       " 'recently , we described our difficulties creating physical adversary stop signs that fool a detector .',\n",
       " 'more recently , evtimov et al . produced a physical adversary stop sign that fools a proxy model of a detector .',\n",
       " 'in this paper , we show that these physical adversary stop signs do not fool two standard detectors ( yolo and faster rcnn ) in standard configuration .',\n",
       " 'evtimov et al .',\n",
       " \"'s construction relies on a crop of the image to the stop sign ; this crop is then resized and presented to a classify .\",\n",
       " 'we argue that the cropping and resizing procedure largely eliminates the effects of rescaling and of view angle .',\n",
       " 'whether an adversary attack is robust under rescaling and change of view direction remains moot .',\n",
       " 'we argue that attacking a classify is very different from attacking a detector , and that the structure of detectors - which must search for their own bounding box , and which can not estimate that box very accurately - likely makes it difficult to make adversary patterns .',\n",
       " 'finally , an adversary pattern on a physical object that could fool a detector would have to be adversary in the face of a wide family of parametric distortions ( scale ; view angle ; box shift inside the detector ; illumination ; and so on ) .',\n",
       " 'such a pattern would be of great theoretical and practical interest .',\n",
       " 'there is currently no evidence that such patterns exist .',\n",
       " 'the residual neural network ( resnet ) is a popular deep network architecture which has the ability to obtain high-accuracy results on several image processing problems .',\n",
       " 'in order to analyze the behavior and structure of resnet , recent work has been on establishing connections between resnets and continuous-time optimal control problems .',\n",
       " 'in this work , we show that the post-activation resnet is related to an optimal control problem with differential inclusions , and provide continuous-time stability results for the differential inclusion associated with resnet .',\n",
       " 'motivated by the stability conditions , we show that alterations of either the architecture or the optimization problem can generate variants of resnet which improve the theoretical stability bounds .',\n",
       " 'in addition , we establish stability bounds for the full ( discrete ) network associated with two variants of resnet , in particular , bounds on the growth of the features and a measure of the sensitivity of the features with respect to perturbations .',\n",
       " 'these results also help to show the relationship between the depth , regularization , and stability of the feature space .',\n",
       " 'computational experiments on the proposed variants show that the accuracy of resnet is preserved and that the accuracy seems to be monotone with respect to the depth and various corruptions .',\n",
       " 'acute kidney injury ( aki ) is a common and serious complication after a surgery which is associated with morbidity and mortality .',\n",
       " 'the majority of existing perioperative aki risk score prediction models are limited in their generalizability and do not fully utilize the physiological intraoperative time-series data .',\n",
       " \"thus , there is a need for intelligent , accurate , and robust systems , able to leverage information from large data to predict patient 's risk of developing postoperative aki .\",\n",
       " 'a retrospective single-center cohort of 2,911 adult patients who underwent surgery at the university of florida health has been used for this study .',\n",
       " 'we used machine learning and statistical analysis techniques to develop perioperative models to predict the risk of aki ( risk during the first 3 days , 7 days , and until the discharge day ) before and after the surgery .',\n",
       " 'in particular , we examined the improvement in risk prediction by incorporating three intraoperative physiologic time series data , i.e. , mean arterial blood pressure , minimum alveolar concentration , and heart rate .',\n",
       " 'for an individual patient , the preoperative model produces a probability aki risk score , which will be enriched by integrating intraoperative statistical features through a machine learning stacking approach inside a random forest classify .',\n",
       " 'we compared the performance of our model based on the area under the receiver operating characteristics curve ( auroc ) , accuracy and net reclassification improvement ( nri ) .',\n",
       " 'the predictive performance of the proposed model is better than the preoperative data only model .',\n",
       " 'for aki-7day outcome : the auc was 0.86 ( accuracy was 0.78 ) in the proposed model , while the preoperative auc was 0.84 ( accuracy 0.76 ) .',\n",
       " 'furthermore , with the integration of intraoperative features , we were able to classify patients who were misclassified in the preoperative model .',\n",
       " \"smart cities are an actual trend being pursued by research that , fundamentally , tries to improve city 's management on behalf of a better human quality of live .\",\n",
       " 'this paper proposes a new autonomic complementary approach for smart cities management .',\n",
       " 'it is argued that smart city management systems with autonomic characteristics will improve and facilitate management functionalities in general .',\n",
       " 'a framework is also presented as use case considering specific application scenarios like smart-health , smart-grid , smart-environment and smart-streets .',\n",
       " 'artificial intelligence and machine learning have been major research interests in computer science for the better part of the last few decades .',\n",
       " 'however , all too recently , both ai and ml have rapidly grown to be media frenzies , pressuring companies and researchers to claim they use these technologies .',\n",
       " 'as ml continues to percolate into daily life , we , as computer scientists and machine learning researchers , are responsible for ensuring we clearly convey the extent of our work and the humanity of our models .',\n",
       " \"regularizing ml for mass adoption requires a rigorous standard for model interpretability , a deep consideration for human bias in data , and a transparent understanding of a model 's societal effects .\",\n",
       " 'solar forecasting accuracy is affected by weather conditions , and weather awareness forecasting models are expected to improve the performance .',\n",
       " 'however , it may not be available and reliable to classify different forecasting tasks by using only meteorological weather categorization .',\n",
       " 'in this paper , an technical clustering-based ( uc-based ) solar forecasting methodology is developed for short-term ( 1-hour-ahead ) global horizontal irradiance ( ghi ) forecasting .',\n",
       " 'this methodology consists of three parts : ghi time series technical cluster , pattern recognition , and uc-based forecasting .',\n",
       " 'the daily ghi time series is first clustered by an optimized cross-validated cluster ( occur ) method , which determines the optimal number of clusters and best cluster results .',\n",
       " \"then , support vector machine pattern recognition ( svm-pr ) is adopted to recognize the category of a certain day using the first few hours ' data in the forecasting stage .\",\n",
       " 'ghi forecasts are generated by the most suitable models in different clusters , which are built by a two-layer machine learning based multi-model ( m3 ) forecasting framework .',\n",
       " 'the developed uc-based methodology is validated by using 1-year of data with six solar features .',\n",
       " 'numerical results show that ( i ) uc-based models better non-uc ( all-in-one ) models with the same m3 architecture by approximately 20 % ; ( ii ) m3-based models also better the single-algorithm machine learning ( saml ) models by approximately 20 % .',\n",
       " 'we focus on adversary patrolling games on arbitrary graphs , where the defender can control a mobile resource , the targets are alarmed by an alarm system , and the attacker can observe the actions of the mobile resource of the defender and perform different attacks exploiting multiple resources .',\n",
       " 'this scenario can be modeled as a zero-sum extensive-form game in which each player can play multiple times .',\n",
       " 'the game tree is exponentially large both in the size of the graph and in the number of attacking resources .',\n",
       " \"we show that when the number of the attacker 's resources is free , the problem of computing the equilibrium path is np-hard , while when the number of resources is fixed , the equilibrium path can be computed in poly-time .\",\n",
       " \"we provide a dynamic-programming algorithm that , given the number of the attacker 's resources , computes the equilibrium path requiring poly-time in the size of the graph and exponential time in the number of the resources .\",\n",
       " \"furthermore , since in world scenarios it is implausible that the defender knows the number of attacking resources , we study the robust of the defender 's strategy when she makes a wrong guess about that number .\",\n",
       " \"we show that even the error of just a single resource can lead to an arbitrary inefficiency , when the inefficiency is defined as the ratio of the defender 's utilities obtained with a wrong guess and a correct guess .\",\n",
       " \"however , a more suitable definition of inefficiency is given by the difference of the defender 's utilities : this way , we observe that the higher the error in the estimation , the higher the loss for the defender .\",\n",
       " \"then , we investigate the performance of online algorithms when no information about the attacker 's resources is available .\",\n",
       " 'finally , we resort to randomized online algorithms showing that we can obtain a competitive factor that is twice better than the one that can be achieved by any deterministic online algorithm .',\n",
       " 'robotic systems are complex and critical : they are inherently hybrid , combining both hardware and software ; they typically exhibit both cyber-physical attributes and autonomous capabilities ; and are required to be at least safe and often ethical .',\n",
       " 'while for many engineered systems testing , either through real deployment or via simulation , is deemed sufficient the uniquely challenging elements of robotic systems , together with the crucial dependence on sophisticated software control and decision-making , requires a stronger form of verification .',\n",
       " 'the increasing deployment of robotic systems in safety-critical scenarios exacerbates this still further and leads us towards the use of formal methods to ensure the correctness of , and provide sufficient evidence for the certification of , robotic systems .',\n",
       " 'there have been many approaches that have used some variety of formal specification or formal verification in autonomous robotics , but there is no resource that collates this activity in to one place .',\n",
       " 'this paper systematically surveys the state-of-the art in specification formalisms and tools for verifying robotic systems .',\n",
       " 'specifically , it describes the challenges arising from autonomy and software architecture , avoiding low-level hardware control and is subsequently identifies approaches for the specification and verification of robotic systems , while avoiding more general approaches .',\n",
       " 'this paper presents a multi-contact approach to generalized humanoid fall mitigation planning that unifies inertial shaping , protective stepping , and hand contact strategies .',\n",
       " 'the planner optimizes both the contact sequence and the robot state trajectories .',\n",
       " 'a high-level tree search is conducted to iteratively grow a contact transition tree .',\n",
       " 'at each edge of the tree , trajectory optimization is used to calculate robot stabilization trajectories that produce the desired contact transition while minimizing kinetic energy .',\n",
       " 'also , at each node of the tree , the optimizer attempts to find a self-motion ( inertial shaping movement ) to eliminate kinetic energy .',\n",
       " 'this paper also presents an efficient and effective method to generate initial seeds to facilitate trajectory optimization .',\n",
       " 'experiments demonstrate show that our proposed algorithm can generate complex stabilization strategies for a simulated robot under varying initial pushes and environment shapes .',\n",
       " 'feedback mechanism based algorithms are frequently used to solve network optimization problems .',\n",
       " 'these schemes involve users and network exchanging information ( e.g . requests for bandwidth allocation and pricing ) to achieve convergence towards an optimal solution .',\n",
       " 'however , in the implementation , these algorithms do not guarantee that messages will be delivered to the destination when network congestion occurs .',\n",
       " 'this in turn often results in packet drops , which may cause information loss , and this condition may lead to algorithm failing to converge .',\n",
       " 'to prevent this failure , we propose least square ( ls ) estimation algorithm to recover the missing information when packets are dropped from the network .',\n",
       " 'the simulation results involving several scenarios demonstrate that ls estimation can provide the convergence for feedback mechanism based algorithm .',\n",
       " 'we propose a method to perform audio event detection under the common constraint that only limited training data are available .',\n",
       " 'in training a deep learning system to perform audio event detection , two practical problems arise .',\n",
       " \"firstly , most data are `` weakly labelled '' having only a list of events present in each recording without any temporal information for training .\",\n",
       " 'secondly , deep neural networks need a very large amount of labelled training data to achieve good quality performance , yet in practice it is difficult to collect enough samples for most classes of interest .',\n",
       " 'in this paper , we propose a data-efficient training of a stacked technical and technical neural network .',\n",
       " 'this neural network is trained in a multi instance learning setting for which we introduce a new loss function that leads to improved training compared to the usual approaches for weakly supervised learning .',\n",
       " 'we successfully test our approach on two low-resource data that lack temporal labels .',\n",
       " 'objects may appear at arbitrary scales in perspective images of a scene , posing a challenge for recognition systems that process images at a fixed resolution .',\n",
       " 'we propose a depth-aware gating module that adaptively selects the pooling field size in a technical network architecture according to the object scale ( inversely proportional to the depth ) so that small details are preserved for distant objects while larger receptive fields are used for those nearby .',\n",
       " 'the depth gating signal is provided by stereo disparity or estimated directly from monocular input .',\n",
       " 'we integrate this depth-aware gating into a technical technical neural network to perform semantic segment .',\n",
       " 'our technical module iteratively refines the segment results , leveraging the depth and semantic predictions from the previous iterations .',\n",
       " 'through extensive experiments on four popular large rgb-d data , we demonstrate this approach achieve competitive semantic segment performance with a model which is substantially more compact .',\n",
       " 'we carry out extensive analysis of this architecture including variants that operate on monocular rgb but use depth as side-information during training , technical gating as a generic attentional mechanism , and multi-resolution gating .',\n",
       " 'we find that gated pooling for joint semantic segment and depth yields newest results for quantitative monocular depth estimation .',\n",
       " 'newest branch and bound algorithms for mixed integer programming make use of special methods for making branching decisions .',\n",
       " 'strategies that have gained prominence include modern variants of so-called strong branching ( applegate , et al.,1995 ) and reliability branching ( achterberg , koch and martin , 2005 ; hendel , 2015 ) , which select variables for branching by solving associated linear programs and exploit pseudo-costs ( benichou et al. , 1971 ) .',\n",
       " 'we suggest new branching criteria and propose alternative branching approaches called narrow gauge and analytical branching .',\n",
       " 'the perspective underlying our approaches is to focus on prioritization of child nodes to examine fewer candidate variables at the current node of the b & b tree , balanced with procedures to extrapolate the implications of choosing these candidates by generating a small-depth look-ahead tree .',\n",
       " 'our procedures can also be used in rules to select among open tree nodes ( those whose child nodes have not yet been generated ) .',\n",
       " 'we incorporate pre- and post-winnowing procedures to progressively isolate preferred branching candidates , and employ derivative ( created ) variables whose branches are able to explore the solution space more deeply .',\n",
       " 'we present a proof procedure for univariate real polynomial problems in isabelle/hol .',\n",
       " 'the core mathematics of our procedure is based on univariate cylindrical algebraic decomposition .',\n",
       " \"we follow the approach of untrusted certificates , separating solving from verifying : efficient external tools perform expensive real algebraic computations , producing evidence that is formally checked within isabelle 's logic .\",\n",
       " 'this allows us to exploit highly-tuned computer algebra systems like mathematica to guide our procedure without impacting the correctness of its results .',\n",
       " 'we present experiments demonstrating the efficacy of this approach , in many cases yielding orders of magnitude improvements over previous methods .',\n",
       " 'in this report , we present our findings from benchmarking experiments for information extraction on historical handwritten marriage records esposalles from iehhr - icdar 2017 robust reading competition .',\n",
       " 'the information extraction is modeled as semantic labeling of the sequence across 2 set of labels .',\n",
       " 'this can be achieved by sequentially or jointly applying handwritten text recognition ( htr ) and named entity recognition ( ner ) .',\n",
       " 'we deploy a pipeline approach where first we use newest htr and use its output as input for ner .',\n",
       " 'we show that given low resource setup and simple structure of the records , high performance of htr ensures overall high performance .',\n",
       " 'we explore the various configurations of conditional random fields and neural networks to standard ner on given certain noisy input .',\n",
       " 'the best model on 10-fold cross-validation as well as blind test data uses n-gram features with bidirectional long short-term memory .',\n",
       " 'we introduce a hybrid ( discrete -- continuous ) safety controller which enforces strict state and input constraints on a system -- -but only acts when necessary , preserving transparent operation of the original system within some safe region of the state space .',\n",
       " 'we define this space using a min-quadratic barrier function , which we construct along the equilibrium manifold using the lyapunov functions which result from linear matrix inequality controller synthesis for locally valid uncertain linearizations .',\n",
       " 'we also introduce the concept of a barrier pair , which makes it easy to extend the approach to include trajectory-based augmentations to the safe region , in the style of lqr-trees .',\n",
       " 'we demonstrate our controller and barrier pair synthesis method in simulation-based examples .',\n",
       " 'this paper structures a novel vision for olap by fundamentally redefining several of the pillars on which olap has been based for the last 20 years .',\n",
       " \"we redefine olap query , in order to move to higher degrees of abstraction from roll-up 's and drill-down 's , and we propose a set of novel intentional olap operators , namely , describe , assess , explain , predict , and suggest , which express the user 's need for results .\",\n",
       " 'we fundamentally redefine what a query answer is , and escape from the constraint that the answer is a set of tuples ; on the contrary , we complement the set of tuples with models ( typically , but not exclusively , results of data mining algorithms over the involved data ) that concisely represent the internal structure or correlations of the data .',\n",
       " 'due to the diverse nature of the involved models , we come up ( for the first time ever , to the best of our knowledge ) with a unifying framework for them , that places its pillars on the extension of each data cell of a cube with information about the models that pertain to it -- practically converting the small parts that build up the models to data that annotate each cell .',\n",
       " 'we exploit this data-to-model mapping to provide highlights of the data , by isolating data and models that maximize the delivery of new information to the user .',\n",
       " 'we introduce a novel method for assessing the surprise that a new query result brings to the user , with respect to the information contained in previous results the user has seen via a new interestingness measure .',\n",
       " 'the individual parts of our proposal are integrated in a new data model for olap , which we call the intentional analytics model .',\n",
       " 'we complement our contribution with a list of significant open problems for the community to address .',\n",
       " 'in this paper , we propose a novel framework for performance optimization in internet of things ( iot ) -based next-generation wireless sensor networks .',\n",
       " 'in particular , a computationally-convenient system is presented to combat two major research problems in sensor networks .',\n",
       " 'first is the conventionally-tackled resource optimization problem which triggers the drainage of battery at a faster rate within a network .',\n",
       " 'such drainage promotes inefficient resource usage thereby causing sudden death of the network .',\n",
       " 'the second main bottleneck for such networks is that of data degradation .',\n",
       " 'this is because the nodes in such networks communicate via a wireless channel , where the inevitable presence of noise corrupts the data making it unsuitable for practical applications .',\n",
       " 'therefore , we present a layer-adaptive method via 3-tier communication mechanism to ensure the efficient use of resources .',\n",
       " 'this is supported with a mathematical coverage model that deals with the formation of coverage holes .',\n",
       " 'we also present a transform-domain based robust algorithm to effectively remove the unwanted components from the data .',\n",
       " 'our proposed framework offers a handy algorithm that enjoys desirable complexity for real-time applications as shown by the extensive simulation results .',\n",
       " 'recently , there have been increasing demands to construct compact deep architecture to remove unnecessary redundancy and to improve the inference speed .',\n",
       " 'while many recent works focus on reducing the redundancy by eliminating unneeded weight parameters , it is not possible to apply a single deep architecture for multiple devices with different resources .',\n",
       " 'when a new device or circumstantial condition requires a new deep architecture , it is necessary to construct and train a new network from scratch .',\n",
       " 'in this work , we propose a novel deep learning framework , called a nested sparse network , which exploits an n-in-1-type nested structure in a neural network .',\n",
       " 'a nested sparse network consists of multiple levels of networks with a different sparsity ratio associated with each level , and higher level networks share parameters with lower level networks to enable stable nested learning .',\n",
       " 'the proposed framework realizes a resource-aware versatile architecture as the same network can meet diverse resource requirements .',\n",
       " 'moreover , the proposed nested network can learn different forms of knowledge in its internal networks at different levels , enabling multiple tasks using a single network , such as coarse-to-fine hierarchical classification .',\n",
       " 'in order to train the proposed nested sparse network , we propose efficient weight connection learning and channel and layer scheduling strategies .',\n",
       " 'we evaluate our network in multiple tasks , including adaptive deep compression , knowledge distillation , and learning class hierarchy , and demonstrate that nested sparse networks perform competitively , but more efficiently , compared to existing methods .',\n",
       " 'we present a quasi-experiment to investigate whether , and to what extent , sleep deprivation impacts the performance of novice software developers using the agile practice of test-first development ( tfd ) .',\n",
       " 'we recruited 45 undergraduates and asked them to tackle a programming task .',\n",
       " 'among the participants , 23 agreed to stay awake the night before carrying out the task , while 22 slept usually .',\n",
       " 'we analyzed the quality ( i.e. , the functional correctness ) of the implementations delivered by the participants in both groups , their engagement in writing source code ( i.e. , the amount of activities performed in the ide while tackling the programming task ) and ability to apply tfd ( i.e. , the extent to which a participant can use this practice ) .',\n",
       " 'by comparing the two groups of participants , we found that a single night of sleep deprivation leads to a reduction of 50 % in the quality of the implementations .',\n",
       " \"there is important evidence that the developers ' engagement and their prowess to apply tfd are negatively impacted .\",\n",
       " 'our results also show that sleep-deprived developers make more fixes to syntactic mistakes in the source code .',\n",
       " 'we conclude that sleep deprivation has possibly disruptive effects on software development activities .',\n",
       " \"the results open opportunities for improving developers ' performance by integrating the study of sleep with other psycho-physiological factors in which the software engineering research community has recently taken an interest in .\",\n",
       " 'high-order parametric models that include terms for feature interactions are applied to various data mining tasks , where ground truth depends on interactions of features .',\n",
       " 'however , with sparse data , the high- dimensional parameters for feature interactions often face three issues : expensive computation , difficulty in parameter estimation and lack of structure .',\n",
       " 'previous work has proposed approaches which can partially re- solve the three issues .',\n",
       " 'in particular , models with factorized parameters ( e.g .',\n",
       " 'factorization machines ) and sparse learning algorithms ( e.g .',\n",
       " 'ftrl-proximal ) can tackle the first two issues but fail to address the third .',\n",
       " 'regarding to unstructured parameters , constraints or complicated regularization terms are applied such that hierarchical structures can be imposed .',\n",
       " 'however , these methods make the optimization problem more challenging .',\n",
       " 'in this work , we propose strongly hierarchical factorization machines and anova kernel regression where all the three issues can be addressed without making the optimization problem more difficult .',\n",
       " 'experimental results show the proposed models significantly better the newest in two data mining tasks : cold-start user response time prediction and stock volatility prediction .',\n",
       " 'information technology ( it ) significantly impacts the environment throughout its life cycle .',\n",
       " 'most enterprises have not paid enough attention to this until recently .',\n",
       " \"it 's environmental impact can be significantly reduced by behavioral changes , as well as technology changes .\",\n",
       " 'given the relative energy and materials inefficiency of most it infrastructures today , many green it initiatives can be easily tackled at no incremental cost .',\n",
       " 'the green grid - a non-profit trade organization of it professionals is such an initiative , formed to initiate the issues of power and cooling in data centers , scattered world-wide .',\n",
       " 'the green grid seeks to define best practices for optimizing the efficient consumption of power at it equipment and facility levels , as well as the manner in which cooling is delivered at these levels hence , providing promising attitude in bringing down the environmental hazards , as well as proceeding to the new era of green computing .',\n",
       " 'in this paper we review the various analytical aspects of the green grid upon the data centers and found green facts .',\n",
       " 'sparse subspace cluster ( ssc ) has been used extensively for subspace identification tasks due to its theoretical guarantees and relative ease of implementation .',\n",
       " 'however ssc has quadratic computation and memory requirements with respect to the number of input data points .',\n",
       " 'this burden has prohibited sscs use for all but the smallest data .',\n",
       " 'to overcome this we propose a new method , k-ssc , that screens out a large number of data points to both reduce ssc to linear memory and computational requirements .',\n",
       " 'we provide theoretical analysis for the bounds of success for k-ssc .',\n",
       " 'our experiments show that k-ssc exceeds theoretical expectations and better existing ssc approximations by maintaining the classification performance of ssc .',\n",
       " 'furthermore in the spirit of reproducible research we have publicly released the source code for k-ssc',\n",
       " 'this paper describes a set of neural network architecture , called prediction neural networks set ( pnns ) , based on both fully-connected and technical neural networks , for intra image prediction .',\n",
       " 'the choice of neural network for predicting a given image block depends on the block size , hence does not need to be signalled to the decoder .',\n",
       " 'it is shown that , while fully-connected neural networks give good performance for small block sizes , technical neural networks provide better predictions in large blocks with complex textures .',\n",
       " 'thanks to the use of masks of random sizes during training , the neural networks of pnns well adapt to the available context that may vary , depending on the position of the image block to be predicted .',\n",
       " 'when integrating pnns into a h.265 codec , psnr-rate performance gains going from 1.46 % to 5.20 % are obtained .',\n",
       " 'these gains are on average 0.99 % larger than those of prior neural network based methods .',\n",
       " 'unlike the h.265 intra prediction modes , which are each specialized in predicting a specific texture , the proposed pnns can model a large set of complex textures .',\n",
       " 'we discuss several new results on nonnegative approximate controllability for the one-dimensional heat equation governed by either multiplicative or nonnegative additive control , acting within a proper subset of the space domain at every moment of time .',\n",
       " 'our methods allow us to link these two types of controls to some extend .',\n",
       " 'the main results include approximate controllability properties both for the static and mobile control supports .',\n",
       " 'machine learning is increasingly targeting areas where input data can not be accurately described by a single vector , but can be modeled instead using the more flexible concept of random vectors , namely probability measures or more simply point clouds of varying cardinality .',\n",
       " 'using deep architecture on measures poses , however , many challenging issues .',\n",
       " 'indeed , deep architecture are originally designed to handle fixedlength vectors , or , using recursive mechanisms , ordered sequences thereof .',\n",
       " 'in sharp contrast , measures describe a varying number of weighted observations with no particular order .',\n",
       " 'we propose in this work a deep framework designed to handle crucial aspects of measures , namely permutation invariances , variations in weights and cardinality .',\n",
       " 'architecture derived from this pipeline can ( i ) map measures to measures - using the concept of push-forward operators ; ( ii ) bridge the gap between measures and euclidean spaces - through integration steps .',\n",
       " 'this allows to design discriminative networks ( to classify or reduce the dimensionality of input measures ) , generate architecture ( to synthesize measures ) and technical pipelines ( to predict measure dynamics ) .',\n",
       " \"we provide a theoretical analysis of these building blocks , review our architecture ' approximation abilities and robust w.r.t . perturbation , and try them on various discriminative and generate tasks .\",\n",
       " 'during the life span of large software projects , developers often apply the same code changes to different code locations in slight variations .',\n",
       " 'since the application of these changes to all locations is time-consuming and error-prone , tools exist that learn change patterns from input examples , search for possible pattern applications , and generate corresponding recommendations .',\n",
       " 'in many cases , the generated recommendations are syntactically or semantically wrong due to code movements in the input examples .',\n",
       " 'thus , they are of low accuracy and developers can not directly copy them into their projects without adjustments .',\n",
       " 'we present the accurate recommendation system ( ares ) that achieve a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations .',\n",
       " 'on average , the recommendations by ares have an accuracy of 96 % with respect to code changes that developers have manually performed in commits of source code archives .',\n",
       " 'at the same time ares achieve precision and recall values that are on par with other tools .',\n",
       " 'e-learning is efficient , task relevant and just-in-time learning grown from the learning requirements of the new and dynamically changing world .',\n",
       " 'the term semantic web covers the steps to create a new www architecture that augments the content with formal semantics enabling better possibilities of navigation through the cyberspace and its contents .',\n",
       " 'in this paper , we present the semantic web-based model for our e-learning system taking into account the learning environment at saudi arabian universities .',\n",
       " 'the proposed system is mainly based on ontology-based descriptions of content , context and structure of the learning materials .',\n",
       " 'it further provides flexible and personalized access to these learning materials .',\n",
       " 'the framework has been validated by an interview based qualitative method .',\n",
       " 'artificial intelligence ( ai ) has achieved superhuman performance in a growing number of tasks , but understanding and explaining ai remain challenging .',\n",
       " 'this paper clarifies the connections between machine-learning algorithms to develop ais and the econometrics of dynamic structural models through the case studies of three famous game ais .',\n",
       " \"chess-playing deep blue is a calibrated value function , whereas shogi-playing bonanza is an estimated value function via rust 's ( 1987 ) nested fixed-point method .\",\n",
       " \"alphago 's `` supervised-learning policy network '' is a deep neural network implementation of hotz and miller 's ( 1993 ) conditional choice probability estimation ; its `` reinforcement-learning value network '' is equivalent to hotz , miller , sanders , and smith 's ( 1994 ) conditional choice simulation method .\",\n",
       " \"relaxing these ais ' implicit econometric assumptions would improve their structural interpretability .\",\n",
       " 'the assumption that training and testing samples are generated from the same distribution does not always hold for world machine-learning applications .',\n",
       " 'the procedure of tackling this discrepancy between the training ( source ) and testing ( target ) domains is known as domain adaptation .',\n",
       " 'we propose an technical version of domain adaptation that considers the presence of only unlabelled data in the target domain .',\n",
       " 'our approach centers on finding correspondences between samples of each domain .',\n",
       " 'the correspondences are obtained by treating the source and target samples as graphs and using a convex criterion to match them .',\n",
       " 'the criteria used are first-order and second-order similarities between the graphs as well as a class-based regularization .',\n",
       " 'we have also developed a computationally efficient routine for the convex optimization , thus allowing the proposed method to be used widely .',\n",
       " 'to verify the effectiveness of the proposed method , computer simulations were conducted on synthetic , image classification and sentiment classification data .',\n",
       " 'results validated that the proposed local sample-to-sample matching method out-performs traditional moment-matching methods and is competitive with respect to current local domain-adaptation methods .',\n",
       " \"the novel `` volume-enclosing surface extraction algorithm '' ( vesta ) generates triangular isosurfaces from computed tomography volumetric images and/or three-dimensional ( 3d ) simulation data .\",\n",
       " 'here , we present various benchmarks for gpu-based code implementations of both vesta and the current newest marching cubes algorithm ( mca ) .',\n",
       " 'one major result of this study is that vesta runs significantly faster than the mca .',\n",
       " 'geometric model fitting is a fundamental task in computer graphics and computer vision .',\n",
       " 'however , most geometric model fitting methods are unable to fit an arbitrary geometric model ( e.g . a surface with holes ) to incomplete data , due to that the similarity metric used in these methods are unable to measure the rigid partial similarity between arbitrary models .',\n",
       " 'this paper hence proposes a novel rigid geometric similarity metric , which is able to measure both the full similarity and the partial similarity between arbitrary geometric models .',\n",
       " 'the proposed metric enables us to perform partial procedural geometric model fitting ( ppgmf ) .',\n",
       " 'the task of ppgmf is to search a procedural geometric model space for the model rigidly similar to a query of non-complete point set .',\n",
       " 'models in the procedural model space are generated according to a set of parametric modeling rules .',\n",
       " 'a typical query is a point cloud .',\n",
       " 'ppgmf is very useful as it can be used to fit arbitrary geometric models to non-complete ( incomplete , over-complete or hybrid-complete ) point cloud data .',\n",
       " 'for example , most laser scanning data is non-complete due to occlusion .',\n",
       " 'our ppgmf method uses markov chain monte carlo technique to optimize the proposed similarity metric over the model space .',\n",
       " 'to accelerate the optimization process , the method also employs a novel coarse-to-fine model dividing strategy to reject dissimilar models in advance .',\n",
       " 'our method has been demonstrated on a variety of geometric models and non-complete data .',\n",
       " 'experimental results show that the ppgmf method based on the proposed metric is able to fit non-complete data , while the method based on other metric is unable .',\n",
       " 'it is also shown that our method can be accelerated by several times via early rejection .',\n",
       " 'secure spontaneous authentication between devices worn at arbitrary location on the same body is a challenging , yet unsolved problem .',\n",
       " 'we propose bandana , the first-ever implicit secure device-to-device authentication scheme for devices worn on the same body .',\n",
       " 'our approach leverages instantaneous variation in acceleration patterns from gait sequences to extract always-fresh secure secrets .',\n",
       " 'it enables secure spontaneous pairing of devices worn on the same body or interacted with .',\n",
       " 'the method is robust against noise in sensor readings and active attackers .',\n",
       " 'we demonstrate the robust of bandana on two gait data and discuss the discriminability of intra- and inter-body cases , robust to statistical bias , as well as possible attack scenarios .',\n",
       " 'along with the emergence and popularity of social communications on the internet , topic discovery from short texts becomes fundamental to many applications that require semantic understanding of textual content .',\n",
       " 'as a rising research field , short text topic modeling presents a new and complementary algorithmic methodology to supplement regular text topic modeling , especially targets to limited word co-occurrence information in short texts .',\n",
       " 'this paper presents the first comprehensive open-source package , called sttm , for use in java that integrates the newest models of short text topic modeling algorithms , standard data , and abundant functions for model inference and evaluation .',\n",
       " 'the package is designed to facilitate the expansion of new methods in this research field and make evaluations between the new approaches and existing ones accessible .',\n",
       " 'sttm is open-sourced at https : //github.com/qiang2100/sttm .',\n",
       " 'based on the in-depth analysis of the essence and features of vague phenomena , this paper focuses on establishing the axiomatical foundation of membership degree theory for vague phenomena , presents an axiomatic system to govern membership degrees and their interconnections .',\n",
       " 'on this basis , the concept of vague partition is introduced , further , the concept of fuzzy set introduced by zadeh in 1965 is redefined based on vague partition from the perspective of axiomatization .',\n",
       " 'the thesis defended in this paper is that the relationship among vague attribute values should be the starting point to recognize and model vague phenomena from a quantitative view .',\n",
       " 'we consider a variation of construction a of lattices from linear codes based on two classes of number fields , totally real and cm galois number fields .',\n",
       " 'we propose a generic construction with explicit generator and gram matrices , then focus on modular and unimodular lattices , obtained in the particular cases of totally real , respectively , imaginary , quadratic fields .',\n",
       " 'our motivation comes from coding theory , thus some relevant properties of modular lattices , such as minimal norm , theta series , kissing number and secrecy gain are analyzed .',\n",
       " 'interesting lattices are exhibited .',\n",
       " 'current approaches in video forecasting attempt to generate videos directly in pixel space using generate adversary networks ( gans ) or variation autoencoders ( vaes ) .',\n",
       " 'however , since these approaches try to model all the structure and scene dynamics at once , in unconstrained settings they often generate uninterpretable results .',\n",
       " 'our insight is to model the forecasting problem at a higher level of abstraction .',\n",
       " 'specifically , we exploit human pose detectors as a free source of supervision and break the video forecasting problem into two discrete steps .',\n",
       " 'first we explicitly model the high level structure of active objects in the scene -- -humans -- -and use a vae to model the possible future movements of humans in the pose space .',\n",
       " 'we then use the future poses generated as conditional information to a gan to predict the future frames of the video in pixel space .',\n",
       " 'by using the structured space of pose as an intermediate representation , we sidestep the problems that gans have in generating video pixels directly .',\n",
       " 'we show through quantitative and qualitative evaluation that our method better newest methods for video prediction .',\n",
       " 'objective : predict patient-specific vitals deemed medically acceptable for discharge from a pediatric intensive care unit ( icu ) .',\n",
       " \"design : the means of each patient 's hr , sbp and dbp measurements between their medical and physical discharge from the icu were computed as a proxy for their physiologically acceptable state space ( pass ) for successful icu discharge .\",\n",
       " 'these individual pass values were compared via root mean squared error ( rmse ) to population age-normal vitals , a polynomial regression through the pass values of a pediatric icu ( picu ) population and predictions from two technical neural network models designed to predict personalized pass within the first twelve hours following icu admission .',\n",
       " \"setting : picu at children 's hospital los angeles ( chla ) .\",\n",
       " 'patients : 6,899 picu episodes ( 5,464 patients ) collected between 2009 and 2016 .',\n",
       " 'interventions : none .',\n",
       " 'measurements : each episode data contained 375 variables representing vitals , labs , interventions , and drugs .',\n",
       " 'they also included a time indicator for picu medical discharge and physical discharge .',\n",
       " 'main results : the rmses between individual pass values and population age-normals ( hr : 25.9 bpm , sbp : 13.4 mmhg , dbp : 13.0 mmhg ) were larger than the rmses corresponding to the polynomial regression ( hr : 19.1 bpm , sbp : 12.3 mmhg , dbp : 10.8 mmhg ) .',\n",
       " 'the rmses from the best performing technical model were the lowest ( hr : 16.4 bpm ; sbp : 9.9 mmhg , dbp : 9.0 mmhg ) .',\n",
       " 'conclusion : picu patients are a unique subset of the general population , and general age-normal vitals may not be suitable as target values indicating physiologic stability at discharge .',\n",
       " \"age-normal vitals that were specifically derived from the medical-to-physical discharge window of icu patients may be more appropriate targets for 'acceptable ' physiologic state for critical care patients .\",\n",
       " 'going beyond simple age bins , an technical model can provide more personalized target values .',\n",
       " 'crowdsourcing has become very popular among the machine learning community as a way to obtain labels that allow a ground truth to be estimated for a given data .',\n",
       " 'in most of the approaches that use crowdsourced labels , annotators are asked to provide , for each presented instance , a single class label .',\n",
       " 'such a request could be inefficient , that is , considering that the labelers may not be experts , that way to proceed could fail to take real advantage of the knowledge of the labelers .',\n",
       " 'in this paper , the use of candidate labeling for crowd learning is proposed , where the annotators may provide more than a single label per instance to try not to miss the real label .',\n",
       " 'the main hypothesis is that , by allowing candidate labeling , knowledge can be extracted from the labelers more efficiently by than in the standard crowd learning scenario .',\n",
       " 'empirical evidence which supports that hypothesis is presented .',\n",
       " 'extreme learning machine ( elm ) is an extremely fast learning method and has a powerful performance for pattern recognition tasks proven by enormous researches and engineers .',\n",
       " 'however , its good generalization ability is built on large numbers of hidden neurons , which is not beneficial to real time response in the test process .',\n",
       " \"in this paper , we proposed new ways , named `` constrained extreme learning machines '' ( celms ) , to randomly select hidden neurons based on sample distribution .\",\n",
       " 'compared to completely random selection of hidden nodes in elm , the celms randomly select hidden nodes from the constrained vector space containing some basic combinations of original sample vectors .',\n",
       " 'the experimental results show that the celms have better generalization ability than traditional elm , svm and some other related methods .',\n",
       " 'additionally , the celms have a similar fast learning speed as elm .',\n",
       " 'connection calculi allow for very compact implementations of goal-directed proof search .',\n",
       " 'we give an overview of our work related to connection tableaux calculi : first , we show optimised functional implementations of clausal and nonclausal proof search , including a consistent skolemisation procedure for machine learning .',\n",
       " 'then , we show two guidance methods based on machine learning , namely reordering of proof steps with naive technical probablities , and expansion of a proof search tree with monte carlo tree search .',\n",
       " 'finally , we give a translation of connection proofs to lk , enabling proof certification and automatic proof search in interactive theorem provers .',\n",
       " 'the major aim of this survey is to identify the strengths and weaknesses of a representative set of data-mining and integration ( dmi ) query languages .',\n",
       " 'we describe a set of properties of dmi-related languages that we use for a systematic evaluation of these languages .',\n",
       " 'in addition , we introduce a scoring system that we use to quantify our opinion on how well a dmi-related language supports a property .',\n",
       " 'the languages surveyed in this paper include : dmql , minesql , msql , m2mql , dmfsql , oledb for dm , mine rule , and oracle data mining .',\n",
       " 'this survey may help researchers to propose a dmi language that is beyond the newest , or it may help practitioners to select an existing language that fits well a purpose .',\n",
       " 'in this paper we describe the overall idea and results of a recently proposed radio access technique based on filter bank multicarrier ( fbmc ) communication system using two orthogonal polarizations : dual-polarization fbmc ( dp-fbmc ) .',\n",
       " 'using this system we can alleviate the intrinsic interference problem in fbmc systems .',\n",
       " 'this enables use of all the multicarrier techniques used in cyclic-prefix orthogonal frequency-division multiplexing ( cp-ofdm ) systems for channel equalization , multiple-input/multiple-output ( mimo ) processing , etc. , without using the extra processing required for conventional fbmc .',\n",
       " 'dp-fbmc also provides other interesting advantages over cp-ofdm and fbmc such as more robust in multipath fading channels , and more robust to receiver carrier frequency offset ( cfo ) and timing offset ( to ) .',\n",
       " 'for dp-fbmc we propose three different structures based on different multiplexing techniques in time , frequency , and polarization .',\n",
       " 'we will show that one of these structures has exactly the same system complexity and equipment as conventional fbmc .',\n",
       " 'in our simulation results dp-fbmc has better bit error ratio ( ber ) performance in dispersive channels .',\n",
       " 'based on these results , dp-fbmc has potential as a promising candidate for future wireless communication systems .',\n",
       " 'cooperative multi-agent planning ( map ) is a relatively recent research field that combines technologies , algorithms and techniques developed by the artificial intelligence planning and multi-agent systems communities .',\n",
       " 'while planning has been generally treated as a single-agent task , map generalizes this concept by considering multiple intelligent agents that work cooperatively to develop a course of action that satisfies the goals of the group .',\n",
       " 'this paper reviews the most relevant approaches to map , putting the focus on the solvers that took part in the 2015 competition of distributed and multi-agent planning , and classifies them according to their key features and relative performance .',\n",
       " 'computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which : ( i ) an agent can perceive , navigate , and take actions guided by ai algorithms , ( ii ) properties of the worlds can be modified ( e.g. , material and reflectance ) , ( iii ) physical simulations can be performed , and ( iv ) algorithms can be learnt and evaluated .',\n",
       " 'but creating realistic virtual worlds is not easy .',\n",
       " 'the game industry , however , has spent a lot of effort creating 3d worlds , which a player can interact with .',\n",
       " 'so researchers can build on these resources to create virtual worlds , provided we can access and modify the internal data structures of the games .',\n",
       " 'to enable this we created an open-source plugin unrealcv ( http : //unrealcv.github.io ) for a popular game engine unreal engine 4 ( ue4 ) .',\n",
       " 'we show two applications : ( i ) a proof of concept image data , and ( ii ) linking caffe with the virtual world to test deep network algorithms .',\n",
       " 'three-dimensional ( 3d ) biomedical image sets are often acquired with in-plane pixel spacings that are far less than the out-of-plane spacings between images .',\n",
       " 'the resultant anisotropy , which can be detrimental in many applications , can be decreased using image interpolation .',\n",
       " 'optical flow and/or other registration-based interpolators have proven useful in such interpolation roles in the past .',\n",
       " 'when acquired images are comprised of signals that describe the flow velocity of fluids , additional information is available to guide the interpolation process .',\n",
       " 'in this paper , we present an optical-flow based framework for image interpolation that also minimizes resultant divergence in the interpolated data .',\n",
       " 'in this paper we present a new and simple language-independent method for word-alignment based on the use of external sources of bilingual information such as machine translation systems .',\n",
       " 'we show that the few parameters of the aligner can be trained on a very small corpus , which leads to results comparable to those obtained by the newest tool giza++ in terms of precision .',\n",
       " 'regarding other metric , such as alignment error rate or f-measure , the parametric aligner , when trained on a very small gold-standard ( 450 pairs of sentences ) , provides results comparable to those produced by giza++ when trained on an in-domain corpus of around 10,000 pairs of sentences .',\n",
       " \"furthermore , the results obtained indicate that the training is domain-independent , which enables the use of the trained aligner 'on the fly ' on any new pair of sentences .\",\n",
       " 'conventionally , selective harmonic elimination ( she ) method in 2-level inverters , finds best switching angles to reach first voltage harmonic to reference level and eliminate other harmonics , simultaneously .',\n",
       " 'considering induction motor ( im ) as the inverter load , and wide dc bus voltage variations , the inverter must operate in both over-modulation and linear modulation region .',\n",
       " 'main objective of the modified she is to reduce harmonic torques through finding the best switching angles .',\n",
       " 'in this paper , optimization is based on optimizing phasor equations in which harmonic torques are calculated .',\n",
       " 'the procedure of this method is that , first , the ratio of the same torque harmonics is estimated , secondly , by using that estimation , the ratio of voltage harmonics that generates homogeneous torques is calculated .',\n",
       " 'for the estimation and the calculation of the ratios motor parameter , mechanical speed of the rotor , the applied frequency , and the concept of slip are used .',\n",
       " 'the advantage of this approach is highlighted when mechanical load and dc bus voltage variations are taken into consideration .',\n",
       " 'simulation results are presented under a wide range of working conditions in an induction motor to demonstrate the effectiveness of the proposed method .',\n",
       " 'research on optical tempest has moved forward since 2002 when the first pair of papers on the subject emerged independently and from widely separated locations in the world within a week of each other .',\n",
       " 'since that time , vulnerabilities have evolved along with systems , and several new threat vectors have consequently appeared .',\n",
       " 'although the supply chain ecosystem of ethernet has reduced the vulnerability of billions of devices through use of standardised phy solutions , other recent trends including the internet of things ( iot ) in both industrial settings and the general population , high frequency trading ( hft ) in the financial sector , the european general data protection regulation ( gdpr ) , and inexpensive drones have made it relevant again for consideration in the design of new products for privacy .',\n",
       " 'one of the general principles of security is that vulnerabilities , once fixed , sometimes do not stay that way .',\n",
       " 'analyzing videos of human actions involves understanding the temporal relationships among video frames .',\n",
       " 'newest action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for technical .',\n",
       " 'such a two-stage approach is computationally expensive , storage demanding , and not end-to-end trainable .',\n",
       " 'in this paper , we present a novel cnn architecture that implicitly captures motion information between adjacent frames .',\n",
       " 'we name our approach hidden two-stream technical because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow .',\n",
       " 'our end-to-end approach is 10x faster than its two-stage baseline .',\n",
       " 'experimental results on four challenging action recognition data : ucf101 , hmdb51 , thumos14 and activitynet v1.2 show that our approach significantly better the previous best real-time approaches .',\n",
       " 'context : pre-publication peer review of scientific articles is considered a key element of the research process in software engineering , yet it is often perceived as not to work fully well .',\n",
       " \"objective : we aim at understanding the perceptions of and attitudes towards peer review of authors and reviewers at one of software engineering 's most prestigious venues , the international conference on software engineering ( icse ) .\",\n",
       " 'method : we invited 932 icse 2014/15/16 authors and reviewers to participate in a survey with 10 closed and 9 open questions .',\n",
       " 'results : we present a multitude of results , such as : respondents perceive only one third of all reviews to be good , yet one third as useless or misleading ; they propose double-blind or zero-blind reviewing regimes for improvement ; they would like to see showable proofs of ( good ) reviewing work be introduced ; attitude change trends are weak .',\n",
       " 'conclusion : the perception of the current state of software engineering peer review is fairly negative .',\n",
       " 'also , we found hardly any trend that suggests reviewing will improve by itself over time ; the community will have to make explicit efforts .',\n",
       " 'fortunately , our ( mostly senior ) respondents appear more open for trying different peer reviewing regimes than we had expected .',\n",
       " 'inspired by previous work of shoup , lenstra-de smit and couveignes-lercier , we give fast algorithms to compute in ( the first levels of ) the ell-adic closure of a finite field .',\n",
       " 'in many cases , our algorithms have quasi-linear complexity .',\n",
       " 'technical neural networks are designed for dense data , but vision data is often sparse ( stereo depth , point clouds , pen stroke , etc . ) .',\n",
       " 'we present a method to handle sparse depth data with optional dense rgb , and accomplish depth completion and semantic segment changing only the last layer .',\n",
       " 'our proposal efficiently learns sparse features without the need of an additional validity mask .',\n",
       " 'we show how to ensure network robust to varying input sparsities .',\n",
       " 'our method even works with densities as low as 0.8 % ( 8 layer lidar ) , and better all published newest on the kitti depth completion standard .',\n",
       " 'we present shrinking horizon model predictive control ( shmpc ) for discrete-time linear systems with signal temporal logic ( stl ) specification constraints under random disturbances .',\n",
       " 'the control objective is to maximize an optimization function under the restriction that a given stl specification is satisfied with high probability against random uncertainties .',\n",
       " 'we formulate a general solution , which does not require precise knowledge of the probability distributions of the ( possibly dependent ) random disturbances ; only the bounded support intervals of the density functions and moment intervals are used .',\n",
       " 'for the specific case of disturbances that are independent and normally distributed , we optimize the controllers further by utilizing knowledge of the disturbance probability distributions .',\n",
       " 'we show that in both cases , the control law can be obtained by solving optimization problems with linear constraints at each step .',\n",
       " 'we experimentally demonstrate effectiveness of this approach by synthesizing a controller for an hvac system .',\n",
       " 'bugs that surface in mobile applications can be difficult to reproduce and fix due to several confounding factors including the highly gui-driven nature of mobile apps , varying contextual states , differing platform versions and device fragmentation .',\n",
       " 'it is clear that developers need support in the form of automated tools that allow for more precise reporting of application defects in order to facilitate more efficient and effective bug fixes .',\n",
       " 'in this paper , we present a tool aimed at supporting application testers and developers in the process of on-device bug reporting .',\n",
       " 'our tool , called odbr , leverages the uiautomator framework and low-level event stream capture to offer support for recording and replaying a series of input gesture and sensor events that describe a bug in an android application .',\n",
       " 'evolution sculpts both the body plans and nervous systems of agents together over time .',\n",
       " \"in contrast , in ai and robotics , a robot 's body plan is usually designed by hand , and control policies are then optimized for that fixed design .\",\n",
       " 'the task of simultaneously co-optimizing the morphology and controller of an embodied robot has remained a challenge .',\n",
       " 'in psychology , the theory of embodied cognition posits that behavior arises from a close coupling between body plan and sensorimotor control , which suggests why co-optimizing these two subsystems is so difficult : most evolutionary changes to morphology tend to adversely impact sensorimotor control , leading to an overall decrease in behavioral performance .',\n",
       " \"here , we further examine this hypothesis and demonstrate a technique for `` morphological innovation protection '' , which temporarily reduces selection pressure on recently morphologically-changed individuals , thus enabling evolution some time to `` readapt '' to the new morphology with subsequent control policy mutations .\",\n",
       " 'we show the potential for this method to avoid local optima and converge to similar highly fit morphologies across widely varying initial conditions , while sustaining fitness improvements further into optimization .',\n",
       " 'while this technique is admittedly only the first of many steps that must be taken to achieve scalable optimization of embodied machines , we hope that theoretical insight into the cause of evolutionary stagnation in current methods will help to enable the automation of robot design and behavioral training -- while simultaneously providing a testbed to investigate the theory of embodied cognition .',\n",
       " 'we propose a new design for a cellular neural network with spintronic neurons and cmos-based synapses .',\n",
       " 'harnessing the magnetoelectric and inverse rashba-edelstein effects allows natural emulation of the behavior of an ideal cellular network .',\n",
       " 'this combination of effects offers an increase in speed and efficiency over other spintronic neural networks .',\n",
       " 'a rigorous performance analysis via simulation is provided .',\n",
       " 'integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity .',\n",
       " 'however , this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm , and in sufficiently complex environments , the dynamics model will almost always be imperfect .',\n",
       " 'as a result , a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance .',\n",
       " 'we propose random ensemble value expansion ( steve ) , a novel model-based technique that addresses this issue .',\n",
       " 'by dynamically interpolating between model rollouts of various horizon lengths for each individual example , steve ensures that the model is only utilized when doing so does not introduce significant errors .',\n",
       " 'our approach better model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency , and in contrast to previous model-based approaches , performance does not degrade in complex environments .',\n",
       " 'the internet of things ( iot ) being a promising technology of the future is expected to connect billions of devices .',\n",
       " 'the increased number of communication is expected to generate mountains of data and the security of data can be a threat .',\n",
       " 'the devices in the architecture are essentially smaller in size and low powered .',\n",
       " 'conventional encryption algorithms are generally computationally expensive due to their complexity and requires many rounds to encrypt , essentially wasting the constrained energy of the gadgets .',\n",
       " 'less complex algorithm , however , may compromise the desired integrity .',\n",
       " 'in this paper we propose a lightweight encryption algorithm named as secure iot ( sit ) .',\n",
       " 'it is a 64-bit block cipher and requires 64-bit key to encrypt the data .',\n",
       " 'the architecture of the algorithm is a mixture of feistel and a uniform substitution-permutation network .',\n",
       " 'simulations result shows the algorithm provides substantial security in just five encryption rounds .',\n",
       " 'the hardware implementation of the algorithm is done on a low cost 8-bit micro-controller and the results of code size , memory utilization and encryption/decryption execution cycles are compared with standard encryption algorithms .',\n",
       " 'the matlab code for relevant simulations is available online at https : //goo.gl/uw7e0w .',\n",
       " 'social media for news consumption is becoming increasingly popular due to its easy access , fast dissemination , and low cost .',\n",
       " \"however , social media also enable the wide propagation of `` fake news '' , i.e. , news with intentionally false information .\",\n",
       " 'fake news on social media poses significant negative societal effects , and also presents unique challenges .',\n",
       " 'to tackle the challenges , many existing works exploit various features , from a network perspective , to detect and mitigate fake news .',\n",
       " 'in essence , news dissemination ecosystem involves three dimensions on social media , i.e. , a content dimension , a social dimension , and a temporal dimension .',\n",
       " 'in this chapter , we will review network properties for studying fake news , introduce popular network types and how these networks can be used to detect and mitigation fake news on social media .',\n",
       " 'increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code .',\n",
       " 'these vulnerabilities can pose serious risk of exploit and result in system compromise , information leaks , or denial of service .',\n",
       " 'we leveraged the wealth of c and c++ open-source code available to develop a large function-level vulnerability detection system using machine learning .',\n",
       " 'to supplement existing labeled vulnerability data , we compiled a vast data of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits .',\n",
       " 'the labeled data is available at : https : //osf.io/d45bw/ .',\n",
       " 'using these data , we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code .',\n",
       " 'we evaluated our tool on code from both real software packages and the nist sate iv standard data .',\n",
       " 'our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection .',\n",
       " 'the process of recording electroencephalography ( eeg ) signals is onerous , limiting the amount of training data available for downstream machine learning tasks , especially when using deep learning methods .',\n",
       " 'some previous works applied data augmentation techniques that slightly improve the performance , but there is no data like more data .',\n",
       " 'in this work , we propose the event-related potential encoder network ( erpenet ) ; a semi-supervised autoencoder-based model , that can be applied to any erp-related tasks .',\n",
       " 'the strength of erpenet lies in its capability to handle various data across multiple recording setups , enabling joint training across data , thereby increasing the amount of training data .',\n",
       " 'erpenet incorporates technical neural networks ( technical ) and long short-term memory ( technical ) , in an technical autoencoder setup , which tries to compress the input eeg signal into a hidden vector .',\n",
       " 'the network also includes the classification part , consisting of a one-layer fully connected , for attended and unattended events predictions making the network semi-supervised .',\n",
       " 'we experimented on six different p300 data .',\n",
       " 'the results show that the hidden vector exhibits better compression capability than the previous newest model .',\n",
       " \"we also tested erpenet 's capabilities in adapting to unseen data for attended and unattended events classification .\",\n",
       " 'erpenet significantly better xdawn , achieving 79.37 % - 88.52 % accuracy , depending on the data .',\n",
       " 'neural network based architecture used for sound recognition are usually adapted from other application domains , which may not harness sound related properties .',\n",
       " 'the conditional neural network ( clnn ) is designed to consider the relational properties across frames in a temporal signal , and its extension the masked conditional neural network ( mclnn ) embeds a filterbank behavior within the network , which enforces the network to learn in frequency bands rather than bins .',\n",
       " 'additionally , it automates the exploration of different feature combinations analogous to handcrafting the optimum combination of features for a recognition task .',\n",
       " 'we applied the mclnn to the environmental sounds of the esc-10 data .',\n",
       " 'the mclnn achieved competitive accuracies compared to newest technical neural networks and hand-crafted attempts .',\n",
       " 'neural networks with relu activations have achieved great empirical success in various domains .',\n",
       " 'however , existing results for learning relu networks either pose assumptions on the underlying data distribution being e.g .',\n",
       " 'normal , or require the network size and/or training size to be sufficiently large .',\n",
       " 'in this context , the problem of learning a two-layer relu network is approached in a binary classification setting , where the data are linearly separable and a hinge loss criterion is adopted .',\n",
       " 'leveraging the power of random noise , this contribution presents a novel random gradient descent ( sgd ) algorithm , which can provably train any single-hidden-layer relu network to attain global optimality , despite the presence of infinitely many bad local minima and saddle points in general .',\n",
       " 'this result is the first of its kind , requiring no assumptions on the data distribution , training/network size , or initialization .',\n",
       " 'convergence of the resultant iterative algorithm to a global minimum is analyzed by establishing both an upper bound and a lower bound on the number of effective ( non-zero ) updates to be performed .',\n",
       " 'furthermore , generalization guarantees are developed for relu networks trained with the novel sgd .',\n",
       " 'these guarantees highlight a fundamental difference ( at least in the worst case ) between learning a relu network as well as a leaky relu network in terms of sample complexity .',\n",
       " 'numerical tests using synthetic data and real images validate the effectiveness of the algorithm and the practical merits of the theory .',\n",
       " 'deep learning based speech enhancement and source separation systems have recently reached unprecedented levels of quality , to the point that performance is reaching a new ceiling .',\n",
       " 'most systems rely on estimating the magnitude of a target source by estimating a real-valued mask to be applied to a time-frequency representation of the mixture signal .',\n",
       " 'a limiting factor in such approaches is a lack of phase estimation : the phase of the mixture is most often used when reconstructing the estimated time-domain signal .',\n",
       " \"here , we propose `magbook ' , `phasebook ' , and `combook ' , three new types of layers based on discrete representations that can be used to estimate complex time-frequency masks .\",\n",
       " 'magbook layers extend classical sigmoidal units and a recently introduced convex softmax activation for mask-based magnitude estimation .',\n",
       " 'phasebook layers use a similar structure to give an estimate of the phase mask without suffering from phase wrapping issues .',\n",
       " 'combook layers are an alternative to the magbook-phasebook combination that directly estimate complex masks .',\n",
       " 'we present various training and inference regimes involving these representations , and explain in particular how to include them in an end-to-end learning framework .',\n",
       " 'we also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations .',\n",
       " 'we evaluate the proposed methods on the wsj0-2mix data , a well-studied corpus for single-channel speaker-independent speaker separation , matching the performance of newest mask-based approaches without requiring additional phase reconstruction steps .',\n",
       " 'previous research has pointed that software applications should not depend on programmers to provide security for end-users as majority of programmers are not experts of computer security .',\n",
       " \"on the other hand , some studies have revealed that security experts believe programmers have a major role to play in ensuring the end-users ' security .\",\n",
       " \"however , there has been no investigation on what programmers perceive about their responsibility for the end-users ' security of applications they develop .\",\n",
       " \"in this work , by conducting a qualitative experimental study with 40 software developers , we attempted to understand the programmer 's perception on who is responsible for ensuring end-users ' security of the applications they develop .\",\n",
       " \"results revealed majority of programmers perceive that they are responsible for the end-users ' security of applications they develop .\",\n",
       " \"furthermore , results showed that even though programmers aware of things they need to do to ensure end-users ' security , they do not often follow them .\",\n",
       " 'we believe these results would change the current view on the role that different stakeholders of the software development process ( i.e . researchers , security experts , programmers and application programming interface ( api ) developers ) have to play in order to ensure the security of software applications .',\n",
       " 'dbscan is a classical density-based cluster procedure with tremendous practical relevance .',\n",
       " 'however , dbscan implicitly needs to compute the empirical density for each sample point , leading to a quadratic worst-case time complexity , which is too slow on large data .',\n",
       " 'we propose dbscan++ , a simple modification of dbscan which only requires computing the densities for a chosen subset of points .',\n",
       " 'we show empirically that , compared to traditional dbscan , dbscan++ can provide not only competitive performance but also added robust in the bandwidth hyperparameter while taking a fraction of the runtime .',\n",
       " 'we also present statistical consistency guarantees showing the trade-off between computational cost and estimation rates .',\n",
       " 'surprisingly , up to a certain point , we can enjoy the same estimation rates while lowering computational cost , showing that dbscan++ is a sub-quadratic algorithm that attains minimax optimal rates for level-set estimation , a quality that may be of independent interest .',\n",
       " 'generate models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image data .',\n",
       " 'in this paper , we offer contributions in both these areas to enable similar progress in audio modeling .',\n",
       " 'first , we detail a powerful new wavenet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform .',\n",
       " 'second , we introduce nsynth , a large and high-quality data of musical notes that is an order of magnitude larger than comparable public data .',\n",
       " 'using nsynth , we demonstrate improved qualitative and quantitative performance of the wavenet autoencoder over a well-tuned spectral autoencoder baseline .',\n",
       " 'finally , we show that the model learns a manifold of technical that allows for morphing between instruments , meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive .',\n",
       " 'there has been a lot of recent interest in adopting machine learning methods for scientific and engineering applications .',\n",
       " 'this has in large part been inspired by recent successes and advances in the domains of natural language processing ( nlp ) and image classification ( ic ) .',\n",
       " 'however , scientific and engineering problems have their own unique characteristics and requirements raising new challenges for effective design and deployment of machine learning approaches .',\n",
       " 'there is a strong need for further mathematical developments on the foundations of machine learning methods to increase the level of rigor of employed methods and to ensure more reliable and interpretable results .',\n",
       " 'also as reported in the recent literature on newest results and indicated by the no free lunch theorems of statistical learning theory incorporating some form of inductive bias and domain knowledge is essential to success .',\n",
       " 'consequently , even for existing and widely used methods there is a strong need for further mathematical work to facilitate ways to incorporate prior scientific knowledge and related inductive biases into learning frameworks and algorithms .',\n",
       " 'we briefly discuss these topics and discuss some ideas proceeding in this direction .',\n",
       " 'in this paper , we present a set of simulation models to more realistically mimic the behaviour of users reading messages .',\n",
       " 'we propose a user behaviour model , where a simulated user reacts to a message by a flexible set of possible reactions ( e.g . ignore , read , like , save , etc . ) and a mobility-based reaction ( visit a place , run away from danger , etc . ) .',\n",
       " 'we describe our models and their implementation in omnet++ .',\n",
       " 'we strongly believe that these models will significantly contribute to the state of the art of simulating realistically opportunistic networks .',\n",
       " 'recently , neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework .',\n",
       " 'from the optimization perspective , residual connections are adopted to improve learning performance for both encoder and decoder in most of these deep architecture , and advanced attention connections are applied as well .',\n",
       " 'inspired by the success of the densenet model in computer vision problems , in this paper , we propose a densely connected nmt architecture ( densenmt ) that is able to train more efficiently for nmt .',\n",
       " 'the proposed densenmt not only allows dense connection in creating new features for both encoder and decoder , but also uses the dense attention structure to improve attention quality .',\n",
       " 'our experiments on multiple data show that densenmt structure is more competitive and efficient .',\n",
       " 'acoustic event detection for content analysis in most cases relies on lots of labeled data .',\n",
       " 'however , manually annotating data is a time-consuming task , which thus makes few annotated resources available so far .',\n",
       " 'unlike audio event detection , automatic audio tagging , a multi-label acoustic event classification task , only relies on weakly labeled data .',\n",
       " 'this is highly desirable to some practical applications using audio analysis .',\n",
       " 'in this paper we propose to use a fully deep neural network ( dnn ) framework to handle the multi-label classification task in a regression way .',\n",
       " 'considering that only chunk-level rather than frame-level labels are available , the whole or almost whole frames of the chunk were fed into the dnn to perform a multi-label regression for the expected tags .',\n",
       " 'the fully dnn , which is regarded as an encoding function , can well map the audio features sequence to a multi-tag vector .',\n",
       " 'a deep pyramid structure was also designed to extract more robust high-level features related to the target tags .',\n",
       " 'further improved methods were adopted , such as the dropout and background noise aware training , to enhance its generalization capability for new audio recordings in mismatched environments .',\n",
       " 'compared with the conventional normal mixture model ( gmm ) and support vector machine ( svm ) methods , the proposed fully dnn-based method could well utilize the long-term temporal information with the whole chunk as the input .',\n",
       " 'the results show that our approach obtained a 15 % relative improvement compared with the official gmm-based method of dcase 2016 challenge .',\n",
       " 'in this paper a secret message/image transmission technique has been proposed through ( 2 , 2 ) visual cryptographic share which is non-interpretable in general .',\n",
       " 'a binary image is taken as cover image and authenticating message/image has been fabricated into it through a hash function where two bits in each pixel within four bits from lsb of the pixel is embedded and as a result it converts the binary image to gray scale one .',\n",
       " '( 2,2 ) visual cryptographic shares are generated from this converted gray scale image .',\n",
       " 'during decoding shares are combined to regenerate the authenticated image from where the secret message/image is obtained through the same hash function along with reduction of noise .',\n",
       " 'noise reduction is also done on regenerated authenticated image to regenerate original cover image at destination .',\n",
       " 'we illustrate how elementary information-theoretic ideas may be employed to provide proofs for well-known , nontrivial results in number theory .',\n",
       " 'specifically , we give an elementary and fairly short proof of the following asymptotic result : the sum of ( log p ) /p , taken over all primes p not exceeding n , is asymptotic to log n as n tends to infinity .',\n",
       " 'we also give finite-n bounds refining the above limit .',\n",
       " 'this result , originally proved by chebyshev in 1852 , is closely related to the celebrated prime number theorem .',\n",
       " 'the ring learning-with-errors ( lwe ) problem , whose security is based on hard ideal lattice problems , has proven to be a promising primitive with diverse applications in cryptography .',\n",
       " 'there are however recent discoveries of faster algorithms for the principal ideal svp problem , and attempts to generalize the attack to non-principal ideals .',\n",
       " 'in this work , we study the lwe problem on group rings , and build cryptographic schemes based on this new primitive .',\n",
       " 'one can regard the lwe on cyclotomic integers as a special case when the underlying group is cyclic , while our proposal utilizes non-commutative groups , which eliminates the weakness associated with the principal ideal lattices .',\n",
       " 'in particular , we show how to build public key encryption schemes from dihedral group rings , which maintains the efficiency of the ring-lwe and improves its security .',\n",
       " 'this paper addresses the general problem of blind echo retrieval , i.e. , given m sensors measuring in the discrete-time domain m mixtures of k delayed and attenuated copies of an unknown source signal , can the echo locations and weights be recovered ?',\n",
       " 'this problem has broad applications in fields such as sonars , seismol-ogy , ultrasounds or room acoustics .',\n",
       " 'it belongs to the broader class of blind channel identification problems , which have been intensively studied in signal processing .',\n",
       " 'existing methods in the literature proceed in two steps : ( i ) blind estimation of sparse discrete-time filters and ( ii ) echo information retrieval by peak-picking on filters .',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extend.replaced_sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "\n",
    "for ind,item in df_extend.id_bert.iteritems():\n",
    "    X_all.append([df_extend.id_bert[ind],\n",
    "                  df_extend.mask_bert[ind],\n",
    "                  df_extend.segment_bert[ind],\n",
    "                  df_extend.orders[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y from the dataframe\n",
    "y_all = df_extend.label_y.values.tolist()\n",
    "\n",
    "# y: convert it into one-hot encoder\n",
    "for i in range(len(y_all)):\n",
    "    y_all[i] =  tf.one_hot(y_all[i],depth=6)\n",
    "\n",
    "# some y have more than one tensor --> add them together!\n",
    "y_all_combine = []\n",
    "\n",
    "for i in range(len(y_all)):\n",
    "    if y_all[i].shape[0]>1:\n",
    "        tmp = tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0],shape=(1,6))\n",
    "        for j in range(len(y_all[i])):\n",
    "            tmp = tmp + y_all[i][j]\n",
    "        y_all_combine.append(tmp)\n",
    "    else:\n",
    "        y_all_combine.append(y_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_all_combine)):\n",
    "    y_all_combine[i] = tf.reshape(y_all_combine[i],(6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46867"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46867"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_all_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all_combine, test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input1 = []\n",
    "train_input2 = []\n",
    "train_input3 = []\n",
    "train_input4 = []\n",
    "for i in range(len(X_train)):\n",
    "    train_input1.append(X_train[i][0])\n",
    "    train_input2.append(X_train[i][1])\n",
    "    train_input3.append(X_train[i][2])\n",
    "    train_input4.append(X_train[i][3])\n",
    "\n",
    "val_input1 = []\n",
    "val_input2 = []\n",
    "val_input3 = []\n",
    "val_input4 = []\n",
    "for j in range(len(X_test)):\n",
    "    val_input1.append(X_test[j][0])\n",
    "    val_input2.append(X_test[j][1])\n",
    "    val_input3.append(X_test[j][2])\n",
    "    val_input4.append(X_test[j][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37493"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": train_input1, \"input_2\": train_input2, \"input_3\": train_input3,\"input_4\": train_input4},y_train)).shuffle(50000).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": val_input1, \"input_2\": val_input2, \"input_3\": val_input3,\"input_4\": val_input4},y_test)).shuffle(50000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sentences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #print(sentences['input_4'].shape)\n",
    "        out = model([sentences['input_1'],\n",
    "                     sentences['input_2'],\n",
    "                     sentences['input_3'],\n",
    "                     tf.reshape(sentences['input_4'],(-1,1))])    \n",
    "        # Calculate the loss of each class\n",
    "        loss = loss_object(labels, out)      \n",
    "        \n",
    "    train_loss(loss) # Calculate accumulative average loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_accuracy(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(sentences, labels):\n",
    "    out = model([sentences['input_1'],\n",
    "                 sentences['input_2'],\n",
    "                 sentences['input_3'],\n",
    "                 tf.reshape(sentences['input_4'],(-1,1))])    \n",
    "    loss = loss_object(labels, out)   \n",
    "    val_loss(loss)    \n",
    "    val_accuracy(labels,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "orders (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           orders[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 769)]        0           keras_layer[1][0]                \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6)            4620        tf_op_layer_concat_1[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 109,486,863\n",
      "Trainable params: 109,486,862\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "input_order = tf.keras.layers.Input(shape=(1), dtype=tf.int32, name=\"orders\")\n",
    "\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "x_order = tf.keras.layers.Dense(1)(input_order)\n",
    "\n",
    "merge_x = tf.concat([pooled_output, x_order], axis=1)\n",
    "\n",
    "#x = tf.keras.layers.Dropout(0.3)(merge_x)\n",
    "x = tf.keras.layers.Dense(6, activation='sigmoid')(merge_x)\n",
    "\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids, input_order], outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_1: (None, 300), input_2: (None, 300), input_3: (None, 300), input_4: (None,)}, (None, 6)), types: ({input_1: tf.int32, input_2: tf.int32, input_3: tf.int32, input_4: tf.int32}, tf.float32)>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100], Loss: 0.36, Accuracy: 89.06 \n",
      "[Step 200], Loss: 0.33, Accuracy: 88.54 \n",
      "[Step 300], Loss: 0.31, Accuracy: 89.32 \n",
      "[Step 400], Loss: 0.30, Accuracy: 85.42 \n",
      "[Step 500], Loss: 0.30, Accuracy: 89.32 \n",
      "[Epoch 1], Validation Loss: 0.27, Validation Accuracy: 88.77\n",
      "-----------------------------------------\n",
      "[Step 600], Loss: 0.25, Accuracy: 91.41 \n",
      "[Step 700], Loss: 0.25, Accuracy: 89.58 \n",
      "[Step 800], Loss: 0.25, Accuracy: 87.24 \n",
      "[Step 900], Loss: 0.24, Accuracy: 89.32 \n",
      "[Step 1000], Loss: 0.25, Accuracy: 91.15 \n",
      "[Step 1100], Loss: 0.25, Accuracy: 91.41 \n",
      "[Epoch 2], Validation Loss: 0.27, Validation Accuracy: 88.79\n",
      "-----------------------------------------\n",
      "[Step 1200], Loss: 0.21, Accuracy: 92.19 \n",
      "[Step 1300], Loss: 0.20, Accuracy: 91.41 \n",
      "[Step 1400], Loss: 0.20, Accuracy: 92.45 \n",
      "[Step 1500], Loss: 0.20, Accuracy: 90.10 \n",
      "[Step 1600], Loss: 0.20, Accuracy: 89.84 \n",
      "[Step 1700], Loss: 0.20, Accuracy: 94.53 \n",
      "[Epoch 3], Validation Loss: 0.29, Validation Accuracy: 88.49\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# train on whole dataset\n",
    "\n",
    "import math\n",
    "\n",
    "EPOCHS = 3\n",
    "step = 0\n",
    "exp = 1\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "checkpoint_path = \"exp/exp%d/ckpt/epoch-{}.ckpt\"%exp\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for sentences, labels in train_dataset:       \n",
    "        train_step(sentences, labels)\n",
    "        step+=1\n",
    "        \n",
    "        if step%math.ceil(len(train_input1)/BATCH_SIZE)==0:\n",
    "            train_loss_history.append(train_loss.result())\n",
    "            train_acc_history.append(train_accuracy.result())\n",
    "\n",
    "        \n",
    "        if step%100==0:\n",
    "            template = '[Step {:0}], Loss: {:.2f}, Accuracy: {:.2f} '\n",
    "            print(template.format(step,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100))\n",
    "            \n",
    "            \n",
    "                            \n",
    "        # Reset the metrics for the next step\n",
    "        train_accuracy.reset_states()\n",
    "               \n",
    "    for val_sentences, val_labels in val_dataset:\n",
    "        val_step(val_sentences, val_labels)\n",
    "\n",
    "    template = '[Epoch {:0}], Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,val_loss.result(),val_accuracy.result()*100))\n",
    "    print('-----------------------------------------')\n",
    "        \n",
    "    val_loss_history.append(val_loss.result())\n",
    "    val_acc_history.append(val_accuracy.result())\n",
    "   \n",
    "    \n",
    "   # Saving history records to HDD\n",
    "    train_acc_history_save = np.asarray(train_acc_history)\n",
    "    val_acc_history_save = np.asarray(val_acc_history)\n",
    "\n",
    "    np.save('exp/exp%d/history/train_loss.npy'%exp,np.asarray(train_loss_history))\n",
    "    np.save('exp/exp%d/history/val_loss.npy'%exp,np.asarray(val_loss_history))\n",
    "    \n",
    "    np.save('exp/exp%d/history/train-acc-epoch%d.npy'%(exp,epoch+1),train_acc_history_save)\n",
    "    np.save('exp/exp%d/history/val-acc-epoch%d.npy'%(exp,epoch+1),val_acc_history_save)\n",
    "\n",
    "    \n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    model.save_weights(checkpoint_path.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa1601c64e0>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('exp/exp1/ckpt/epoch-1.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n"
     ]
    }
   ],
   "source": [
    "# convert private set into bert input type\n",
    "\n",
    "private_csv = pd.read_csv('task1_private_testset.csv')\n",
    "\n",
    "columns = ['Id','Title','Sentences','Authors',\n",
    "           'Categories','Created Date']\n",
    "private_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "for index, row in private_csv.iterrows():\n",
    "    sentences = row['Abstract'].split('$$$')\n",
    "    #labels = row['Task 1'].split(' ')\n",
    "    \n",
    "    if index%500==0:\n",
    "        print(index)\n",
    "        \n",
    "    for i in range(len(sentences)):\n",
    "        s = pd.Series({'Id':row['Id'], 'Title':row['Title'],'Sentences':sentences[i],\n",
    "                      'Authors':row['Authors'] , 'Categories':row['Categories'],\n",
    "                      'Created Date':row['Created Date']})\n",
    "        private_test = private_test.append(s, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>replaced_sentences</th>\n",
       "      <th>id_bert</th>\n",
       "      <th>mask_bert</th>\n",
       "      <th>segment_bert</th>\n",
       "      <th>orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>Many real-world analytics problems involve two...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>many world analytics problems involve two sign...</td>\n",
       "      <td>[101, 2116, 2088, 25095, 3471, 9125, 2048, 327...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>Due to the typically complex nature of each ch...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>due to the typically complex nature of each ch...</td>\n",
       "      <td>[101, 2349, 2000, 1996, 4050, 3375, 3267, 1997...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>By and large, machine learning tools are inten...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>by and large , machine learning tools are inte...</td>\n",
       "      <td>[101, 2011, 1998, 2312, 1010, 3698, 4083, 5906...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>In contrast, we propose a new and very general...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>in contrast , we propose a new and very genera...</td>\n",
       "      <td>[101, 1999, 5688, 1010, 2057, 16599, 1037, 204...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>A key component of our framework is the SPO lo...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>a key component of our framework is the spo lo...</td>\n",
       "      <td>[101, 1037, 3145, 6922, 1997, 2256, 7705, 2003...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>Training a model with respect to the SPO loss ...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>training a model with respect to the spo loss ...</td>\n",
       "      <td>[101, 2731, 1037, 2944, 2007, 4847, 2000, 1996...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>We also propose a stochastic gradient descent ...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>we also propose a random gradient descent algo...</td>\n",
       "      <td>[101, 2057, 2036, 16599, 1037, 6721, 17978, 69...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T20001</td>\n",
       "      <td>Smart \"Predict, then Optimize\"</td>\n",
       "      <td>Finally, we perform computational experiments ...</td>\n",
       "      <td>Elmachtoub/Grigas</td>\n",
       "      <td>math.OC/cs.LG/stat.ML</td>\n",
       "      <td>2017-10-22</td>\n",
       "      <td>finally , we perform computational experiments...</td>\n",
       "      <td>[101, 2633, 1010, 2057, 4685, 15078, 7885, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T20002</td>\n",
       "      <td>On the variable hierarchy of first-order spectra</td>\n",
       "      <td>The spectrum of a first-order logic sentence i...</td>\n",
       "      <td>Kopczynski/Tan</td>\n",
       "      <td>cs.LO/cs.CC</td>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>the spectrum of a first-order logic sentence i...</td>\n",
       "      <td>[101, 1996, 8674, 1997, 1037, 2034, 1011, 2344...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T20002</td>\n",
       "      <td>On the variable hierarchy of first-order spectra</td>\n",
       "      <td>In this paper we study the hierarchy of first-...</td>\n",
       "      <td>Kopczynski/Tan</td>\n",
       "      <td>cs.LO/cs.CC</td>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>in this paper we study the hierarchy of first-...</td>\n",
       "      <td>[101, 1999, 2023, 3259, 2057, 2817, 1996, 1257...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                             Title  \\\n",
       "0  T20001                    Smart \"Predict, then Optimize\"   \n",
       "1  T20001                    Smart \"Predict, then Optimize\"   \n",
       "2  T20001                    Smart \"Predict, then Optimize\"   \n",
       "3  T20001                    Smart \"Predict, then Optimize\"   \n",
       "4  T20001                    Smart \"Predict, then Optimize\"   \n",
       "5  T20001                    Smart \"Predict, then Optimize\"   \n",
       "6  T20001                    Smart \"Predict, then Optimize\"   \n",
       "7  T20001                    Smart \"Predict, then Optimize\"   \n",
       "8  T20002  On the variable hierarchy of first-order spectra   \n",
       "9  T20002  On the variable hierarchy of first-order spectra   \n",
       "\n",
       "                                           Sentences            Authors  \\\n",
       "0  Many real-world analytics problems involve two...  Elmachtoub/Grigas   \n",
       "1  Due to the typically complex nature of each ch...  Elmachtoub/Grigas   \n",
       "2  By and large, machine learning tools are inten...  Elmachtoub/Grigas   \n",
       "3  In contrast, we propose a new and very general...  Elmachtoub/Grigas   \n",
       "4  A key component of our framework is the SPO lo...  Elmachtoub/Grigas   \n",
       "5  Training a model with respect to the SPO loss ...  Elmachtoub/Grigas   \n",
       "6  We also propose a stochastic gradient descent ...  Elmachtoub/Grigas   \n",
       "7  Finally, we perform computational experiments ...  Elmachtoub/Grigas   \n",
       "8  The spectrum of a first-order logic sentence i...     Kopczynski/Tan   \n",
       "9  In this paper we study the hierarchy of first-...     Kopczynski/Tan   \n",
       "\n",
       "              Categories Created Date  \\\n",
       "0  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "1  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "2  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "3  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "4  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "5  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "6  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "7  math.OC/cs.LG/stat.ML   2017-10-22   \n",
       "8            cs.LO/cs.CC   2014-03-10   \n",
       "9            cs.LO/cs.CC   2014-03-10   \n",
       "\n",
       "                                  replaced_sentences  \\\n",
       "0  many world analytics problems involve two sign...   \n",
       "1  due to the typically complex nature of each ch...   \n",
       "2  by and large , machine learning tools are inte...   \n",
       "3  in contrast , we propose a new and very genera...   \n",
       "4  a key component of our framework is the spo lo...   \n",
       "5  training a model with respect to the spo loss ...   \n",
       "6  we also propose a random gradient descent algo...   \n",
       "7  finally , we perform computational experiments...   \n",
       "8  the spectrum of a first-order logic sentence i...   \n",
       "9  in this paper we study the hierarchy of first-...   \n",
       "\n",
       "                                             id_bert  \\\n",
       "0  [101, 2116, 2088, 25095, 3471, 9125, 2048, 327...   \n",
       "1  [101, 2349, 2000, 1996, 4050, 3375, 3267, 1997...   \n",
       "2  [101, 2011, 1998, 2312, 1010, 3698, 4083, 5906...   \n",
       "3  [101, 1999, 5688, 1010, 2057, 16599, 1037, 204...   \n",
       "4  [101, 1037, 3145, 6922, 1997, 2256, 7705, 2003...   \n",
       "5  [101, 2731, 1037, 2944, 2007, 4847, 2000, 1996...   \n",
       "6  [101, 2057, 2036, 16599, 1037, 6721, 17978, 69...   \n",
       "7  [101, 2633, 1010, 2057, 4685, 15078, 7885, 200...   \n",
       "8  [101, 1996, 8674, 1997, 1037, 2034, 1011, 2344...   \n",
       "9  [101, 1999, 2023, 3259, 2057, 2817, 1996, 1257...   \n",
       "\n",
       "                                           mask_bert  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        segment_bert orders  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [1]  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [2]  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [3]  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [4]  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [5]  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [6]  \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [7]  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [8]  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [1]  \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...    [2]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['replaced_sentences'] = private_test.Sentences.apply(lambda x:replace_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['id_bert'] = private_test.replaced_sentences.apply(lambda x:make_id(x))\n",
    "private_test['mask_bert'] = private_test.replaced_sentences.apply(lambda x:make_mask(x))\n",
    "private_test['segment_bert'] = private_test.replaced_sentences.apply(lambda x:make_segment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['id_bert'] = private_test.id_bert.apply(lambda x:np.asarray(x))\n",
    "private_test['mask_bert'] = private_test.mask_bert.apply(lambda x:np.asarray(x))\n",
    "private_test['segment_bert'] = private_test.segment_bert.apply(lambda x:np.asarray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['id_bert'] = private_test.id_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "private_test['mask_bert'] = private_test.mask_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "private_test['segment_bert'] = private_test.segment_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 'T20001'\n",
    "order = 0\n",
    "orders = []\n",
    "\n",
    "for ind,row in private_test.iterrows():\n",
    "    if row.Id == id_seq:\n",
    "        order += 1\n",
    "    else:\n",
    "        id_seq = row.Id\n",
    "        order = 1  \n",
    "    orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['orders'] = orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test['orders'] = private_test.orders.apply(lambda x:np.reshape(x,(1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_dataset = tf.data.Dataset.from_tensor_slices({'input_word_ids':private_test['id_bert'],\n",
    "                                                   'input_mask':private_test['mask_bert'],\n",
    "                                                   'segment_ids':private_test['segment_bert'],\n",
    "                                                   'orders':private_test['orders']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict(private_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set = pd.read_pickle('test_with_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['replaced_sentences'] = public_testing_set.Sentences.apply(lambda x:replace_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.replaced_sentences.apply(lambda x:make_id(x))\n",
    "public_testing_set['mask_bert'] = public_testing_set.replaced_sentences.apply(lambda x:make_mask(x))\n",
    "public_testing_set['segment_bert'] = public_testing_set.replaced_sentences.apply(lambda x:make_segment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.id_bert.apply(lambda x:np.asarray(x))\n",
    "public_testing_set['mask_bert'] = public_testing_set.mask_bert.apply(lambda x:np.asarray(x))\n",
    "public_testing_set['segment_bert'] = public_testing_set.segment_bert.apply(lambda x:np.asarray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.id_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "public_testing_set['mask_bert'] = public_testing_set.mask_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "public_testing_set['segment_bert'] = public_testing_set.segment_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 'T00001'\n",
    "order = 0\n",
    "orders = []\n",
    "\n",
    "for ind,row in public_testing_set.iterrows():\n",
    "    if row.Id == id_seq:\n",
    "        order += 1\n",
    "    else:\n",
    "        id_seq = row.Id\n",
    "        order = 1  \n",
    "    orders.append(order)\n",
    "    \n",
    "public_testing_set['orders'] = orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['orders'] = public_testing_set.orders.apply(lambda x:np.reshape(x,(1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices({'input_word_ids':public_testing_set['id_bert'],\n",
    "                                                   'input_mask':public_testing_set['mask_bert'],\n",
    "                                                   'segment_ids':public_testing_set['segment_bert'],\n",
    "                                                   'orders':public_testing_set['orders']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_public = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131166, 6)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_public.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131782, 6)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = np.concatenate((r_public, r), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262948, 6)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c1 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c2 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c3 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c4 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c5 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c6 = np.zeros((submission.shape[0]),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [c1,c2,c3,c4,c5,c6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(final[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(final.shape[0]):\n",
    "    for j in range(6):\n",
    "        if final[i][j]>=THRESHOLD:\n",
    "            c[j][i] = 1\n",
    "            \n",
    "    if c[0][i]==0&c[1][i]==0&c[2][i]==0&c[3][i]==0&c[4][i]==0&c[5][i]==0:\n",
    "        count += 1\n",
    "        c[np.argmax(final[i])][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194227"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv('dataset/task1_sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00001_S006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00001_S007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00002_S001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00002_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00002_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      order_id  BACKGROUND  OBJECTIVES  METHODS  RESULTS  CONCLUSIONS  OTHERS\n",
       "0  T00001_S001           0           0        0        0            0       0\n",
       "1  T00001_S002           0           0        0        0            0       0\n",
       "2  T00001_S003           0           0        0        0            0       0\n",
       "3  T00001_S004           0           0        0        0            0       0\n",
       "4  T00001_S005           0           0        0        0            0       0\n",
       "5  T00001_S006           0           0        0        0            0       0\n",
       "6  T00001_S007           0           0        0        0            0       0\n",
       "7  T00002_S001           0           0        0        0            0       0\n",
       "8  T00002_S002           0           0        0        0            0       0\n",
       "9  T00002_S003           0           0        0        0            0       0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.BACKGROUND = c1\n",
    "submission.OBJECTIVES = c2\n",
    "submission.METHODS = c3\n",
    "submission.RESULTS = c4\n",
    "submission.CONCLUSIONS = c5\n",
    "submission.OTHERS = c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00001_S006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00001_S007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00002_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00002_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00002_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      order_id  BACKGROUND  OBJECTIVES  METHODS  RESULTS  CONCLUSIONS  OTHERS\n",
       "0  T00001_S001           1           0        0        0            0       0\n",
       "1  T00001_S002           1           0        0        0            0       0\n",
       "2  T00001_S003           0           1        1        0            0       0\n",
       "3  T00001_S004           0           1        1        0            0       0\n",
       "4  T00001_S005           0           0        1        1            0       0\n",
       "5  T00001_S006           0           0        1        1            0       0\n",
       "6  T00001_S007           0           0        0        1            1       0\n",
       "7  T00002_S001           1           0        0        0            0       0\n",
       "8  T00002_S002           0           1        0        0            0       0\n",
       "9  T00002_S003           0           0        1        0            0       0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('summit_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
