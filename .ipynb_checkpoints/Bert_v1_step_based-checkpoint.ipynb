{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert import tokenization\n",
    "from bert import bert_tokenization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_id(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    #input_masks = get_masks(stokens, max_seq_length)\n",
    "    #input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def make_mask(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    #input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    #input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_masks\n",
    "\n",
    "def make_segment(sentence):\n",
    "    stokens = tokenizer.tokenize(sentence)\n",
    "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "\n",
    "    #input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    #input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    #pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "\n",
    "    return input_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend = pd.read_pickle('input_bert.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_seq = 'D00001'\n",
    "order = 0\n",
    "orders = []\n",
    "\n",
    "for ind,row in df_extend.iterrows():\n",
    "    if row.Id == id_seq:\n",
    "        order += 1\n",
    "    else:\n",
    "        id_seq = row.Id\n",
    "        order = 1  \n",
    "    orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(orders) # prepare for the one-hot encoding???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['orders'] = orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Sentences</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>encode_x</th>\n",
       "      <th>label_y</th>\n",
       "      <th>id_bert</th>\n",
       "      <th>mask_bert</th>\n",
       "      <th>segment_bert</th>\n",
       "      <th>order</th>\n",
       "      <th>orders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>Rapid popularity of Internet of Things (IoT) a...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>[67018, 50647, 57206, 39455, 1390, 27420, 4691...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[101, 5915, 6217, 1997, 4274, 1997, 2477, 1006...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>To ensure secure and reliable data communicati...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>OBJECTIVES</td>\n",
       "      <td>[76338, 100433, 97478, 27420, 57689, 49296, 11...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[101, 2000, 5676, 5851, 1998, 10539, 2951, 480...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>This paper introduces a Neuro-Fuzzy based Brai...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>[70484, 40636, 88204, 100425, 2583, 40562, 108...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[101, 2023, 3259, 13999, 1037, 11265, 10976, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>The proposed TMM utilizes node behavioral trus...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>METHODS</td>\n",
       "      <td>[22910, 55237, 16568, 69221, 61489, 64999, 144...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[101, 1996, 3818, 1056, 7382, 21852, 13045, 14...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>In contrast to the existing fuzzy based TMMs, ...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>[92177, 73437, 12088, 82732, 5232, 84607, 4056...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[101, 1999, 5688, 2000, 1996, 4493, 18001, 224...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>D00001</td>\n",
       "      <td>A Brain-Inspired Trust Management Model to Ass...</td>\n",
       "      <td>With the growing usage of cloud based IoT fram...</td>\n",
       "      <td>Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...</td>\n",
       "      <td>cs.CR/cs.AI/q-bio.NC</td>\n",
       "      <td>2018-01-11</td>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>[41541, 82732, 5858, 54546, 57206, 69711, 4056...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[101, 2007, 1996, 3652, 8192, 1997, 6112, 2241...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>In this paper, we address the problem of compu...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>OBJECTIVES</td>\n",
       "      <td>[92177, 14492, 40636, 62985, 8539, 82732, 6855...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[101, 1999, 2023, 3259, 1010, 2057, 4769, 1996...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>Given initial and final configurations of the ...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>OTHERS</td>\n",
       "      <td>[95902, 38868, 27420, 85089, 80733, 57206, 827...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[101, 2445, 3988, 1998, 2345, 22354, 1997, 199...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>We provide a novel geometrical analysis of the...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>METHODS/RESULTS</td>\n",
       "      <td>[104212, 80674, 100425, 57683, 78849, 75061, 5...</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[101, 2057, 3073, 1037, 3117, 14965, 2389, 410...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D00002</td>\n",
       "      <td>On Efficient Computation of Shortest Dubins Pa...</td>\n",
       "      <td>We then show how our method can be used to qui...</td>\n",
       "      <td>Sadeghi/Smith</td>\n",
       "      <td>cs.SY/cs.RO/math.OC</td>\n",
       "      <td>2016-09-21</td>\n",
       "      <td>RESULTS</td>\n",
       "      <td>[104212, 36294, 35912, 101740, 73071, 68424, 1...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[101, 2057, 2059, 2265, 2129, 2256, 4118, 2064...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                              Title  \\\n",
       "0  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "1  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "2  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "3  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "4  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "5  D00001  A Brain-Inspired Trust Management Model to Ass...   \n",
       "6  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "7  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "8  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "9  D00002  On Efficient Computation of Shortest Dubins Pa...   \n",
       "\n",
       "                                           Sentences  \\\n",
       "0  Rapid popularity of Internet of Things (IoT) a...   \n",
       "1  To ensure secure and reliable data communicati...   \n",
       "2  This paper introduces a Neuro-Fuzzy based Brai...   \n",
       "3  The proposed TMM utilizes node behavioral trus...   \n",
       "4  In contrast to the existing fuzzy based TMMs, ...   \n",
       "5  With the growing usage of cloud based IoT fram...   \n",
       "6  In this paper, we address the problem of compu...   \n",
       "7  Given initial and final configurations of the ...   \n",
       "8  We provide a novel geometrical analysis of the...   \n",
       "9  We then show how our method can be used to qui...   \n",
       "\n",
       "                                             Authors            Categories  \\\n",
       "0  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "1  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "2  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "3  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "4  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "5  Mahmud/Kaiser/Rahman/Rahman/Shabut/Al-Mamun/Hu...  cs.CR/cs.AI/q-bio.NC   \n",
       "6                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "7                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "8                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "9                                      Sadeghi/Smith   cs.SY/cs.RO/math.OC   \n",
       "\n",
       "  Created Date            Label  \\\n",
       "0   2018-01-11       BACKGROUND   \n",
       "1   2018-01-11       OBJECTIVES   \n",
       "2   2018-01-11          METHODS   \n",
       "3   2018-01-11          METHODS   \n",
       "4   2018-01-11          RESULTS   \n",
       "5   2018-01-11      CONCLUSIONS   \n",
       "6   2016-09-21       OBJECTIVES   \n",
       "7   2016-09-21           OTHERS   \n",
       "8   2016-09-21  METHODS/RESULTS   \n",
       "9   2016-09-21          RESULTS   \n",
       "\n",
       "                                            encode_x label_y  \\\n",
       "0  [67018, 50647, 57206, 39455, 1390, 27420, 4691...     [0]   \n",
       "1  [76338, 100433, 97478, 27420, 57689, 49296, 11...     [1]   \n",
       "2  [70484, 40636, 88204, 100425, 2583, 40562, 108...     [2]   \n",
       "3  [22910, 55237, 16568, 69221, 61489, 64999, 144...     [2]   \n",
       "4  [92177, 73437, 12088, 82732, 5232, 84607, 4056...     [3]   \n",
       "5  [41541, 82732, 5858, 54546, 57206, 69711, 4056...     [4]   \n",
       "6  [92177, 14492, 40636, 62985, 8539, 82732, 6855...     [1]   \n",
       "7  [95902, 38868, 27420, 85089, 80733, 57206, 827...     [5]   \n",
       "8  [104212, 80674, 100425, 57683, 78849, 75061, 5...  [2, 3]   \n",
       "9  [104212, 36294, 35912, 101740, 73071, 68424, 1...     [3]   \n",
       "\n",
       "                                             id_bert  \\\n",
       "0  [101, 5915, 6217, 1997, 4274, 1997, 2477, 1006...   \n",
       "1  [101, 2000, 5676, 5851, 1998, 10539, 2951, 480...   \n",
       "2  [101, 2023, 3259, 13999, 1037, 11265, 10976, 1...   \n",
       "3  [101, 1996, 3818, 1056, 7382, 21852, 13045, 14...   \n",
       "4  [101, 1999, 5688, 2000, 1996, 4493, 18001, 224...   \n",
       "5  [101, 2007, 1996, 3652, 8192, 1997, 6112, 2241...   \n",
       "6  [101, 1999, 2023, 3259, 1010, 2057, 4769, 1996...   \n",
       "7  [101, 2445, 3988, 1998, 2345, 22354, 1997, 199...   \n",
       "8  [101, 2057, 3073, 1037, 3117, 14965, 2389, 410...   \n",
       "9  [101, 2057, 2059, 2265, 2129, 2256, 4118, 2064...   \n",
       "\n",
       "                                           mask_bert  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "5  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "6  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "7  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "8  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "9  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                        segment_bert order  orders  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       1  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       2  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       3  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       4  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       5  \n",
       "5  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       6  \n",
       "6  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       1  \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       2  \n",
       "8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       3  \n",
       "9  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   NaN       4  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extend.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BACKGROUND                                           11948\n",
       "METHODS                                              10471\n",
       "RESULTS                                               7813\n",
       "OBJECTIVES                                            6396\n",
       "CONCLUSIONS                                           2650\n",
       "RESULTS/CONCLUSIONS                                   2020\n",
       "OBJECTIVES/METHODS                                    1270\n",
       "METHODS/RESULTS                                       1072\n",
       "OTHERS                                                 901\n",
       "BACKGROUND/OBJECTIVES                                  894\n",
       "OBJECTIVES/RESULTS                                     268\n",
       "BACKGROUND/METHODS                                     224\n",
       "METHODS/RESULTS/CONCLUSIONS                            192\n",
       "OBJECTIVES/METHODS/RESULTS                             125\n",
       "OBJECTIVES/CONCLUSIONS                                 102\n",
       "METHODS/CONCLUSIONS                                    100\n",
       "BACKGROUND/RESULTS                                      77\n",
       "OBJECTIVES/RESULTS/CONCLUSIONS                          58\n",
       "BACKGROUND/OBJECTIVES/METHODS                           50\n",
       "OBJECTIVES/METHODS/RESULTS/CONCLUSIONS                  45\n",
       "BACKGROUND/OBJECTIVES/METHODS/RESULTS/CONCLUSIONS       37\n",
       "BACKGROUND/CONCLUSIONS                                  32\n",
       "OBJECTIVES/METHODS/CONCLUSIONS                          31\n",
       "BACKGROUND/OBJECTIVES/RESULTS                           19\n",
       "BACKGROUND/METHODS/RESULTS                              18\n",
       "BACKGROUND/OBJECTIVES/CONCLUSIONS                       17\n",
       "BACKGROUND/RESULTS/CONCLUSIONS                          14\n",
       "BACKGROUND/OBJECTIVES/METHODS/RESULTS                    8\n",
       "BACKGROUND/OBJECTIVES/METHODS/CONCLUSIONS                6\n",
       "BACKGROUND/OBJECTIVES/RESULTS/CONCLUSIONS                3\n",
       "BACKGROUND/METHODS/RESULTS/CONCLUSIONS                   3\n",
       "BACKGROUND/METHODS/CONCLUSIONS                           3\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_extend.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['orders'] = df_extend.orders.apply(lambda x:np.asarray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extend['orders'] = df_extend.orders.apply(lambda x:np.reshape(x,(1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = []\n",
    "\n",
    "for ind,item in df_extend.id_bert.iteritems():\n",
    "    X_all.append([df_extend.id_bert[ind],\n",
    "                  df_extend.mask_bert[ind],\n",
    "                  df_extend.segment_bert[ind],\n",
    "                  df_extend.orders[ind]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x and y from the dataframe\n",
    "y_all = df_extend.label_y.values.tolist()\n",
    "\n",
    "# y: convert it into one-hot encoder\n",
    "for i in range(len(y_all)):\n",
    "    y_all[i] =  tf.one_hot(y_all[i],depth=6)\n",
    "\n",
    "# some y have more than one tensor --> add them together!\n",
    "y_all_combine = []\n",
    "\n",
    "for i in range(len(y_all)):\n",
    "    if y_all[i].shape[0]>1:\n",
    "        tmp = tf.constant([0.0, 0.0, 0.0, 0.0, 0.0, 0.0],shape=(1,6))\n",
    "        for j in range(len(y_all[i])):\n",
    "            tmp = tmp + y_all[i][j]\n",
    "        y_all_combine.append(tmp)\n",
    "    else:\n",
    "        y_all_combine.append(y_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_all_combine)):\n",
    "    y_all_combine[i] = tf.reshape(y_all_combine[i],(6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46867"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46867"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_all_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all_combine, test_size=0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input1 = []\n",
    "train_input2 = []\n",
    "train_input3 = []\n",
    "train_input4 = []\n",
    "for i in range(len(X_train)):\n",
    "    train_input1.append(X_train[i][0])\n",
    "    train_input2.append(X_train[i][1])\n",
    "    train_input3.append(X_train[i][2])\n",
    "    train_input4.append(X_train[i][3])\n",
    "\n",
    "val_input1 = []\n",
    "val_input2 = []\n",
    "val_input3 = []\n",
    "val_input4 = []\n",
    "for j in range(len(X_test)):\n",
    "    val_input1.append(X_test[j][0])\n",
    "    val_input2.append(X_test[j][1])\n",
    "    val_input3.append(X_test[j][2])\n",
    "    val_input4.append(X_test[j][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32806"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": train_input1, \"input_2\": train_input2, \"input_3\": train_input3,\"input_4\": train_input4},y_train)).shuffle(50000).batch(BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": val_input1, \"input_2\": val_input2, \"input_3\": val_input3,\"input_4\": val_input4},y_test)).shuffle(50000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.BinaryAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(sentences, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #print(sentences['input_4'].shape)\n",
    "        out = model([sentences['input_1'],\n",
    "                     sentences['input_2'],\n",
    "                     sentences['input_3'],\n",
    "                     tf.reshape(sentences['input_4'],(-1,1))])    \n",
    "        # Calculate the loss of each class\n",
    "        loss = loss_object(labels, out)      \n",
    "        \n",
    "    train_loss(loss) # Calculate accumulative average loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_accuracy(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(sentences, labels):\n",
    "    out = model([sentences['input_1'],\n",
    "                 sentences['input_2'],\n",
    "                 sentences['input_3'],\n",
    "                 tf.reshape(sentences['input_4'],(-1,1))])    \n",
    "    loss = loss_object(labels, out)   \n",
    "    val_loss(loss)    \n",
    "    val_accuracy(labels,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "orders (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 1)            2           orders[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_9 (TensorFlo [(None, 769)]        0           keras_layer_1[33][0]             \n",
      "                                                                 dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 769)          0           tf_op_layer_concat_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 6)            4620        dropout_35[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 109,486,863\n",
      "Trainable params: 109,486,862\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "input_order = tf.keras.layers.Input(shape=(1), dtype=tf.int32, name=\"orders\")\n",
    "\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "x_order = tf.keras.layers.Dense(1)(input_order)\n",
    "\n",
    "merge_x = tf.concat([pooled_output, x_order], axis=1)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.3)(merge_x)\n",
    "x = tf.keras.layers.Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids, input_order], outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_1: (None, 240), input_2: (None, 240), input_3: (None, 240), input_4: (None,)}, (None, 6)), types: ({input_1: tf.int32, input_2: tf.int32, input_3: tf.int32, input_4: tf.int32}, tf.float32)>"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 100], Loss: 0.17, Accuracy: 100.00 \n",
      "[Step 200], Loss: 0.11, Accuracy: 99.22 \n",
      "[Step 300], Loss: 0.08, Accuracy: 100.00 \n",
      "[Step 400], Loss: 0.07, Accuracy: 99.74 \n",
      "[Step 500], Loss: 0.06, Accuracy: 99.74 \n",
      "[Epoch 1], Validation Loss: 0.03, Validation Accuracy: 99.33\n",
      "\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "EPOCHS = 5\n",
    "step = 0\n",
    "exp = 1\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "checkpoint_path = \"exp/exp%d/ckpt/epoch-{}.ckpt\"%exp\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for sentences, labels in train_dataset:       \n",
    "        train_step(sentences, labels)\n",
    "        step+=1\n",
    "        \n",
    "        if step%math.ceil(len(train_input1)/BATCH_SIZE)==0:\n",
    "            train_loss_history.append(train_loss.result())\n",
    "            train_acc_history.append(train_accuracy.result())\n",
    "\n",
    "        \n",
    "        if step%100==0:\n",
    "            template = '[Step {:0}], Loss: {:.2f}, Accuracy: {:.2f} '\n",
    "            print(template.format(step,\n",
    "                           train_loss.result(),\n",
    "                           train_accuracy.result()*100))\n",
    "            \n",
    "            \n",
    "                            \n",
    "        # Reset the metrics for the next step\n",
    "        train_accuracy.reset_states()\n",
    "               \n",
    "    for val_sentences, val_labels in val_dataset:\n",
    "        val_step(val_sentences, val_labels)\n",
    "\n",
    "    template = '[Epoch {:0}], Validation Loss: {:.2f}, Validation Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,val_loss.result(),val_accuracy.result()*100))\n",
    "    print('\\n********************************')\n",
    "        \n",
    "    val_loss_history.append(val_loss.result())\n",
    "    val_acc_history.append(val_accuracy.result())\n",
    "   \n",
    "    \n",
    "   # Saving history records to HDD\n",
    "    train_acc_history_save = np.asarray(train_acc_history)\n",
    "    val_acc_history_save = np.asarray(val_acc_history)\n",
    "\n",
    "    np.save('exp/exp%d/history/train_loss.npy'%exp,np.asarray(train_loss_history))\n",
    "    np.save('exp/exp%d/history/val_loss.npy'%exp,np.asarray(val_loss_history))\n",
    "    \n",
    "    np.save('exp/exp%d/history/train-acc-epoch%d.npy'%(exp,epoch+1),train_acc_history_save)\n",
    "    np.save('exp/exp%d/history/val-acc-epoch%d.npy'%(exp,epoch+1),val_acc_history_save)\n",
    "\n",
    "    \n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    model.save_weights(checkpoint_path.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc857d74da0>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('exp/exp1/ckpt/epoch-3.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set = pd.read_pickle('test_with_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.Sentences.apply(lambda x:make_id(x))\n",
    "public_testing_set['mask_bert'] = public_testing_set.Sentences.apply(lambda x:make_mask(x))\n",
    "public_testing_set['segment_bert'] = public_testing_set.Sentences.apply(lambda x:make_segment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.id_bert.apply(lambda x:np.asarray(x))\n",
    "public_testing_set['mask_bert'] = public_testing_set.mask_bert.apply(lambda x:np.asarray(x))\n",
    "public_testing_set['segment_bert'] = public_testing_set.segment_bert.apply(lambda x:np.asarray(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_testing_set['id_bert'] = public_testing_set.id_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "public_testing_set['mask_bert'] = public_testing_set.mask_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))\n",
    "public_testing_set['segment_bert'] = public_testing_set.segment_bert.apply(lambda x:np.reshape(x,(1,max_seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices({'input_word_ids':public_testing_set['id_bert'],\n",
    "                                                   'input_mask':public_testing_set['mask_bert'],\n",
    "                                                   'segment_ids':public_testing_set['segment_bert']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c2 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c3 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c4 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c5 = np.zeros((submission.shape[0]),dtype=np.int32)\n",
    "c6 = np.zeros((submission.shape[0]),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [c1,c2,c3,c4,c5,c6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(r.shape[0]):\n",
    "    for j in range(6):\n",
    "        if r[i][j]>=THRESHOLD:\n",
    "            c[j][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.read_csv('dataset/task1_sample_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00001_S006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00001_S007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00002_S001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00002_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00002_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      order_id  BACKGROUND  OBJECTIVES  METHODS  RESULTS  CONCLUSIONS  OTHERS\n",
       "0  T00001_S001           0           0        0        0            0       0\n",
       "1  T00001_S002           0           0        0        0            0       0\n",
       "2  T00001_S003           0           0        0        0            0       0\n",
       "3  T00001_S004           0           0        0        0            0       0\n",
       "4  T00001_S005           0           0        0        0            0       0\n",
       "5  T00001_S006           0           0        0        0            0       0\n",
       "6  T00001_S007           0           0        0        0            0       0\n",
       "7  T00002_S001           0           0        0        0            0       0\n",
       "8  T00002_S002           0           0        0        0            0       0\n",
       "9  T00002_S003           0           0        0        0            0       0"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.BACKGROUND = c1\n",
    "submission.OBJECTIVES = c2\n",
    "submission.METHODS = c3\n",
    "submission.RESULTS = c4\n",
    "submission.CONCLUSIONS = c5\n",
    "submission.OTHERS = c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T00001_S006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T00001_S007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T00002_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T00002_S002</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T00002_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      order_id  BACKGROUND  OBJECTIVES  METHODS  RESULTS  CONCLUSIONS  OTHERS\n",
       "0  T00001_S001           1           0        0        0            0       0\n",
       "1  T00001_S002           1           0        0        0            0       0\n",
       "2  T00001_S003           0           0        1        0            0       0\n",
       "3  T00001_S004           0           0        0        0            0       0\n",
       "4  T00001_S005           0           0        0        1            0       0\n",
       "5  T00001_S006           0           0        0        1            0       0\n",
       "6  T00001_S007           0           0        0        1            1       0\n",
       "7  T00002_S001           1           1        0        0            0       0\n",
       "8  T00002_S002           0           1        0        0            0       0\n",
       "9  T00002_S003           0           0        0        0            0       0"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('summit_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
